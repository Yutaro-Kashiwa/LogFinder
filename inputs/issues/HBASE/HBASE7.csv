Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
NullPointerException with an open scanner that expired causing an immediate region server shutdown,HBASE-2077,12444359,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,spullara,spullara,spullara,30/Dec/09 07:08,30/Sep/16 09:01,01/Jul/25 07:49,21/Jun/11 20:58,0.20.2,0.20.3,,,,0.90.4,,,regionserver,,,,2,"2009-12-29 18:05:55,432 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Scanner -4250070597157694417 lease expired
2009-12-29 18:05:55,443 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1310)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:136)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
	at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:641)
	at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
	at java.util.PriorityQueue.poll(PriorityQueue.java:523)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:113)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.nextInternal(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1719)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1944)
	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:648)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
2009-12-29 18:05:55,446 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 7 on 55260, call next(-4250070597157694417, 10000) from 192.168.1.90:54011: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:869)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:859)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1965)
	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:648)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1310)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:136)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
	at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:641)
	at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
	at java.util.PriorityQueue.poll(PriorityQueue.java:523)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:113)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.nextInternal(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1719)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1944)
	... 5 more
2009-12-29 18:05:55,447 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server Responder, call next(-4250070597157694417, 10000) from 192.168.1.90:54011: output error
2009-12-29 18:05:55,448 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 7 on 55260 caught: java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:126)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
	at org.apache.hadoop.hbase.ipc.HBaseServer.channelWrite(HBaseServer.java:1125)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Responder.processResponse(HBaseServer.java:615)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Responder.doRespond(HBaseServer.java:679)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:943)

2009-12-29 18:05:56,322 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 55260
2009-12-29 18:05:56,322 INFO org.apache.hadoop.ipc.HBaseServer: Stopping IPC Server listener on 55260
","Hadoop 0.20.0, Mac OS X, Java 6",3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,HBASE-2503,HBASE-16737,,,"20/May/11 20:57;stack;2077-suggestion.txt;https://issues.apache.org/jira/secure/attachment/12479956/2077-suggestion.txt","21/May/11 04:27;stack;2077-v4.txt;https://issues.apache.org/jira/secure/attachment/12479982/2077-v4.txt","31/Mar/10 20:43;jdcryans;HBASE-2077-3.patch;https://issues.apache.org/jira/secure/attachment/12440402/HBASE-2077-3.patch","23/Feb/10 02:46;jdcryans;HBASE-2077-redux.patch;https://issues.apache.org/jira/secure/attachment/12436666/HBASE-2077-redux.patch","30/Dec/09 07:17;spullara;[Bug_HBASE-2077]_Fixes_a_very_rare_race_condition_between_lease_expiration_and_renewal.patch;https://issues.apache.org/jira/secure/attachment/12429123/%5BBug_HBASE-2077%5D_Fixes_a_very_rare_race_condition_between_lease_expiration_and_renewal.patch",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26134,Reviewed,,,,Fri Nov 20 13:01:44 UTC 2015,,,,,,,,,,"0|i0hgcn:",99911,Removes lease from lease monitor while operation is running inside the server.,,,,,,,,,,,,,,,,,,,,"30/Dec/09 07:11;spullara;jdcryans helped narrow it down to an issue in Leases where the lease would polled from the queue at the same time it was being renewed.  since there is no sychronization protection there is a race condition.  I am attaching a patch that should fix the problem though it is very difficult to reproduce.
;;;","30/Dec/09 07:17;spullara;Patch to fix race condition;;;","30/Dec/09 07:40;jdcryans;I ran the client test and it passes. Committed to branch and trunk. Made Sam a new contributor, thanks for the patch!;;;","30/Dec/09 17:47;stack;Patch looks good to me.;;;","23/Feb/10 01:38;jdcryans;This is not fixed in 0.20.3, even tho the patch got in we still get the error:

{code}
2010-02-22 19:18:06,638 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Scanner -4405675591371793964 lease expired
2010-02-22 19:18:06,639 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1310)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:136)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
	at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:641)
	at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
	at java.util.PriorityQueue.poll(PriorityQueue.java:523)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:113)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.nextInternal(HRegion.java:1807)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1771)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1894)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
{code};;;","23/Feb/10 02:46;jdcryans;The way we are using the DelayQueue looks broken. We wait for leaseCheckFrequency (or the delay time, whichever happens first) in Leases and then we just expire the scanner. I think there's times when we really expire scanners that aren't done yet. This patch adds a check in run() to verify the element is really expired.;;;","23/Feb/10 02:47;jdcryans;Moving in 0.20.4 and marking critical.;;;","23/Feb/10 11:54;ykulbak;Not sure whether this is related, but we've seen similar behaviour:

We've seen several of these exact stack traces (on 0.20.4-dev) which were accompanied by an immediate region server shutdown. At start it seemed that this error causes the shutdown but in every case, after a thorough examination of the logs, we also found a zookeeper session timeout.  Eventually we discovered it was a faulty disk causing large delays at unpredictable times.

In our cases we also had messages like:
2010-02-05 16:44:20,821 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 63152ms, ten times longer than scheduled: 1000
and
2010-02-05 16:44:26,448 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: ZooKeeper session expired

I can provide more details if this is relevant
;;;","23/Feb/10 16:32;jdcryans;@Yoram

The NPEs are enough to kill a region servers, what you pasted is a GC pause that took more than 1 minute. But, I see how that could happen... If for some reason the user scans more than 1 row (scanner caching), and that during the scan the thread is paused for some reason, there's a good possibility that on the next iteration in HRS.next() the lease has already been expired. 

I would love to see a log of such an event.;;;","23/Feb/10 19:24;ipye;I've applied the patch to 0.20.3, and things look like they are working fine for me now. The same map/reduce job which crashed with the above stack trace now completes without error.
In my case, I have an expensive reduce phase, which is running on 1701037 rows input. It looks like this was taking so long to complete that the scanner timeout problem was happening.;;;","24/Feb/10 03:39;jdcryans;@Ian, that's good news.

Also I got Yoram's logs off-list, I'll take a look tomorrow and see if there's anything that's in the scope of this jira.;;;","25/Feb/10 00:37;jdcryans;WRT Yoram's logs, it went like I thought it was eg a scan begins before the GC pause and when it ends the lease is expired and around the same time a scanner tries to finish. Does it make sense to handle such a case? From a client perspective, a pause of 1 minute probably already timed it out.

I will go forward and commit this patch as it fixed Ian's problem.;;;","25/Feb/10 01:26;jdcryans;Second patch committed to branch and trunk.;;;","26/Feb/10 00:56;jdcryans;Further testing shows that my patch in fact introduced a bug that was keeping the leases opened for a long time. Reopening to see if we can dig deeper in Ian's issue.;;;","31/Mar/10 18:23;jdcryans;So I'm thinking about taking a new approach to this bug. Since the major problem here is that we need knowledge of any client using a scanner (or anything else lease-related), I think we should add a new AtomicInteger inside Leases.Lease and increment it every time a user renews that lease. When you are down with the lease-related action, you decrease the AtomicInteger. This protects us from any GC pause happening while, for example, a scanner is next'ing 100 rows and gets a 60 secs pause right in the middle.;;;","31/Mar/10 20:43;jdcryans;Patch that implements my latest idea. The user has the choice of keeping track of the lease usage or not. Passes the tests that currently pass.;;;","02/Apr/10 18:18;hbasebuild;What if you renewed the lease on entry and on the way out as insurance
against long-running 'next' invocation?  Would that be 'cleaner'?


On Wed, Mar 31, 2010 at 1:45 PM, Jean-Daniel Cryans (JIRA)
;;;","05/Apr/10 20:08;jdcryans;How would it be cleaner?;;;","05/Apr/10 20:50;stack;No new long that you increment/decrement and keep an account of.

The method name stays as renewLease rather than talk about increment/decrement (""Why do I have to do increment/decrement on a lease when  all I'm interested in is lease renewal""). ;;;","06/Apr/10 04:53;jdcryans;bq. The method name stays as renewLease rather than talk about increment/decrement (""Why do I have to do increment/decrement on a lease when all I'm interested in is lease renewal"").

The bulk of the issue is about not timing out a lease that someone currently uses, whether there's a GC or not. To be certain we have acquired the lease during the whole operation, unless we renew the lease after each line, I don't see how we can insure the same level of safety that my patch offers.

This patch also allows multiple users to share the lease if it's needed (hence incrementing/decrementing).;;;","06/Apr/10 05:22;stack;.bq This patch also allows multiple users to share the lease if it's needed (hence incrementing/decrementing).

This seems perverse to me.  When would such a usecase make sense?

.bq To be certain we have acquired the lease during the whole operation, unless we renew the lease after each line, I don't see how we can insure the same level of safety that my patch offers.

Ok.  Makes sense that while the scanner is inside the server, then the lease moves to a different 'state'.  My suggestion doesn't cover case of our timing out because of GC while scanner is a server-side resident. 

Why not remove the lease on entry and then renew it on the way out?  (IMO, the increment/decrement semantic is confusing).
;;;","08/Apr/10 18:21;jdcryans;Let's punt this to 0.20.5 then;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","29/May/10 02:53;tlipcon;What's the status of this patch in 0.20 branch? It seems it was committed then reverted, but the revert didn't actually do a full revert (left a new getExpirationTime() method in there). We're seeing the issue on 0.20.4. Does anyone have thoughts on a good fix?;;;","29/May/10 17:03;jdcryans;The first patch was committed, the second reverted (maybe I forgot something in there tho). HBASE-2503 plays around the same part of the code, and I'm pretty sure it fixes the NPE but I can't tell for sure since to trip on it you need a heavily GCing region server.

So it should be fixed in 0.20.5, it would be awesome if you can confirm.;;;","05/Oct/10 21:35;ryanobjc;core issue: no concurrency control between next() calls and lease timeouts.  While nice to have, this is a corner case and can't hold up 0.90;;;","20/May/11 20:57;stack;Here is a suggestion where we remove lease from leases while we are processing a request then on the way out in a finally we renew lease.;;;","21/May/11 04:27;stack;Ahemm.. this is a version that actually works (TestFromClientSide is a good test for this change).;;;","21/Jun/11 20:40;jdcryans;+1 on latest patch, I like it.;;;","21/Jun/11 20:58;stack;Applied 2077-v4.txt to branch and trunk as a 'part2' on this issue.  I'm now closing this since its gone all over the place.  We've not seen Sam's original issue in a while and it'll look different in current codebase; lets open new issue then.;;;","21/Jun/11 21:02;jdcryans;Yeah the NPEs are gone for me but I was still able to easily get myself into a situation that triggered expired leases by just scanning a table. This patch will solve that nicely.;;;","28/Jun/11 22:42;hudson;Integrated in HBase-TRUNK #1995 (See [https://builds.apache.org/job/HBase-TRUNK/1995/])
    ;;;","09/Aug/11 20:07;tlipcon;This is long since committed, but just a request:

In the future could we open separate JIRAs rather than doing a ""part 2"" when the commits are more than a day apart? It's very difficult to figure out what went on in the history of this JIRA, since it was committed for 0.20 in Dec '09, briefly amended in Feb '10, amendation partially reverted the next day, and then another change in Jun '11 for 0.90.4 to solve an entirely different bug than the description indicates. This makes it very difficult to support past branches or maintain distributions, since it appears this was fixed long ago but in fact 0.90.3 lacks a major part of the JIRA.;;;","09/Aug/11 20:35;stack;Sorry Todd.  Will be better going forward.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,
HBaseMaster requires HDFS superuser privileges due to waitOnSafeMode,HBASE-2075,12444338,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,bassam,bassam,29/Dec/09 23:04,12/Oct/12 06:14,01/Jul/25 07:49,30/Dec/09 00:51,0.20.3,,,,,0.20.3,0.90.0,,master,,,,0,"Repro:
1) Enable dfs.permissions
2) Start HBaseMaster in a different linux user account from HDFS. 

I get the following exception in the log. It looks like waitOnSafeMode requires HDFS superuser privileges which I do not grant to HBase.

{code}
2009-12-29 14:44:27,503 ERROR org.apache.hadoop.hbase.master.HMaster: Can not start master
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1227)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1268)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.security.AccessControlException: Superuser privilege is requ
ired
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkSuperuserPrivilege(FSNamesystem.java:4528)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.datanodeReport(FSNamesystem.java:3560)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getDatanodeReport(NameNode.java:596)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

        at org.apache.hadoop.ipc.Client.call(Client.java:739)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
        at $Proxy0.getDatanodeReport(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy0.getDatanodeReport(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.datanodeReport(DFSClient.java:818)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getDataNodeStats(DistributedFileSystem.java:353)
        at org.apache.hadoop.hbase.util.FSUtils.waitOnSafeMode(FSUtils.java:146)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:197)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/09 00:49;apurtell;HBASE-2075.patch;https://issues.apache.org/jira/secure/attachment/12429107/HBASE-2075.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26133,,,,,Wed Dec 30 01:45:42 UTC 2009,,,,,,,,,,"0|i08tav:",49347,,,,,,,,,,,,,,,,,,,,,"29/Dec/09 23:09;jdcryans;That was added in HBASE-1960.

Maybe we should just catch the exception, log a WARN and continue?;;;","30/Dec/09 00:51;apurtell;bq. Maybe we should just catch the exception, log a WARN and continue?

Committed attached patch which should resolve this issue to trunk and branch. Just catches the exception and continues. This code catches what should be a fairly rare corner case (except for up on EC2) so warning will cause more confusion than help. ;;;","30/Dec/09 01:45;jdcryans;Fair enough, thx Andrew!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MetricsRate is missing ""registry"" parameter",HBASE-2068,12444010,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,larsgeorge,larsgeorge,larsgeorge,22/Dec/09 13:46,12/Oct/12 06:14,01/Jul/25 07:49,04/Jan/10 21:02,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,"I am trying to get the graphing going using Edward Capriolo's great JMX to Cacti [guide|http://www.jointhegrid.com/hadoop/]. I checked and I am missing the request rate in the JMX MBean:

{noformat}
# sh 0_20/regionserver/RegionServerStatistics.sh service:jmx:rmi:///jndi/rmi://foobar:10102/jmxrmi controlRole mypass hadoop:name=RegionServerStatistics,service=RegionServer
storefiles:493 blockCacheFree:139771296 storefileIndexSizeMB:102 memstoreSizeMB:0 stores:658 blockCacheCount:8400 
regions:83 blockCacheHitRatio:0 blockCacheSize:717478944 atomicIncrementTimeNumOps:0 atomicIncrementTimeAvgTime:0 
atomicIncrementTimeMinTime:-1 atomicIncrementTimeMaxTime:0 
{noformat} 

I checked the code and the difference between requests and the other attributes is that MetricsRate does not register itself in the MetricsRegistry used by the dynamic MBean like for example the MetricsLongValue does.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/09 17:47;larsgeorge;HBASE-2068-0.20.patch;https://issues.apache.org/jira/secure/attachment/12429145/HBASE-2068-0.20.patch","02/Jan/10 18:28;ghelmling;HBASE-2068-2.patch;https://issues.apache.org/jira/secure/attachment/12429278/HBASE-2068-2.patch","02/Jan/10 18:25;ghelmling;HBASE-2068-2_0.20.patch;https://issues.apache.org/jira/secure/attachment/12429277/HBASE-2068-2_0.20.patch","30/Dec/09 17:47;larsgeorge;HBASE-2068.patch;https://issues.apache.org/jira/secure/attachment/12429144/HBASE-2068.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26128,Reviewed,,,,Mon Jan 04 21:02:15 UTC 2010,,,,,,,,,,"0|i08tgv:",49374,"Converts MasterStatistics ""cluster_requests"" attribute to a MetricsRate value (computed as requests / sec).  Also adds a ""requests"" attribute to RegionServerStatistics MBean, also as a MetricsRate value.",,,,,,,,,,,,,,,,,,,,"22/Dec/09 14:52;larsgeorge;For this to work MetricsRate has to implement MetricsBase, just like MetricsLongValue etc. does. During creation of the MBean the MetricsDynamicMBeanBase iterates over the MetricsRegistry and declares the attributes dynamically. That is where the ""request"" falls off the plate. This only concerns JMX btw. as using the file or Ganglia  based context employs the MetricsRecord directly.;;;","22/Dec/09 20:33;stack;@Gary Any comment on above?  Is it an oversight in the jmx hookup work?;;;","29/Dec/09 09:45;larsgeorge;Another inconsistency is that MasterMetrics uses a MetricsIntValue 

{code}
  /*
   * Count of requests to the cluster since last call to metrics update
   */
  private final MetricsIntValue cluster_requests =
    new MetricsIntValue(""cluster_requests"", registry);
{code}

and the RegionsServerMetrics uses a MetricsRate class

{code}
  /*
   * Count of requests to the regionservers since last call to metrics update
   */
    private final MetricsRate requests = new MetricsRate(""requests"");
{code}

I suggest after fixing MetricsRate to replace the MasterMetrics one with it as well.

I am also wondering if it would have been better to call it MetricsIntRate to indicate the size of the internal counter.;;;","30/Dec/09 17:47;larsgeorge;Patch adds MetricsRate support for adding itself to the MetricsRegistry and also exchanged the MetricsIntValue in MasterMetrics for the MetricsRate class.;;;","30/Dec/09 18:29;jdcryans;Looks good, committed to trunk and branch.;;;","30/Dec/09 20:56;ghelmling;It looks like MetricsRate is not going to work with the Hadoop dynamic MBean support for JMX

Trying out this patch, I get this message in logs:
2009-12-30 14:43:45,010 ERROR org.apache.hadoop.metrics.MetricsUtil: unknown metrics type: org.apache.hadoop.hbase.metrics.MetricsRate

and the requests attributes do not show up in JConsole for the RS or master stats.  Traced the log message back to org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase:

{code}
    for (MetricsBase o : metricsRegistry.getMetricsList()) {

      if (MetricsTimeVaryingRate.class.isInstance(o)) {
      ...
      }  else if ( MetricsIntValue.class.isInstance(o) || MetricsTimeVaryingInt.class.isInstance(o) ) {
      ...
      } else if ( MetricsLongValue.class.isInstance(o) || MetricsTimeVaryingLong.class.isInstance(o) ) {
      ...
      } else {
        MetricsUtil.LOG.error(""unknown metrics type: "" + o.getClass().getName());
      }
{code}

So for this to work with JMX, we'll need to extend MetricsDynamicMBeanBase with another class that understands MetricsRate.  I'll pull together a patch for that.;;;","30/Dec/09 21:42;larsgeorge;Thanks Gary! Another option would be to simply use the same MetricsIntValue plus reset to zero as the MasterMetrics employed. That does work with what Hadoop has now.;;;","30/Dec/09 21:43;larsgeorge;Reopened to address issue found by Gary. As this can be fixed quite quickly one way or the other I reopened this issue.;;;","02/Jan/10 17:55;ghelmling;This patch adds JMX support for MetricsRate instances, by adding a MetricsMBeanBase class which understands HBase metrics classes.;;;","02/Jan/10 17:57;ghelmling;Patch adding MetricsMBeanBase to support exporting MetricsRate under JMX.;;;","02/Jan/10 18:25;ghelmling;Previous patch was missing the new MetricsMBeanBase class and unit test;;;","02/Jan/10 18:28;ghelmling;Patch against trunk adding MetricsMBeanBase and unit test;;;","02/Jan/10 23:12;larsgeorge;Fantastic Gary, I give that a shot asap and report back. ;;;","04/Jan/10 19:41;larsgeorge;+1

Works great! There are few now obsolete imports, like

{code}
-import org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase;
{code}

in the changed Statistics classes. Could remove on commit.

Otherwise please commit.;;;","04/Jan/10 21:02;stack;Committed.  Thanks for the patches lads.   I removed the unused import.  Added a couple of licenses too.   Excellent.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Perf: parallelize puts,HBASE-2066,12443964,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,22/Dec/09 05:09,20/Nov/15 13:01,01/Jul/25 07:49,22/Mar/10 23:39,0.20.2,0.20.3,,,,0.90.0,,,,,,,0,"Right now with large region count tables, the write buffer is not efficient.  This is because we issue potentially N RPCs, where N is the # of regions in the table.  When N gets large (lets say 1200+) things become sloowwwww.

Instead if we batch things up using a different RPC and use thread pools, we could see higher performance!

This requires a RPC change...",,,,,,,,,,,,,,,,,,,,,,,HBASE-1845,,,,,,"21/Mar/10 04:23;ryanobjc;HBASE-2066-20-branch.txt;https://issues.apache.org/jira/secure/attachment/12439394/HBASE-2066-20-branch.txt","12/Feb/10 02:17;ryanobjc;HBASE-2066-3.patch;https://issues.apache.org/jira/secure/attachment/12435650/HBASE-2066-3.patch","22/Dec/09 05:28;ryanobjc;HBASE-2066-branch.patch;https://issues.apache.org/jira/secure/attachment/12428693/HBASE-2066-branch.patch","25/Jan/10 23:33;ryanobjc;HBASE-2066-v2.patch;https://issues.apache.org/jira/secure/attachment/12431378/HBASE-2066-v2.patch","09/Feb/10 19:31;clehene;TestBatchPut.java;https://issues.apache.org/jira/secure/attachment/12435331/TestBatchPut.java",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26127,,,,,Fri Nov 20 13:01:36 UTC 2015,,,,,,,,,,"0|i0hgbj:",99906,,,,,,,,,,,,,,,,,,,,,"22/Dec/09 05:28;ryanobjc;include rpc version bump :-(;;;","22/Dec/09 21:56;stack;Patch looks great.  

We can't do a version bump in 0.20 branch.  Adding a new method to the interface w/o version bumping doesn't work I suppose.  How about a version in 0.20 that doesn't pass ExcecutorService and a timeout and whose method is named processBatchOfRows rather processBatchOfPuts?

Any chance of some tests?  

Will this fix help in 0.20 branch?

{code}
@@ -845,8 +855,9 @@ public class HConnectionManager implements HConstants {
 
           // by nature of the map, we know that the start key has to be < 
           // otherwise it wouldn't be in the headMap. 
-          if (KeyValue.getRowComparator(tableName).compareRows(endKey, 0, endKey.length,
-              row, 0, row.length) <= 0) {
+          if (Bytes.equals(endKey, HConstants.EMPTY_END_ROW) ||
+              KeyValue.getRowComparator(tableName).compareRows(endKey, 0, endKey.length,
+              row, 0, row.length) > 0) {
             // delete any matching entry
             HRegionLocation rl =
               tableLocations.remove(matchingRegions.lastKey());
{code}

Do you want to change these:

{code}
+            LOG.debug(""Failed all from "" + request.address + "" due to ExecutionException"");
{code}

.. so the are instead:

{code}
+            LOG.debug(""Failed all from "" + request.address, e);
{code}

Is this done once, getCurrentNrHRS, in the HTable constructor?

looks really good;;;","20/Jan/10 11:30;hammer;How does this relate to HBASE-1845?;;;","20/Jan/10 22:04;ryanobjc;This is much less ambitious than HBASE-1845 and seeks to optimize the Put case only. 

One of the problems with the original HBASE-1845 patch is that it requires a new API to take advantage of it, thus requires porting code.  Furthermore there is HTable handy things like write buffering, write buffer size settings, etc, etc.  I started with the 1845 patch, and realized we also needed a way to parallelize puts in the normal API.  This is much simpler than 1845 because we don't have to line up return codes (there are no return codes for puts, just exceptions due to temporary issues).

Short: this is a drop in replacement and makes things go fast now. HBASE-1845 requires a new API.;;;","25/Jan/10 23:33;ryanobjc;this is a trunk version with test. ;;;","26/Jan/10 17:04;stack;Patch looks good.  Make sure all licenses are 2010 on commit and add some class comment to new classes saying what they do on commit.  You don't up the RPC version?  Otherwise it looks great RR.;;;","09/Feb/10 18:05;stack;Hey man, commit already!;;;","09/Feb/10 19:23;clehene;Patch fails to apply on trunk.
After manually applying chunks I got these while doing puts

EXCEPTION 1

java.lang.NullPointerException
  at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.deleteCachedLocation(HConnectionManager.java:889)
  at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfPuts(HConnectionManager.java:1413)
  at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:586)
  at org.apache.hadoop.hbase.client.HTable.put(HTable.java:471)
  at TestBatchPut$MyThread.run(TestBatchPut.java:65)


EXCEPTION 2

java.lang.NullPointerException
  at java.util.TreeMap.rotateRight(TreeMap.java:2057)
  at java.util.TreeMap.fixAfterDeletion(TreeMap.java:2217)
  at java.util.TreeMap.deleteEntry(TreeMap.java:2151)
  at java.util.TreeMap.remove(TreeMap.java:585)
  at org.apache.hadoop.hbase.util.SoftValueSortedMap.remove(SoftValueSortedMap.java:104)
  at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.deleteCachedLocation(HConnectionManager.java:897)
  at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfPuts(HConnectionManager.java:1413)
  at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:586)
  at org.apache.hadoop.hbase.client.HTable.put(HTable.java:471)
  at TestBatchPut$MyThread.run(TestBatchPut.java:65)


Also the throughput went down and the max seconds for a put went up (could be also from the hbase restart).

I'll attach the piece of code I'm using to benchmark it;;;","09/Feb/10 19:31;clehene;run TestBatchPut nr_of_threads nr_of_puts_per_call;;;","09/Feb/10 22:22;ryanobjc;looks like a basic thread concurrency problem here.

Now to the performance issues, the current code uses ONE threadpool for everyone, which is currently set to 10 threads static.  The original code used a thread pool per HTable and sized it to the number of regionservers - that is impossible to do in HCM because of chicken-and-egg bootstrap problems (the call we'd use calls HCM.<init> which calls ...).  

Maybe the threadpool should move back into HTable to support parallelism better?  With 10 worker threads for way more than 10 client threads, yeah put performance is going to nosedive.;;;","12/Feb/10 02:17;ryanobjc;here is the much awaited new version. i'll run some tests on it and then commit if things look good.;;;","12/Feb/10 03:03;ryanobjc;i ran TestBatchPut for a while and inserted 3.3GB of data w/o problems. Ended up with like 4 table splits. No more concurrent exceptions, no major slowdown... the threads got slower as my machine bogged down, but it wasnt some crazy kind of exponential slowdown originally reported. 

if there is no complaints, i'm going to commit this as-is.;;;","12/Feb/10 05:07;ryanobjc;commited to trunk;;;","20/Mar/10 07:44;ryanobjc;this will go into 0.20 branch since now we have HBASE-2219 in there;;;","20/Mar/10 07:44;ryanobjc;adding to 0.20.4;;;","21/Mar/10 04:23;ryanobjc;for the branch;;;","21/Mar/10 11:06;apurtell;Since HBASE-2066 was committed on 0.20 branch o.a.h.h.client.TestGetRowVersions is hanging. 

Before:

{noformat}
test-core:
    [mkdir] Created dir: /home/apurtell/src/Hadoop/hbase.git/build/test/logs
    [junit] Running org.apache.hadoop.hbase.client.TestGetRowVersions
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 36.048 sec
{noformat}

Now:

{noformat}
test-core:
    [mkdir] Created dir: /home/apurtell/src/Hadoop/hbase.git/build/test/logs
    [junit] Running org.apache.hadoop.hbase.client.TestGetRowVersions
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hbase.client.TestGetRowVersions FAILED (timeout)
{noformat}

TestGetRowVersions shuts down and restarts the minicluster mid test. Maybe it could just force flush instead? 

Prior to 2066 this test would exit, but I think only by luck. Now, according to jstack main() is joined to the regionserver thread, which is trying again and again to report for duty to a master thread that has gone away. Neither main nor the regionserver threads are daemon threads, so the JVM does not exit.;;;","22/Mar/10 09:02;apurtell;Ryan committed a fix to SVN which makes the testcase work again.;;;","22/Mar/10 23:39;ryanobjc;commited to branch now;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For hfileoutputformat, on timeout/failure/kill clean up half-written hfile",HBASE-2063,12443925,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,21/Dec/09 16:35,12/Oct/12 06:14,01/Jul/25 07:49,04/Mar/10 16:39,0.20.3,,,,,0.20.4,0.90.0,,,,,,0,"Below is from mailing list.  Read from bottom to top:

{code}
 I was going to write that perhaps you needed to turn mapred.reduce.tasks.speculative.execution off, but if enabling it and things work, that would seem to indicate that a our reducer first takes longer than the task timeout maximum and secondly, on failure, we should clean up the hfile.

On the first issue, you are using KeyValueSortReducer?  Are your values large?  We set reducer status every 100 values.  Maybe this is not enough?  We should set status more frequently?  If you call context setstatus more frequently, do things work w/o speculative execution?
 
On the second, HFileOutputFormat close will set the metadata on the hfile and then close it.  On kill, this code is not being called.   Let me see if can do something about that (e.g. register a shutdown hook to clean away incomplete files -- ).

Thanks,
St.Ack


On Sun, Dec 20, 2009 at 11:26 PM, ChingShen <chingshenchen@gmail.com> wrote:
I think I found a way.
I set the ""mapred.reduce.tasks.speculative.execution"" to true and output
hfiles again, then successfully load hfiles into hbase.
Is it best solution? or HFileOutputFormat bug?

Shen

On Mon, Dec 21, 2009 at 8:25 AM, ChingShen <chingshenchen@gmail.com> wrote:

> Thanks, stack.
>
> I checked this file that isn't empty. But I found that as long as the
> ""Killed Task Attempts"" > 0 in reduce phase, and run the loadtable.rb script
> to load hfiles then failed.
> How to avoid this problem?
>
> Thanks.
>
> Shen
>
>
> On Sat, Dec 19, 2009 at 3:49 AM, stack <stack@duboce.net> wrote:
>
>> Check the
>> file
>> hdfs://domU-12-31-39-09-C5-54.compute-1.internal/osm2_hfile/Level4/197894389945760574.
>>  Is it empty?  Was there an error during running of your MR job?  Perhaps
>> a
>> task failed?
>>
>> St.Ack
>>
>>
>>
>> On Thu, Dec 17, 2009 at 9:46 PM, ChingShen <chingshenchen@gmail.com>
>> wrote:
>>
>> > Hi,
>> >  I use the script loadtable.rb to load my hfiles into hbase, but I got
>> an
>> > exception as below.
>> >  Does anyone have any suggestions?
>> >
>> > 09/12/17 23:59:33 INFO loadtable: 18 read firstkey of -3.9290_52.5534
>> from
>> >
>> >
>> hdfs://domU-12-31-39-09-C5-54.compute-1.internal/osm2_hfile/Level4/1978943899457605747
>> > org/apache/hadoop/hbase/io/hfile/HFile.java:1335:in `deserialize':
>> > java.io.IOException: Trailer 'header' is wrong; does the trailer size
>> match
>> > content? (NativeException)
>> >    from org/apache/hadoop/hbase/io/hfile/HFile.java:813:in `readTrailer'
>> >    from org/apache/hadoop/hbase/io/hfile/HFile.java:758:in
>> `loadFileInfo'
>> >    from sun.reflect.GeneratedMethodAccessor7:-1:in `invoke'
>> >    from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
>> >    from java/lang/reflect/Method.java:597:in `invoke'
>> >    from org/jruby/javasupport/JavaMethod.java:298:in
>> > `invokeWithExceptionHandling'
>> >    from org/jruby/javasupport/JavaMethod.java:259:in `invoke'
>> >    from org/jruby/java/invokers/InstanceMethodInvoker.java:36:in `call'
>> >     ... 18 levels...
>> >    from org/jruby/Main.java:94:in `main'
>> >    from loadtable.rb:83:in `each'
>> >    from loadtable.rb:83
>> > Complete Java stackTrace
>> > java.io.IOException: Trailer 'header' is wrong; does the trailer size
>> match
>> > content?
>> >    at
>> >
>> >
>> org.apache.hadoop.hbase.io.hfile.HFile$FixedFileTrailer.deserialize(HFile.java:1335)
>> >    at
>> >
>> org.apache.hadoop.hbase.io.hfile.HFile$Reader.readTrailer(HFile.java:813)
>> >    at
>> >
>> org.apache.hadoop.hbase.io.hfile.HFile$Reader.loadFileInfo(HFile.java:758)
>> >    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
>> >    at
>> >
>> >
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
>> >    at java.lang.reflect.Method.invoke(Method.java:597)
>> >    at
>> >
>> >
>> org.jruby.javasupport.JavaMethod.invokeWithExceptionHandling(JavaMethod.java:298)
>> >    at org.jruby.javasupport.JavaMethod.invoke(JavaMethod.java:259)
>> >    at
>> >
>> >
>> org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:36)
>> >    at
>> > org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:70)
>> >    at loadtable.ensure_1$RUBY$__ensure___2(loadtable.rb:86)
>> >    at loadtable.block_0$RUBY$__for__(loadtable.rb:85)
>> >    at loadtableBlockCallback$block_0$RUBY$__for__xx1.call(Unknown
>> Source)
>> >    at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:102)
>> >    at org.jruby.runtime.Block.yield(Block.java:100)
>> >    at
>> org.jruby.java.proxies.ArrayJavaProxy.each(ArrayJavaProxy.java:112)
>> >    at
>> >
>> >
>> org.jruby.java.proxies.ArrayJavaProxy$i_method_0_0$RUBYINVOKER$each.call(org/jruby/java/proxies/ArrayJavaProxy$i_method_0_0$RUBYINVOKER$each.gen)
>> >    at
>> >
>> >
>> org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:263)
>> >    at
>> >
>> >
>> org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:81)
>> >    at
>> >
>> >
>> org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:96)
>> >    at loadtable.__file__(loadtable.rb:83)
>> >    at loadtable.load(loadtable.rb)
>> >    at org.jruby.Ruby.runScript(Ruby.java:577)
>> >    at org.jruby.Ruby.runNormally(Ruby.java:480)
>> >    at org.jruby.Ruby.runFromMain(Ruby.java:354)
>> >    at org.jruby.Main.run(Main.java:229)
>> >    at org.jruby.Main.run(Main.java:110)
>> >    at org.jruby.Main.main(Main.java:94)
>> >
>>
>
>
>
>


--
*****************************************************
Ching-Shen Chen
Advanced Technology Center,
Information & Communications Research Lab.
E-mail: chenchingshen@itri.org.tw
Tel:+886-3-5915542
*****************************************************

{code}",,,,,,,,,,,,,,,,,HBASE-2280,,,,,,,,,,,,"02/Mar/10 21:40;lansa;HBASE-2063-v1.patch;https://issues.apache.org/jira/secure/attachment/12437647/HBASE-2063-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26126,Reviewed,,,,Thu Mar 04 16:39:55 UTC 2010,,,,,,,,,,"0|i08stj:",49269,,,,,,,,,,,,,,,,,,,,,"02/Mar/10 21:40;lansa;attached is the patch (HBASE-2063-v1.patch) for HFileOutputFormat:
to avoid such kind of problems need to use working path instead of output path in HFileOutputFormat.RecordWriter.
so, temporary data of failed attempts will be removed and corrupted HFiles will not appear in output folder and loadtable.rb script will go without exceptions.;;;","02/Mar/10 23:42;stack;Moving into 0.20.4.;;;","02/Mar/10 23:43;stack;Ruslan: Patch looks great.  Thanks for fixing this.  Will apply soon.;;;","02/Mar/10 23:49;stack;Marking patch available;;;","03/Mar/10 00:56;chingshen;I just patched this issue, but the problem still exists.;;;","03/Mar/10 13:05;lansa;Ching-Shen: Let's double check, can you see _temporary folder in your output path during MR execution (after MR execution you will have the folder with the same name as a column of your table and _temporary folder will be removed)? if not then probably patch was not applied.;;;","04/Mar/10 07:12;chingshen;Thanks Ruslan, It works well. (I think I forgot to distribute the hbase.jar to slaves, It's my fault.);;;","04/Mar/10 16:39;stack;Commmitted branch and trunk.  Thanks for the nice patch Ruslan Salyakhov.   Thanks for verifying fix Ching-Shen Chen .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metrics documentation outdated,HBASE-2062,12443911,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,larsgeorge,larsgeorge,larsgeorge,21/Dec/09 12:03,12/Oct/12 06:14,01/Jul/25 07:49,28/Dec/09 18:37,0.20.1,0.20.2,,,,0.20.3,,,documentation,,,,0,"HBASE-1687 fixes this supposedly but during the update of the metrics.xml there seemed to be a missed deletion of the last section describing the now obsolete OPTS addition. Especially see [this note|https://issues.apache.org/jira/browse/HBASE-1687?focusedCommentId=12754381&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12754381]. Trivial fix by copying the version from trunk over.

I would also ask if we could change JMX_OPTS to HBASE_JMX_OPTS to be in line with other variables and when using this together with Edward's [scripts|http://www.jointhegrid.com/svn/hadoop-cacti-jtg/trunk/doc/INSTALL.txt] who uses HADOOP_JMX_BASE. I asked Edward if he would be OK changing it to the matching HADOOP_JMX_OPTS. I also asked him to remove the copied section about editing the ""hbase"" start script in versions 0.20.1 or later. 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/09 12:33;larsgeorge;HBASE-2062-trunk.patch;https://issues.apache.org/jira/secure/attachment/12428624/HBASE-2062-trunk.patch","21/Dec/09 12:05;larsgeorge;HBASE-2062.patch;https://issues.apache.org/jira/secure/attachment/12428623/HBASE-2062.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26125,Reviewed,,,,Mon Dec 28 18:37:32 UTC 2009,,,,,,,,,,"0|i08th3:",49375,,,,,,,,,,,,,,,,,,,,,"21/Dec/09 12:05;larsgeorge;HBASE-2062.patch removes obsolete section and changes (!) JMX_OPTS to HBASE_JMX_OPTS. If that is OK with everyone then I also make a patch for trunk with the same change. Please advise.;;;","21/Dec/09 12:33;larsgeorge;HBASE-2062-trunk.patch as discussed above.;;;","21/Dec/09 18:59;stack;+1 on patch.  Looks good to me.  I'm good too w/ moving env variable to HBASE_JMX_OPTS.;;;","21/Dec/09 19:01;stack;Marking this issue 'patch available';;;","28/Dec/09 18:37;jdcryans;Committed both patches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing closing tag in mapreduce package-info.java,HBASE-2060,12443847,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,larsgeorge,larsgeorge,larsgeorge,19/Dec/09 19:31,12/Oct/12 06:14,01/Jul/25 07:49,20/Dec/09 19:41,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,There is a closing tag missing which makes half the page appear in fixed width font.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/09 19:32;larsgeorge;HBASE-2060.patch;https://issues.apache.org/jira/secure/attachment/12428543/HBASE-2060.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26124,Reviewed,,,,Sun Dec 20 19:41:15 UTC 2009,,,,,,,,,,"0|i08t4v:",49320,,,,,,,,,,,,,,,,,,,,,"19/Dec/09 19:32;larsgeorge;Patch fixes tag. Trivial fix.;;;","20/Dec/09 19:41;apurtell;Committed to trunk and 0.20 branch. Thanks for the patch Lars!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster won't stop,HBASE-2057,12443799,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ghelmling,jdcryans,jdcryans,18/Dec/09 19:55,12/Oct/12 06:14,01/Jul/25 07:49,22/Jan/10 19:26,0.20.3,0.90.0,,,,0.20.3,0.90.0,,,,,,0,It seems that clusters on trunk have some trouble stopping. Even manually deleting the shutdown file in ZK doesn't always help. Investigate.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/10 22:11;ghelmling;HBASE-2057-2_0.20.patch;https://issues.apache.org/jira/secure/attachment/12430934/HBASE-2057-2_0.20.patch","20/Jan/10 22:29;ghelmling;HBASE-2057-2_trunk.patch;https://issues.apache.org/jira/secure/attachment/12430936/HBASE-2057-2_trunk.patch","21/Jan/10 16:35;ghelmling;HBASE-2057-3_0.20.patch;https://issues.apache.org/jira/secure/attachment/12431039/HBASE-2057-3_0.20.patch","21/Jan/10 16:41;ghelmling;HBASE-2057-3_trunk.patch;https://issues.apache.org/jira/secure/attachment/12431041/HBASE-2057-3_trunk.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26123,Reviewed,,,,Fri Jan 22 19:26:46 UTC 2010,,,,,,,,,,"0|i08tgf:",49372,,,,,,,,,,,,,,,,,,,,,"18/Dec/09 22:40;jdcryans;A first thing I will commit is if the shutdown znode exists then we should no print a huge exception when starting a Master.;;;","27/Dec/09 20:39;jdcryans;I investigated the problem of our cluster and it seems to be that, since we use JMX, when the stop master process starts it fails to bind on the JMX port and then exists. One thing that would be helping is if the first master in a cluster was watching it's own cluster state znode so I'm going to commit this from ZKMasterAddressWatcher.writeAddressToZooKeeper:

{code}
       if(this.zookeeper.writeMasterAddress(address)) {
         this.zookeeper.setClusterState(true);
+        this.zookeeper.setClusterStateWatch(this);
{code};;;","28/Dec/09 19:58;jdcryans;Searching on the intertubes, I found that Tomcat had the same problem years ago (JMX binding). They fixed it by having a special env variable for just ""start"" that isn't used on ""stop"". See http://svn.apache.org/viewvc/tomcat/tc6.0.x/trunk/bin/catalina.sh?diff_format=h&r1=558523&r2=558522&pathrev=558523

We should probably do the same. Punting to 0.20.4;;;","20/Jan/10 21:44;ghelmling;Here's a patch to the bin/hbase script, which only adds the service specific options (used for JMX among other things) when the action passed is ""start"".

This works well in my case, but would definitely appreciate more eyes.  Anyone using these env variables for other options that might be needed in the case of a ""stop""?;;;","20/Jan/10 21:50;jdcryans;Thanks Gary, looks good. I don't think those are used for anything else, I will wait until the release of 0.20.3 to commit.;;;","20/Jan/10 22:11;ghelmling;Sorry, please use this version of the patch.  This changes the final ""exec"" line from:

{code}
exec ""$JAVA"" $JAVA_HEAP_MAX $HBASE_OPTS -classpath ""$CLASSPATH"" $CLASS ""$ACTION"" ""$@""
{code}

to

{code}
exec ""$JAVA"" $JAVA_HEAP_MAX $HBASE_OPTS -classpath ""$CLASSPATH"" $CLASS $ACTION ""$@""
{code}

The quoting of $ACTION in the first version causes ""hbase shell"" to not work.;;;","20/Jan/10 22:29;ghelmling;Patch for bin/hbase script against trunk.  Same as v2 of 0.20 branch patch.;;;","21/Jan/10 16:35;ghelmling;After reading some of the dev list messages on the Thrift changes, I believe that the current patch (HBASE-2057-2) would not work well with the current Thrift server usage.  For example, starting the Thrift with either:

{code}
./bin/hbase thrift --port=1234
./bin/hbase thrift --port=1234 start
{code}

would not pass the HBASE_THRIFT_OPTS env variable to the exec line.

So here is a new, more conservative, patch that only excludes the service specific env variables if the ""stop"" argument is used.  This also drops the unnecessary ACTION variable and corresponding change to the final ""exec"" line.

Sorry for the continuous changes.  This version should be it.  Promise.;;;","21/Jan/10 16:41;ghelmling;Corresponding v3 of patch against trunk.;;;","21/Jan/10 16:45;ghelmling;Please use v3 of patches:

HBASE-2057-3_0.20.patch
HBASE-2057-3_trunk.patch

Should be safer for commands that accept extra CLI params on startup.;;;","21/Jan/10 17:55;larsgeorge;Just looked at the v3 patch and it looks good to me.

+1;;;","21/Jan/10 19:15;stack;Patch looks good to me. ;;;","22/Jan/10 19:26;jdcryans;Committed to branch and trunk. Thanks Gary!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memstore size 0 is >= than blocking -2.0g size,HBASE-2054,12443721,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,18/Dec/09 04:32,12/Oct/12 06:14,01/Jul/25 07:49,18/Dec/09 22:47,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,"On scoundrel's cluster, I made him set his MemStore size at 256MB and hbase.hregion.memstore.block.multiplier to 8 and it seems that it overruns the integer so we get stuff like :

{code}
2009-12-17 21:52:34,314 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 79 on 60020' on region text_storage,,1261107769109: memstore size 0 is >= than blocking -2.0g size
2009-12-17 21:52:44,317 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on text_storage,,1261107769109
2009-12-17 21:52:54,320 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on text_storage,,1261107769109
2009-12-17 21:53:04,323 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on text_storage,,1261107769109
2009-12-17 21:53:14,325 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on text_storage,,1261107769109
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/09 21:49;jdcryans;HBASE-2054.patch;https://issues.apache.org/jira/secure/attachment/12428488/HBASE-2054.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26122,,,,,Sun Dec 20 20:07:10 UTC 2009,,,,,,,,,,"0|i08thj:",49377,,,,,,,,,,,,,,,,,,,,,"18/Dec/09 21:49;jdcryans;Changes ints to longs for memstore-related HRegion variables. Running the tests.;;;","18/Dec/09 22:37;jdcryans;All the tests pass except for TestHeapSize which I then fixed. I will commit in branch and trunk unless objections.;;;","18/Dec/09 22:47;jdcryans;Committed to branch and trunk.;;;","20/Dec/09 20:07;jdcryans;dodger on IRC saw that I forgot the change the setter of the memstore size so I committed a fix:
{code}
-  public void setMemStoreFlushSize(int memstoreFlushSize) {
+  public void setMemStoreFlushSize(long memstoreFlushSize) {
     setValue(MEMSTORE_FLUSHSIZE_KEY,
-      Bytes.toBytes(Integer.toString(memstoreFlushSize)));
+      Bytes.toBytes(Long.toString(memstoreFlushSize)));
{code}

And to UnmodifyableHTableDescriptor.java:

{code}
-   * @see org.apache.hadoop.hbase.HTableDescriptor#setMemStoreFlushSize(int)
+   * @see org.apache.hadoop.hbase.HTableDescriptor#setMemStoreFlushSize(long)
    */
   @Override
-  public void setMemStoreFlushSize(int memstoreFlushSize) {
+  public void setMemStoreFlushSize(long memstoreFlushSize) {
     throw new UnsupportedOperationException(""HTableDescriptor is read-only"");
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup HLog binary log output ,HBASE-2049,12443451,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,davelatham,davelatham,davelatham,15/Dec/09 21:24,12/Oct/12 06:14,01/Jul/25 07:49,17/Dec/09 19:03,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,HLog still logs binary region names.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 21:25;davelatham;2049.patch;https://issues.apache.org/jira/secure/attachment/12428082/2049.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26120,,,,,Thu Dec 17 19:03:54 UTC 2009,,,,,,,,,,"0|i08tfz:",49370,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 21:25;davelatham;Patch uses Bytes.toStringBinary in 7 additional spots.;;;","17/Dec/09 19:03;stack;Committed branch and trunk.  Thanks for the patch Dave.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Small inconsistency in the ""Example API Usage""",HBASE-2048,12443327,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,tsuna,tsuna,tsuna,15/Dec/09 01:31,16/Jul/13 01:18,01/Jul/25 07:49,15/Dec/09 02:04,0.20.2,,,,,0.20.3,0.90.0,,documentation,,,,0,"The example uses ""myLittleRow"" but refers to ""myRow"" in one of the comments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 01:33;tsuna;0001-Fix-a-reference-to-a-nonexistent-variable-in-the-Exa.patch;https://issues.apache.org/jira/secure/attachment/12427992/0001-Fix-a-reference-to-a-nonexistent-variable-in-the-Exa.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26119,Reviewed,,,,Tue Dec 15 02:04:31 UTC 2009,,,,,,,,,,"0|i08t33:",49312,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 01:33;tsuna;Patch that fixes the issue.;;;","15/Dec/09 02:04;jdcryans;Committed to branch and trunk. Thanks Benoit!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Example command in the ""Getting Started"" documentation doesn't work",HBASE-2047,12443323,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,tsuna,tsuna,tsuna,15/Dec/09 00:57,12/Oct/12 06:14,01/Jul/25 07:49,15/Dec/09 01:32,0.20.2,,,,,0.20.3,0.90.0,,documentation,,,,0,"The ""put"" command listed in the example in the ""Running and Confirming Your Installation"" section doesn't work.",Irrelevant,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 01:22;tsuna;0001-HBASE-2047-Fix-an-example-command-in-the-documentati.patch;https://issues.apache.org/jira/secure/attachment/12427989/0001-HBASE-2047-Fix-an-example-command-in-the-documentati.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26118,Reviewed,,,,Tue Dec 15 01:32:33 UTC 2009,,,,,,,,,,"0|i08t9z:",49343,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 01:22;tsuna;Patch that fixes the issue.;;;","15/Dec/09 01:32;jdcryans;Committed to branch and trunk, thanks Benoit!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shell's scan broken,HBASE-2043,12443232,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,14/Dec/09 05:52,12/Oct/12 06:14,01/Jul/25 07:49,14/Dec/09 06:15,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,"From the list:

{code}
Trying to do the following:

create 'allo', {NAME=>'test'}
put 'allo', 'A-B-C', 'test:1', '1'
put 'allo', 'A-B-E', 'test:1', '1'
put 'allo', 'A-D-C', 'test:1', '1'
scan 'allo'
..3 row(s) in 0.0150 seconds
scan 'allo', {STARTROW=>'A-B'}
..0 row(s) in 0.0120 seconds
{code}

It doesn't work because of the way the columns are parsed and passed to the Scan object.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/09 05:54;jdcryans;HBASE-2043.patch;https://issues.apache.org/jira/secure/attachment/12427888/HBASE-2043.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26116,,,,,Mon Dec 14 18:41:59 UTC 2009,,,,,,,,,,"0|i08tf3:",49366,,,,,,,,,,,,,,,,,,,,,"14/Dec/09 05:54;jdcryans;Fixes the stated problem.;;;","14/Dec/09 06:15;jdcryans;I tried the patch with a couple of different queries and I didn't see any regression. Also Olexiy tried my patch and it fixed his problem.;;;","14/Dec/09 10:16;larsgeorge;Sweet JD! I had someone here from the area reporting a similar issue. I sent him the link to this issue and asked him to verify if it fixes his issue too.;;;","14/Dec/09 18:41;stack;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Binary values are formatted wrong in shell,HBASE-2035,12442952,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,stack,davelatham,davelatham,10/Dec/09 04:17,12/Oct/12 06:14,01/Jul/25 07:49,07/Jan/10 06:15,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,"Binary values in the shell don't seem to be formatted correctly.  For example:

{code}
hbase(main):007:0> put 't1', 'r1', 'f1:q1', ""\x91"", 1000
0 row(s) in 0.0160 seconds

hbase(main):008:0> scan 't1'
ROW                          COLUMN+CELL
 r1                          column=f1:q1, timestamp=1260417826655, value=\357\277\275
1 row(s) in 0.1090 seconds
{code}

In this case we insert a single byte (double quotes needed for it to interpret the hex value correctly), but when formatted, it appears as 3 bytes in octal.

The same thing happens when the data is inserted via the Java api.  For example, this code:
{code}
HTableDescriptor tableDesc = new HTableDescriptor(""t2"");
tableDesc.addFamily(new HColumnDescriptor(""f1""));
HBaseAdmin admin = new HBaseAdmin(new HBaseConfiguration());
admin.createTable(tableDesc);
HTable table = new HTable(""t2"");
Put put = new Put(Bytes.toBytes(""r1""));
put.add(Bytes.toBytes(""f1""), Bytes.toBytes(""q1""), new byte[] {(byte) 0x91});
table.put(put);
Result result = table.get(new Get(Bytes.toBytes(""r1"")));
System.out.println(Bytes.toStringBinary(result.raw()[0].getValue()));
{code}
Prints out {{\x91}}
And then accessing via shell gives:
{code}
hbase(main):009:0> scan 't2'
ROW                          COLUMN+CELL
 r1                          column=f1:q1, timestamp=1260418531959, value=\357\277\275
1 row(s) in 0.1100 seconds
{code}
",,,,,,,,,,,,,,,,,,,,,,,HBASE-1611,,,,,,"07/Jan/10 05:58;stack;2035-0.20.patch;https://issues.apache.org/jira/secure/attachment/12429628/2035-0.20.patch","07/Jan/10 02:24;stack;2035.patch;https://issues.apache.org/jira/secure/attachment/12429614/2035.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26112,,,,,Thu Jan 07 06:15:55 UTC 2010,,,,,,,,,,"0|i08t3z:",49316,,,,,,,,,,,,,,,,,,,,,"28/Dec/09 23:08;stack;I've seen this.  Its annoying and presents difficultly working binary keys.;;;","31/Dec/09 00:11;stack;Any byte that is > 0x79 does the weird octal 3-byte thingy.  Looking at outputing using hbase's Bytes.toStringBinary....;;;","07/Jan/10 02:24;stack;Patch that makes the shell do binary as hex.  I tried Dave's combos above and it seems to do right thing.  I'll just commit.;;;","07/Jan/10 05:58;stack;Patch is different for branch.   Problem is made worse by the makeColumnName ruby method.;;;","07/Jan/10 06:15;stack;Committed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HConnectionManager.HBASE_INSTANCES leaks TableServers,HBASE-2027,12442297,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,davelatham,davelatham,davelatham,03/Dec/09 01:27,12/Oct/12 06:14,01/Jul/25 07:49,08/Dec/09 00:25,0.20.0,,,,,0.20.3,0.90.0,,,,,,0,"HConnectionManager.HBASE_INSTANCES is a WeakHashMap from HBaseConfiguration to TableServers.  However, each TableServers has a strong reference back to the HBaseConfiguration key so they are never freed.  (See note at http://java.sun.com/javase/6/docs/api/java/util/WeakHashMap.html : ""Implementation note: The value objects in a WeakHashMap are held by ordinary strong references. Thus care should be taken to ensure that value objects do not strongly refer to their own keys, either directly or indirectly, since that will prevent the keys from being discarded."")

Moreover, HBaseConfiguration implements hashCode() but not equals() so identical HBaseConfiguration objects each get their own TableServers object.

We had a long running HBase client process that was creating new HTable() objects, each creating a new HBaseConfiguration() and thus a new TableServers object.  It eventually went OOM, and gave a heap dump indicating 360 MB of data retained by HBASE_INSTANCES.",,,,,,,,,,,,,,,HBASE-2925,,,,,,,,HBASE-1251,HBASE-1976,,,,,"07/Dec/09 17:49;davelatham;2027-LRU.patch;https://issues.apache.org/jira/secure/attachment/12427204/2027-LRU.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26108,,,,,Tue Dec 08 00:25:54 UTC 2009,,,,,,,,,,"0|i08t9b:",49340,,,,,,,,,,,,,,,,,,,,,"03/Dec/09 01:30;davelatham;Here's a patch for 0.20 that changes the WeakHashMap to use WeakReference's to the TableServers for values and adds an equals method to HBaseConfiguration so that identical configs will share the same TableServers instead of each getting their own.  Also corrects the javadoc for HBaseConfiguration.hashCode().;;;","03/Dec/09 04:08;davelatham;This patch should apply to both trunk and branch.;;;","03/Dec/09 15:41;davelatham;I spent some more time thinking about this issue, and reading through the background HBASE-1251 which set it up.

The cache of connections as it stands now is never reduced or freed due to the strong reference each TableServers holds back to the HBaseConfiguration key in the HBASE_INSTANCES map.  However, with the first patch I provided, if client code keeps a strong reference to an HBaseConfiguration but not to the connection itself, then the connection information may be freed even if the HBaseConfiguration object is still around.  This is not desirable either.

Another possibility would be to convert the reference the TableServers holds to the HBaseConfiguration to a WeakReference instead of converting the HBASE_INSTANCES value to holding a WeakReference to the TableServers.  However, this also presents problems because then if the client held a strong reference to the HConnection but not to the HBaseConfiguration (less likely than the earlier case, I believe) then the configuration reference could be freed and methods that require it would fail.

I propose another simpler method, to get rid of the WeakHashMap / WeakReferences entirely and make the HBASE_INSTANCES map a simple LRU cache of the last 10 HBaseConfiguration to HConnection instances.  This will bound the amount of memory used up by the cache (better than the current implementation) at the risk of hbase clients who need to repeatedly use more than 10 different hbase configurations losing cached connections (I believe this to be very rare, but please enlighten me if I am mistaken.)

I'm attaching a new patch with this solution.  Do you think it would be better to make the size of this cache a config option instead of hardcoding it?;;;","03/Dec/09 20:48;stack;Your simpler bounded count of instances sounds good to me.  Ten should be more than enough.  You could make it configurable without adding the config. to hbase-default.xml; fellas would have to read the code to figure that they could change the value.   Its a static context though when this stuff is being set.  I'd say the patch is good enough as is.  Unless you object, will commit. 

You know, set the max to 31.  ZK maximum connections from a host is default 30.  This way, users will hardly ever hit this hard-coded max.  They'll get complaint from zk instead (And 30 instances is still small potatoes).

Good on you Dave.;;;","03/Dec/09 21:00;davelatham;I just noticed HBASE-1976 which is definitely related and the comments there.  There might be a better key for HBASE_INSTANCES than using the entire HBaseConfiguration.  Why don't you hold off on the commit until I (or someone else) gives that a bit more thought.;;;","05/Dec/09 21:11;stack;@Dave Grand.  My uninformed notion is that the way this HBASE_INSTANCES stuff works is broke.  A static map will always be problematic.  I can understand that in the one JVM you might want differently configured connections but keying with the HBC instance seems off.  Table name might be better but then what if you wanted differently configured connections to the same table?  Would that even make sense?  Thanks for looking into this.;;;","07/Dec/09 17:49;davelatham;Updated patch to set the max cached HConnection instances to 31.

I gave it a bit more thought, and I'm not sure what the best way would look like to track and support different connection sets is, so I propose we go with this patch as a definite improvement over the current set up and leave further improvement to future work.;;;","08/Dec/09 00:25;stack;Agreed about new issue to fix this properly (Can be part of client rewrite when we take on another RPC, one that is non-blocking nio rather than handler-based hadoop RPC).

Committed to branch and trunk.;;;","08/Dec/09 00:25;stack;Thanks for the patch Dave.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in StoreScanner on compaction,HBASE-2026,12442278,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,02/Dec/09 22:53,12/Oct/12 06:14,01/Jul/25 07:49,28/Dec/09 18:43,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,"From Zhenyu:

{code}
2009-12-01 00:35:05,321 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region ip_info_238,41.214.148.221,1259132082707
2009-12-01 00:35:05,572 WARN org.apache.hadoop.hbase.regionserver.Store: Not in setorg.apache.hadoop.hbase.regionserver.StoreScanner@7f821a6c
2009-12-01 00:35:05,572 WARN org.apache.hadoop.hbase.regionserver.Store: Not in setorg.apache.hadoop.hbase.regionserver.StoreScanner@7f821a6c
2009-12-01 00:35:05,572 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region ip_info_238,41.214.148.221,1259132082707
java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:250)
	at org.apache.hadoop.hbase.regionserver.Store.notifyChangedReadersObservers(Store.java:628)
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/09 18:32;jdcryans;HBASE-2026.patch;https://issues.apache.org/jira/secure/attachment/12427618/HBASE-2026.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26107,,,,,Mon Dec 28 18:43:06 UTC 2009,,,,,,,,,,"0|i08tbb:",49349,,,,,,,,,,,,,,,,,,,,,"03/Dec/09 05:41;stack;Here is updateReaders with line numbers from 0.20:

{code}
232   // Implementation of ChangedReadersObserver
233   public synchronized void updateReaders() throws IOException {
234     if (this.closing.get()) return;
235     KeyValue topKey = this.peek();
236     if (topKey == null) return;
237     List<KeyValueScanner> scanners = getScanners();
238 
239     // Seek all scanners to the initial key
240     for(KeyValueScanner scanner : scanners) {
241       scanner.seek(topKey);
242     }
243 
244     // Combine all seeked scanners with a heap
245     heap = new KeyValueHeap(
246         scanners.toArray(new KeyValueScanner[scanners.size()]), store.comparator);
247 
248     // Reset the state of the Query Matcher and set to top row
249     matcher.reset();
250     matcher.setRow(heap.peek().getRow());
251   }
{code}

heap.peek is returning null?

;;;","03/Dec/09 05:42;stack;I see two instances of this exception in the Zhenyu logs.;;;","03/Dec/09 19:00;jdcryans;From the peek() javadoc:

{quote}
Retrieves, but does not remove, the head of this queue, or returns null if this queue is empty. 
{quote}

So this is dangerous.;;;","03/Dec/09 20:00;stack;So easy fix?  Weird it has taken this long to show.  Good stuff;;;","03/Dec/09 20:12;jdcryans;Easy fix I'm not sure... We could check for null and then not set the row but I'm not sure what's going to be the resulting behavior. I'm also not even sure how we can get into that state, I'll try to write a unit test.;;;","03/Dec/09 20:31;stack;I see. If the heap has nothing in it, then matcher should be set to the end?  Or start?   There are no Readers?;;;","04/Dec/09 22:35;streamy;If the KVHeap returns null that means the scanners are done.  They all get seeked to whichever the last key was before the reader update, which could be no more keys.

Should check for null from the heap, and if null then can set the row to whatever is in topKey.  I think the scanner consumers will know what to do from there.  Definitely needs unit test.

Can look at patches but don't have any time to write one.;;;","10/Dec/09 18:32;jdcryans;You mean something like this Jon?;;;","28/Dec/09 18:43;jdcryans;I'm committing my patch to both branch and trunk as it at least won't throw an NPE. If someone is ever able to unit test it, that would be great, because I wasn't able to.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client sync block can cause 1 thread of a multi-threaded client to block all others,HBASE-2023,12442153,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,karthik.ranga,ryanobjc,ryanobjc,01/Dec/09 22:21,12/Oct/12 06:14,01/Jul/25 07:49,12/Mar/10 07:24,0.20.2,,,,,0.20.4,0.90.0,,,,,,0,"Take a highly multithreaded client, processing a few thousand requests a second.  If a table goes offline, one thread will get stuck in ""locateRegionInMeta"" which is located inside the following sync block:

        synchronized(userRegionLock){
          return locateRegionInMeta(META_TABLE_NAME, tableName, row, useCache);
        }

So when other threads need to find a region (EVEN IF ITS CACHED!!!) it will encounter this sync and wait. 

This can become an issue on a busy thrift server (where I first noticed the problem), one region offline can prevent access to all other regions!

Potential solution: narrow this lock, or perhaps just get rid of it completely.",,,,,,,,,,,,,,,,,,,,,,,HBASE-2458,,,,,,"11/Mar/10 19:37;karthik.ranga;HBASE-2023_0.20.3.patch;https://issues.apache.org/jira/secure/attachment/12438529/HBASE-2023_0.20.3.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26104,Reviewed,,,,Fri Mar 12 07:24:05 UTC 2010,,,,,,,,,,"0|i08svb:",49277,,,,,,,,,,,,,,,,,,,,,"09/Dec/09 16:55;jsensarma;i am looking at some pauses while loading data in and trying to figure out if this is applicable. we have multiple machines loading data - each multithreaded - each thread writing to a different range. all get paused at the same times once in a while. there's no cpu/io going on the region servers when this happens. (next time i reproduce - i will get a jstack dump on the regionservers).

can this happen on region splits? (I sure wasn't going any table offline/online during the test).;;;","09/Dec/09 17:10;jdcryans;Yes region splits (takes at least 6 seconds in the pre-0.21 architecture) will generate NotServingRegionException from the RS that was holding the parent of the split so if 1 out of 10 threads (so in the same JVM) goes to write to that location then it will block all threads for that time.;;;","09/Dec/09 17:57;jsensarma;ok - couple of follow on questions:

- would u advise 0.21/trunk for testing instead?
- we haven't run ZK on separate nodes yet. i just wanted to confirm whether that could be exacerbating this problem.;;;","09/Dec/09 18:09;jdcryans;Well the master rewrite code isn't in 0.21 yet, currently the main advantage in trunk is HDFS-265. WRT ZK, as long as you make sure that the quorum members aren't IO starved (eg have their own disk) and there's no swap then you should be good.;;;","10/Dec/09 02:11;jdcryans;So for this issue I see some kind of trade-off. 

 - If all threads synchronize before the method, stuff in cache won't be picked up until another thread is done looking for a another row. On the plus side, that thread waiting in line could be needing the new location that will be put in the cache by the thread holding the lock.

 - If the synchronize is more narrow eg after looking up the cache, the threads won't be blocked but some threads looking for a location in .META. could be looking for the same row and yet will all go through that code.

 - If no synchronization, it's like the previous situation but all threads will query .META. around the same time.

I don't like putting more load on .META. and I don't like having clients waiting sometimes for nothing.;;;","10/Dec/09 18:35;stack;Can you add line numbers from code to your comments above so can follow along with your comments please J-D?  Thanks.

;;;","10/Dec/09 18:47;jdcryans;Ok so in the same order:

 - This is the current situation, synchronization at line 613 of HCM.
 - Narrowing down the sync block we could put it at line 637 and cover the rest of the locateRegionInMeta method.
 - Removing the sync means getting rid of synchronized at line 613.


;;;","30/Jan/10 03:08;jdcryans;Maybe a low hanging fruit would be to narrow the synchronize on the table level:

{code}
synchronized(getTableLocations(tableName)){
  return locateRegionInMeta(META_TABLE_NAME, tableName, row, useCache);
}
{code}

This way you can even disable a table without stopping all request from coming in.;;;","30/Jan/10 04:40;stack;I wonder if it'd be possible to do a mock regionserver implemenation and then do a test that had thousands of clients in the one jvm?  The mock would then on a period do a hold on the lookup to locateRegionInMeta.  See how much it effects other threads?;;;","25/Feb/10 01:32;karthik.ranga;Kannan and I took a look at this issue and came up with yet another possibility in addition to the 3 JD mentioned:

Move the synchronized block inside the try catch loop just around the getClosestRowBefore() call. This causes each thread to give up the lock before sleeping to retry. This allows other threads to make a call in case one particular region was offline. In addition, if useCache is true, we can look at the cache and return the region right away without ever entering the synchronized section. So the new workflow in  locateRegionInMeta() will look as follows:

1. If useCache is true and the region is in the cache, return the region. If not, We have to make a remote call. 
2. for the number of retries
3.   wait for lock
4.   check cache again (someone could have filled the cache while we were waiting). Return if found.
5.   make the remote call
6.   release lock
7.   return on success, otherwise usual error handling/sleep, goto 2

I can work on the fix if this sounds good to you guys.
;;;","25/Feb/10 01:49;jdcryans;@Karthik

That sounds good, only one client hitting the META/ROOT region at a time while not blocking others for seconds.;;;","11/Mar/10 19:37;karthik.ranga;I have moved the synchronized block inside the try catch loop just around the getClosestRowBefore() call. This causes each thread to give up the lock before sleeping to retry. This allows other threads to make a call in case one particular region was offline. In addition, if useCache is true, we can look at the cache and return the region right away without ever entering the synchronized section. So the new workflow in locateRegionInMeta() will look as follows:

1. If useCache is true and the region is in the cache, return the region. If not, We have to make a remote call.
2. for the number of retries
3. wait for lock
4. check cache again (someone could have filled the cache while we were waiting). Return if found.
5. make the remote call
6. release lock
7. return on success, otherwise usual error handling/sleep, goto 2
;;;","11/Mar/10 19:39;karthik.ranga;Hey guys,

I have uploaded the patch, the tests were passing and I did some manual testing as well. Please let me know if you have any comments/suggestions.

thanks
Karthik;;;","12/Mar/10 07:15;stack;Karthik.  Patch looks clean to me.  I tried it here with some small loadings.  Seems fine for variety of smile tests.  I'm going to commit.  If issues, they'll only show in bigger tests.  We can catch them then.

;;;","12/Mar/10 07:24;stack;Committed branch and trunk.  Thanks for the patch Karthik (I added you as contributor if you don't mind).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in housekeeping kills RS,HBASE-2022,12442126,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,01/Dec/09 18:28,12/Oct/12 06:14,01/Jul/25 07:49,07/Dec/09 23:48,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,"Saw this on Zhenyu's 0.20.1 cluster (which for some weird reason seems to have many issues):

{code}
2009-11-30 16:44:48,170 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: Unhandled exception. Aborting...
java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.housekeeping(HRegionServer.java:1280)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:590)
	at java.lang.Thread.run(Thread.java:619)
{code}

This reminds me of HBASE-1386 and in fact this could be the same issue (but I can't confirm). Searching on the web gives me some hits and this is particularly interesting http://forums.sun.com/thread.jspa?threadID=5379669",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/09 00:46;jdcryans;HBASE-2022-v2.patch;https://issues.apache.org/jira/secure/attachment/12427004/HBASE-2022-v2.patch","01/Dec/09 18:50;jdcryans;HBASE-2022.patch;https://issues.apache.org/jira/secure/attachment/12426562/HBASE-2022.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26103,Reviewed,,,,Mon Dec 07 23:48:45 UTC 2009,,,,,,,,,,"0|i08t7r:",49333,,,,,,,,,,,,,,,,,,,,,"01/Dec/09 18:41;jdcryans;It seems that in HRS.Worker.run() we already handle that case:

{code}

e = toDo.poll(threadWakeFrequency, TimeUnit.MILLISECONDS);
if(e == null || stopRequested.get()) {
  continue;
}

{code}

Then we should just check for null in housekeeping.;;;","01/Dec/09 18:50;jdcryans;Patch that breaks from the loop if an element is null. 

I'm beginning to wonder if housekeeping is really useful since the Worker processes stuff and not the main HRS loop (where we call housekeeping).;;;","02/Dec/09 00:00;stack;Patch seems fine.

Reading the forums note you post, the reporter says that once it returns null once, it does so ever after.  I wonder if that will happen here?  Perhaps log it if we get a null out the linkedlist?  Do it here and in HRS.Worker.run since its not supposed to happen.

Could it be a synchronization issue?  I haven't looked?  Maybe the linked list needs synchronizing?  All access to the list?;;;","02/Dec/09 18:10;jdcryans;I guess a synchronized block would help, I'll try that and I'll add more logging.;;;","05/Dec/09 00:46;jdcryans;I just added a log that reports the null value. Adding logging in Worker makes it very chatty because when the master don't give anything to do the list is empty so we get a null (expected). Also I don't think we should synchronize every access since getting a null in housekeeping won't break anything.

Also. thinking about it, it's normal that the list gives null every time after the first one because the list is still always empty.;;;","07/Dec/09 23:35;tlipcon;patch seems fine to me - we got this error on a cluster as well.;;;","07/Dec/09 23:48;jdcryans;I committed this (Stack also told me it was ok) to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updates to .META. blocked under high MemStore load,HBASE-2018,12442002,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,30/Nov/09 22:32,12/Oct/12 06:14,01/Jul/25 07:49,04/Dec/09 04:34,0.20.2,,,,,0.20.3,0.90.0,,,,,,0,"I discovered this on Lars' cluster. The symptom was the good old:

{code}
09/11/30 08:10:26 INFO mapred.JobClient: Task Id : attempt_200911250121_0011_r_000010_1, Status : FAILED
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server Some server, retryOnlyOne=true, index=0, islastrow=false, tries=9, numtries=10, i=14, listsize=20, region=prev-docs,de68fb97795ef3d936a3f10ff8790253,1259573366564 for region prev-docs,ccea967e66ccb53d83c48849c3a23f21,1259542138868, row 'ccff8cd4ca871c41f4fa7d44cffed962', but failed after 10 attempts.
Exceptions:
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers$Batch.process(HConnectionManager.java:1120)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfRows(HConnectionManager.java:1201)
        at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:605)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:470)
        at org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordW
{code}

But the load wasn't that heavy, just lots of splitting going on. Looking at the logs, I see a split taking more than 4 minutes which is explained by this happening on the RS hosting .META. :

{code}
2009-11-30 08:08:39,922 INFO org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Forced flushing of prev-docs,2c9d51e57b20decd5c6419d23ede822b,1259542273901 because global memstore limit of 1.6g exceeded; currently 1.6g and flushing till 1021.9m
...
2009-11-30 08:12:33,743 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~22.9m for region prev-docs,c8fea4fbbc41e746d960854ed4d41dd6,1259587143838 in 14160ms, sequence id=13677, compaction requested=false
2009-11-30 08:12:33,744 INFO org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Forced flushing of prev-docs,39c2995d955c041d21f4dc4a0d0dbf6c,1259587061295 because global memstore limit of 1.6g exceeded; currently 1.0g and flushing till 1021.9m
{code}

So we should not block updates to .META. for any reason. I'm pretty sure this issue explains other issues we've seen on the mailing list.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/09 04:25;apurtell;HBASE-2018-v2.patch;https://issues.apache.org/jira/secure/attachment/12426750/HBASE-2018-v2.patch","30/Nov/09 22:39;jdcryans;HBASE-2018.patch;https://issues.apache.org/jira/secure/attachment/12426463/HBASE-2018.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26102,Reviewed,,,,Fri Dec 04 04:34:31 UTC 2009,,,,,,,,,,"0|i08t3j:",49314,,,,,,,,,,,,,,,,,,,,,"30/Nov/09 22:39;jdcryans;This patch adds a check before calling cacheFlusher.reclaimMemStoreMemory so that we don't go waiting on the synchronized method if it's a .META. update. I'm currently running the tests.;;;","01/Dec/09 01:50;apurtell;+1

Agree with the issue priority also. ;;;","01/Dec/09 03:42;jdcryans;All the tests pass. I would love if Lars could try out my patch before committing.;;;","01/Dec/09 13:07;larsgeorge;Testing now, takes a few hours to ramp up through the map phase. Results forthcoming...;;;","02/Dec/09 15:09;larsgeorge;So testing for me is not going too well. Overall the patch is not harming anything, I say +1. For my testing I am struggling apparently with a too small cluster :(;;;","03/Dec/09 04:25;apurtell;Should this check be for all operations on meta regions, deletes also? See patch -v2. ;;;","03/Dec/09 19:05;jdcryans;New patch makes sense. Also Lars just reported that his job finally was successful (by tweaking other stuff). At least I think this patch covers a very bad corner case.;;;","03/Dec/09 20:17;stack;+1 on v2 of patch.;;;","04/Dec/09 04:34;apurtell;Committed -v2 patch to trunk and 0.20 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master will lose hlog entries while splitting if region has empty oldlogfile.log,HBASE-1994,12441192,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,larsgeorge,clehene,clehene,19/Nov/09 14:22,12/Oct/12 06:14,01/Jul/25 07:49,04/Dec/09 18:38,0.90.0,,,,,0.20.3,0.90.0,,master,,,,0,"I don't know yet how an empty oldlogfile.log can exist, however it happened.
Master will fail to put the splits in the region oldlogfile.log if an empty oldlogfile.log already exists there.
This is the master log after I artificially reproduced it by placing an empty oldlogfile.log in /hbase/.META./1028785192/oldlogfile.log and then killed the regionserver that was holding the .META. table

2009-11-19 09:08:36,012 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Splitting 1 hlog(s) in hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773
2009-11-19 09:08:36,012 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Splitting hlog 1 of 1: hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773/hlog.dat.1258637493128, length=0
2009-11-19 09:08:36,019 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Adding queue for .META.,,1
2009-11-19 09:08:36,037 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Pushed=795 entries from hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773/hlog.dat.1258637493128
2009-11-19 09:08:36,038 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Thread got 795 to process
2009-11-19 09:08:36,043 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: Old hlog file hdfs://b0:9000/hbase/.META./1028785192/oldlogfile.log already exists. Copying existing file to new file
2009-11-19 09:08:36,079 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: Got while writing region .META.,,1 log java.io.EOFException
2009-11-19 09:08:36,081 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: hlog file splitting completed in 70 millis for hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773
",,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,HBASE-1364,HBASE-2358,,,,,"03/Dec/09 10:23;larsgeorge;HBASE-1994-EOFE.patch;https://issues.apache.org/jira/secure/attachment/12426771/HBASE-1994-EOFE.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26095,,,,,Fri Dec 04 18:38:28 UTC 2009,,,,,,,,,,"0|i08tgn:",49373,,,,,,,,,,,,,,,,,,,,,"19/Nov/09 14:45;larsgeorge;This is one of those events I never understood how they happen in the first place. I mean if we have a oldlogfile.log that was not yet replayed, why put another in place and not fail fast. Or use a UUID style filename for them with a known extension that the RS knows how to find and then replays them in order (so filename could be a timestamp?). Because if this can happen, it can only handle one such occurrence. What happens if that code is executed and there is already a oldlogfile.log.old in place? The code overwrites it right now - but is that what one want?

As for empty .log files we could add an easy check that if empty then remove as opposed to rename.;;;","19/Nov/09 14:57;apurtell;bq. Or use a UUID style filename for them with a known extension that the RS knows how to find and then replays them in order (so filename could be a timestamp?).

+1

bq. As for empty .log files we could add an easy check that if empty then remove as opposed to rename.

+1
;;;","19/Nov/09 15:00;stack;Agree (+1).  oldlogfile.log as name of a file of edits is dumbass.;;;","19/Nov/09 15:28;larsgeorge;This could simply follow or use the normal file name conventions for hlogs:
{code}
...
        this.filenum = System.currentTimeMillis();
        Path newPath = computeFilename(this.filenum);
...

  public Path computeFilename(final long fn) {
    if (fn < 0) return null;
    return new Path(dir, HLOG_DATFILE + fn);
  }
{code}

Just with the known extension. Then in RS and Store classes change

{code}
  /** Name of old log file for reconstruction */
  static final String HREGION_OLDLOGFILE_NAME = ""oldlogfile.log"";
{code}

to be a extension only and not pass this on along the Store initialization but scan the FS for such file at the right spot in the code. Seems cleaner too, as it removes at least one parameter.;;;","19/Nov/09 16:10;larsgeorge;From IRC

{code}
<larsgeorge> the write path is in the thread
<clehene> it's wap
<larsgeorge> yeah
<larsgeorge> urgh
<larsgeorge> it also only catches IOException
<larsgeorge> I only know from experience that uncaught exceptions in threads rarely get logged
<larsgeorge> and if then not proper as the stack is different
<clehene> uhm... however an exception in that big try could live it empty
<larsgeorge> yes
<larsgeorge> fragile
<clehene> and next time it would split 
<clehene> it would just throw away all edits because it fails with EOF
<larsgeorge> could be
<larsgeorge> the read deletes the input
<St^Ack> anything in the .out files?
<larsgeorge> so when the write fails
<larsgeorge> tough luck?
<larsgeorge> the read part has the delete in the finally
<larsgeorge> so the input log is deleted for sure
<St^Ack> cosmin you think the master went down while it was splitting a log?
<larsgeorge> shouldn't that be done at the very end? Or in some sort of Atomic commit
<larsgeorge> as in have a big try/catch/finally and either rollback the split or apply it
<St^Ack> I think that general split state needs to be hoisted up into zk
<St^Ack> master takes out a 'lock'
<St^Ack> one that will evaporate if it dies mid-split
<larsgeorge> hmm
<larsgeorge> for a master crash?
<larsgeorge> as this is all done in master start anyways
<St^Ack> yeah
<St^Ack> an empty oldlogfile.log -- i can't stand typing the name even -- would seem to an exit w/o a call to close
{code};;;","20/Nov/09 11:00;larsgeorge;Based on the above the plan could be to use ZK to flag a split in progress and then do the process more resilient

- Flag in ZK that split is running
- Read log but do not delete it yet
- Write logs into region directories with a temp extension and a sequential filename
- Once this all succeeded delete old logs and rename split logs to have non-temp extension
- Clear ZK flag

That way if something happens the original logs are still preserved until properly split. A split dying halfway through is also OK as the temp split files can safely be removed.;;;","20/Nov/09 20:01;jdcryans;+1 

This is a good introductory work to distributed splitting. ;;;","21/Nov/09 14:33;larsgeorge;I am really torn looking at HBASE-1364. It seems there is a lot of planning already to get splits to work better. Is this bug here simply a preliminary fix to avoid the empty log files and also not using the same file names? If so it is fine. But I would like to know what the overall plan is.

Before reading HBASE-1364 I had a similar thought how I would improve on my suggestion above. I thought we could do this:

- Master sees abandoned log and sets marker in ZK for regions contained in log
- RegionServer on start up or region open checks ZK marker and reads the original file extracting the HLogKey's it needs storing them into a local file in the region (this is the distributed log split, not all RS's split here but those that need to read the logs local anyways.) Also possible to also put the data into a MemStore at the same time.
- The RS applies the log and flushes, then sets a semaphore that it completed next to the original log.
- If we have a local copy done as well as per above then the RS can delete it now
- RS clears marker in ZK for its region(s)
- Once the master sees that all RS have read the log and processed it it can delete the original log

I am probably missing some intrinsic details as I have read the HLog etc. code but did not debug it or so to see if I got all the steps right. Let me know what you think and what you want me to do here.;;;","22/Nov/09 18:57;larsgeorge;I read the BigTable paper again and found that my ""approach"" is shunted there:

{quote}
One approach would be for each new tablet server to read this full commit log file and apply just the entries needed for the tablets it needs to recover. However, under such a scheme, if 100 machines were each assigned a single tablet from a failed tablet server, then the log file would be read 100 times (once by each server).
{quote}

Makes sense, if a RS hosted 100 regions (which is often a rather low figure in what is reported by users) those will be spread across the cluster and could result in N RS's reading the file trying to find what is where. So we could keep the threaded split we have or think about simply do the sort as suggest by the BigTable paper and then have simple range reads on each RS.

{quote}
We avoid duplicating log reads by first sorting the commit log entries in order of the keys (table, row name, log sequence number). In the sorted output, all mutations for a particular tablet are contiguous and can therefore be read efficiently with one disk seek followed by a sequential read. To parallelize the sorting, we partition the log file into 64 MB segments, and sort each segment in parallel on different tablet servers. This sorting process is coordinated by the master and is initiated when a tablet server indicates that it needs to recover mutations from some commit log file.{quote}

What did you discuss during the hackathon?;;;","03/Dec/09 10:23;larsgeorge;HBASE-1994-EOFE.patch simply handles the empty oldlogfile.log case where an exception is thrown when attempting to rename the zero byte length file.;;;","03/Dec/09 10:25;larsgeorge;I would say the patch is enough to complete this issue. The discussed items above should carried over to HBASE-1364 or a new issue should be opened instead. Makes sense?;;;","03/Dec/09 19:02;jdcryans;Good stuff, this effectively covers the scope of this jira. +1;;;","03/Dec/09 20:35;stack;+1 this patch overs this issue.

In hbase-1364, lets make sure we catch stuff Lars raises above.;;;","04/Dec/09 15:03;clehene;+1 

I just tested it and got 

{code:title=hbase-master.log}
2009-12-04 09:51:28,731 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Splitting 1 hlog(s) in hdfs://b0:9000/hbase/.logs/b3,60020,1259928238561
2009-12-04 09:51:28,770 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: Old hlog file hdfs://b0:9000/hbase/.META./1028785192/oldlogfile.log is zero length. Deleting existing file
2009-12-04 09:51:28,875 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: hlog file splitting completed in 145 millis for hdfs://b0:9000/hbase/.logs/b3,60020,1259928238561  {code}

No missing edits.;;;","04/Dec/09 18:38;stack;Applied branch and trunk.  Thanks for the patch Lars.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Admin (et al.) not accurate with Column vs. Column-Family usage,HBASE-1989,12441098,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,larsfrancke,dmeil,dmeil,18/Nov/09 21:27,11/Jun/22 23:15,01/Jul/25 07:49,11/May/15 16:47,0.20.1,0.90.1,,,,2.0.0,,,Client,,,,1,"Consider the classes Admin and HColumnDescriptor.

HColumnDescriptor is really referring to a ""column family"" and not a ""column"" (i.e., family:qualifer).

Likewise, in Admin there is a method called ""addColumn"" that takes an HColumnDescriptor instance.

I labeled this a bug in the sense that it produces conceptual confusion because there is a big difference between a column and column-family in HBase and these terms should be used consistently.  The code works, though.
 

",,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2704,HBASE-13645,,,"06/May/15 12:57;larsfrancke;HBASE-1989-v1.patch;https://issues.apache.org/jira/secure/attachment/12730830/HBASE-1989-v1.patch","05/May/15 20:22;larsfrancke;HBASE-1989.patch;https://issues.apache.org/jira/secure/attachment/12730594/HBASE-1989.patch","12/Feb/10 15:48;larsfrancke;hbase1989.patch;https://issues.apache.org/jira/secure/attachment/12435703/hbase1989.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26093,Reviewed,,,,Mon May 11 20:40:19 UTC 2015,,,,,,,,,,"0|i02bin:",11467,,,,,,,,,,,,,,,,,,,,,"19/Nov/09 04:58;stack;I'll take this one. Redoing HCD any ways to go against ZK instead of as a Writable so can change it then.;;;","12/Feb/10 15:48;larsfrancke;I did a small part of this before I found this issue. I only changed HBaseAdmin - this patch deprecates the old methods and changes their usage.

But as I said: This is only a small part of the renaming process but perhaps it is useful anyway;;;","12/Feb/10 19:56;stack;This is a start.  Will I commit it as a start in on this issue, as a part 1?  I'm working on the other part over in master rewrite where HColumnDescriptor becomes instead ColumnFamilyDefinition, etc., throughout the code base.;;;","31/Mar/11 15:07;stack;Marking as patch available.  Has work started by LarsF.;;;","30/May/12 22:36;stack;Stale;;;","30/May/12 23:12;dmeil;Wow, this is from the way-back machine (2009).  But I still think it's a good idea from a terminology standpoint to clear this up in the API;;;","08/Jun/14 21:54;apurtell;Stale issue. Reopen if still relevant (if there's new activity);;;","30/Apr/15 08:21;larsfrancke;I'd like to work on this again because the naming is still confusing. I'll reopen and attach a patch that'll deprecate the ""old"" versions in 2.0.0 so they can be removed in 3.0.0.;;;","05/May/15 20:22;larsfrancke;Attaching a patch that fixes all methods in {{Admin}} and {{HBaseAdmin}}.

This does not change {{HColumnDescriptor}} as that'd take too long for now. If I have time I'll do that in another issue.;;;","05/May/15 23:04;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12730594/HBASE-1989.patch
  against master branch at commit 977f867439e960c668ee6311e47c904efc40f219.
  ATTACHMENT ID: 12730594

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 58 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1900 checkstyle errors (more than the master's current 1896 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13946//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13946//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13946//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13946//console

This message is automatically generated.;;;","06/May/15 08:59;anoop.hbase;bq.void deleteColumnFamily(final TableName tableName, final byte[] columnName) throws IOException
Can you make the param name as 'columnFamily '

HBaseAdmin.java
{quote}
addColumnFamily(final byte[] tableName, HColumnDescriptor columnFamily)
addColumnFamily(final String tableName, HColumnDescriptor columnFamily)
{quote}
May be no need to add newly. Just deprecate its counterparts with replacement as 
addColumnFamily(final TableName tableName, final HColumnDescriptor columnFamily)
(?)
Same case applicable for delete/modify;;;","06/May/15 12:57;larsfrancke;Thanks for taking a look and the comments.

I've attached v1 of the patch that should fix the Checkstyle warning and addresses your comments. I've renamed all parameters I could find in those two classes to {{columnFamily}} and I've removed those extra methods I added.;;;","06/May/15 14:55;anoop.hbase;Latest patch LGTM;;;","06/May/15 16:32;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12730830/HBASE-1989-v1.patch
  against master branch at commit 652929c0ff8c8cec1e86ded834f3e770422b2ace.
  ATTACHMENT ID: 12730830

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 58 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13962//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13962//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13962//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13962//console

This message is automatically generated.;;;","09/May/15 08:23;larsfrancke;[~anoop.hbase] Thanks for taking a look. Jenkins agrees with you. I think this is good to commit if there are no further comments.;;;","11/May/15 03:48;stack;Pushed to master. Thanks [~lars_francke] Nice.;;;","11/May/15 05:38;busbey;this got pushed without a proper commit message (missing jira reference). any objection to revert + reapply with commit?;;;","11/May/15 06:12;larsfrancke;Thanks stack!

[~busbey] I do not object, thank you for catching this.;;;","11/May/15 16:47;stack;Reverted because of bad commit message (Thanks again [~busbey]) and then reapplied.;;;","11/May/15 20:40;hudson;FAILURE: Integrated in HBase-TRUNK #6473 (See [https://builds.apache.org/job/HBase-TRUNK/6473/])
HBASE-1989 Admin (et al.) not accurate with Column vs. Column-Family (stack: rev ec51d7b2e600fab2c3490e794dd13e27ec1cee01)
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestTableDescriptorModification.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotCloneIndependence.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java
* hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithEncryption.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java
* hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestTableDeleteFamilyHandler.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestLoadAndSwitchEncodeOnDisk.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MurmurHash does not yield the same results as the reference C++ implementation when size % 4 >= 2,HBASE-1979,12440608,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,pichenettes,pichenettes,pichenettes,13/Nov/09 11:47,12/Oct/12 06:14,01/Jul/25 07:49,24/Nov/09 19:29,0.20.1,,,,,0.20.3,0.90.0,,util,,,,0,"Last rounds of MurmurHash are done in reverse order. data[length - 3], data[length - 2] and data[length - 1] in the block processing the remaining bytes should be data[len_m +2], data[len_m + 1], data[len_m].
",,3600,3600,,0%,3600,3600,,,,,,,,HADOOP-6372,,,,,,,,,,,,,,"13/Nov/09 12:38;pichenettes;HBASE-1979.patch;https://issues.apache.org/jira/secure/attachment/12424840/HBASE-1979.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26091,Reviewed,,,,Tue Nov 24 19:29:42 UTC 2009,,,,,,,,,,"0|i08t4f:",49318,,,,,,,,,,,,,,,,,,,,,"13/Nov/09 13:02;apurtell;Committed to trunk. Tests running now. Will back out if any failure. Commit to branch pending open of 0.20.3. Thanks for the patch Olivier!;;;","13/Nov/09 13:10;ab;Please note that this may cause a back-compat issue, as existing stored bloom filters will break (hash values are different after this patch, and this affects how bloom filters are populated / tested).;;;","13/Nov/09 13:47;apurtell;Thanks for the note Andrzej. HBase from 0.20.0 and up does not currently use bloom filters. ;;;","14/Nov/09 00:39;ryanobjc;there are no bloom filters in any installations right now AFAIK.

Does this change the basic murmur hash function?  Because it might break the encoded directory names of hbase!;;;","14/Nov/09 00:46;ryanobjc;furthermore, we are going to have to keep both implementations so we can provide a migration path as we will need to restructure and rewrite the entire hbase data tree.;;;","14/Nov/09 01:03;pichenettes;The only use of the hash(...) function I found is in HRegionInfo.encodeRegionName (ryan, is that what you referred to?), but it seems to explicitly use JenkinsHash instead of Murmur.;;;","14/Nov/09 13:34;apurtell;I checked references before making the commit on trunk. HRI.encodeRegionName explicitly uses JenkinsHash. I didn't see any generic use of Hash.hash(), or Hash.getInstance(). The only user of MurmurHash is PE at the moment. ;;;","14/Nov/09 16:44;stack;OK.  Sounds good then.  Safe change.;;;","24/Nov/09 19:29;stack;Commited to 0.20 branch.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get-s can't set a Filter,HBASE-1957,12440006,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,theorm,theorm,06/Nov/09 08:40,12/Oct/12 06:13,01/Jul/25 07:49,06/Nov/09 19:12,0.20.0,,,,,0.20.2,0.90.0,,Filters,,,,0,"This is an issue directly related to HBASE-1646. Get#write and Get#readFields both use  HbaseObjectWritable to write filters and when it comes to custom filters or filters in general that are not hardcoded in HbaseObjectWritable , an exception is thrown. 

It has been fixed in the issue noted above for Scan. Attached patch fixes it fot Get too.",,,,,,,,,,,,,,,,HBASE-1646,,,,,,,,,,,,,"06/Nov/09 08:45;theorm;hbase-filter-serialization.patch;https://issues.apache.org/jira/secure/attachment/12424191/hbase-filter-serialization.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26080,Reviewed,,,,Fri Nov 06 19:12:27 UTC 2009,,,,,,,,,,"0|i08s4v:",49158,,,,,,,,,,,,,,,,,,,,,"06/Nov/09 19:12;stack;Thank you for the patch Roman (FYI, make patch just under $HBASE_HOME rather than at $HBASE_HOME/src/java next time -- but no worries).  Applied branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transactional scans do not see newest put.,HBASE-1954,12439778,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,clint.morgan,clint.morgan,clint.morgan,03/Nov/09 19:17,12/Oct/12 06:13,01/Jul/25 07:49,03/Nov/09 19:41,0.20.1,,,,,0.20.2,0.90.0,,,,,,0,"In a transaction, if I do a put, then a put, then a scan. I will not see the latest put.

The fix is to set the timestamp at put time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/09 19:18;clint.morgan;1954.patch;https://issues.apache.org/jira/secure/attachment/12423940/1954.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26078,Reviewed,,,,Wed Nov 04 17:06:32 UTC 2009,,,,,,,,,,"0|i08s1z:",49145,,,,,,,,,,,,,,,,,,,,,"03/Nov/09 19:18;clint.morgan;This fixes the issue. Adds a test that fails without the patch, passes with.;;;","03/Nov/09 19:41;stack;Committed branch and trunk.  Thanks for patch Clint.;;;","04/Nov/09 06:41;jdcryans;The build is unstable since this commit.;;;","04/Nov/09 17:06;clint.morgan;I saw a failure in TestTHLogRecovery in that Hudson output. Ran the test locally 2 times on both trunk and branch, and it passed all 4 times.

(I don't think this patch would mess with how TTHLR is testing)

Is something else failing? Is there a way I can reproduce?

That TTHLR does need some work, and I'm planning on redoing in 0.21 after the core WAL stuff has settled down.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack overflow when calling HTable.checkAndPut() when deleting a lot of values,HBASE-1951,12439669,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,agemooij,agemooij,02/Nov/09 15:09,12/Oct/12 06:13,01/Jul/25 07:49,04/Nov/09 01:29,0.20.1,,,,,0.20.2,0.90.0,,regionserver,,,,0,"We get a stackoverflow when calling HTable.checkAndPut() from a map-reduce job though the client API after doing a large number of deletes.

Our mapred job is a periodic job (which extends TableMapper) that merges the versions for a value in a column into a new value/version and then deletes the older versions. This is because we use versions to store data so we can do append-only insertion. Our rows can have large/huge (from 1 till > 1M) numbers of columns (aka key-values).

The problem seems to be that the org.apache.hadoop.hbase.regionserver.GetDeleteTracker.isDeleted() method is implemented with recursion but since Java has no tail recursion optimization, this fails for cases where the number of deletes that are being tracked is bigger than the stack size. I'm not sure why recursion is used here but it is not safe without tail-call optimization and it should be optimized into a simple loop.

I'll attach the stacktrace.","Running HBase 0.20.1 on Cloudera distribution of Hadoop 0.20.1+152 on an EC2 test cluster with one master, one embedded zookeeper, and only one region server",,,,,,,,,,,,,,,,,,,,,,HBASE-1781,,,,,,"04/Nov/09 00:13;jdcryans;HBASE-1951-v2.patch;https://issues.apache.org/jira/secure/attachment/12423975/HBASE-1951-v2.patch","03/Nov/09 23:24;jdcryans;HBASE-1951.patch;https://issues.apache.org/jira/secure/attachment/12423972/HBASE-1951.patch","02/Nov/09 15:38;agemooij;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12423825/stacktrace.txt",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26077,Reviewed,,,,Wed Nov 04 15:13:06 UTC 2009,,,,,,,,,,"0|i08s53:",49159,,,,,,,,,,,,,,,,,,,,,"02/Nov/09 15:38;agemooij;Here's the stacktrace;;;","02/Nov/09 15:44;agemooij;HBASE-1781 seems to be caused by the same use of recursion where a simple loop seems the safer, although less elegant, choice. It seems that the depth of the recursion is way to deep for some cases in the wild.;;;","02/Nov/09 17:27;stack;Lets fix in 0.20.2 as well as TRUNK;;;","03/Nov/09 23:24;jdcryans;This patch adds a test to check if our implementation can cause stack overflows.;;;","04/Nov/09 00:13;jdcryans;Patch that adds fix, running tests at the moment. It already passes TestGetDeleteTracker;;;","04/Nov/09 01:20;stack;Looks good +1.  I like the test.   You going to fix HBASE-1781 too?;;;","04/Nov/09 01:29;jdcryans;Committed to branch and trunk. Thanks for the review Stack!;;;","04/Nov/09 15:13;agemooij;Thanks for the quick response !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyValue expiration by Time-to-Live during major compaction is broken,HBASE-1949,12439577,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ghelmling,ghelmling,ghelmling,30/Oct/09 22:17,12/Oct/12 06:13,01/Jul/25 07:49,05/Nov/09 21:52,0.20.1,,,,,0.20.2,0.90.0,,regionserver,,,,0,"During a major compaction on a region in a column family with a configured TTL, it looks like all KeyValues in a row after the first expired KeyValue are skipping and thrown out of the newly written file (regardless of whether the would have been expired or not).

The StoreScanner is skipping to the next row, even when other columns with a non-expirable timestamp exists.  Unless I'm misunderstanding it, it seems like it should just seek to the next column instead.  I discovered this when altering a table to lower the TTL for a column family and force the expiration of some data which led to the entire row being expired in some instances.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/09 15:16;ghelmling;HBASE-1949-0.20.patch;https://issues.apache.org/jira/secure/attachment/12424030/HBASE-1949-0.20.patch","02/Nov/09 22:03;ghelmling;HBASE-1949-trunk.patch;https://issues.apache.org/jira/secure/attachment/12423856/HBASE-1949-trunk.patch","05/Nov/09 17:01;ghelmling;HBASE-1949-v2-0.20.patch;https://issues.apache.org/jira/secure/attachment/12424133/HBASE-1949-v2-0.20.patch","05/Nov/09 17:02;ghelmling;HBASE-1949-v2-trunk.patch;https://issues.apache.org/jira/secure/attachment/12424134/HBASE-1949-v2-trunk.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26076,Reviewed,,,,Thu Nov 05 21:52:39 UTC 2009,,,,,,,,,,"0|i08s27:",49146,Fixed expiring of individual column values within rows via the column family time-to-live configuration.  Previously all column values following the first expired value in a row would be truncated.  Though in practice this might only be seen when lowering the TTL configuration on a column family with existing data.,,,,,,,,,,,,,,,,,,,,"30/Oct/09 22:19;ghelmling;The attached patch (against the 0.20 branch) changes the ScanQueryMatcher.match() return value in this case to just seek to the next column and adds a test for this case.;;;","02/Nov/09 22:03;ghelmling;Same patch for ScanQueryMatcher.match() return value and extra unit test, applied against current trunk.;;;","04/Nov/09 15:12;ghelmling;This patch is the same as previous ttl_expire-0.20.patch, just updated to current hbase-0.20 branch and renamed for consistency.  I'll remove the other to avoid confusion.;;;","04/Nov/09 15:16;ghelmling;Okay, one more time, actually including the issue number in the filename.  JIRA really needs a ""rename file"" feature.;;;","05/Nov/09 17:01;ghelmling;The previous version of this patch missed an additional case in QueryMatcher.match() -- called from ScanFileGetScan -- which would exit early on a row for get requests when the first expired KeyValue was encountered.  This would not actually remove data (like the previous occurance) but would mask existing data from this client.

This version adds a change for the QueryMatcher.match() instance to return MatchCode.SKIP instead of MatchCode.NEXT in order to keep processing any following kvs.;;;","05/Nov/09 17:02;ghelmling;This is an updated version of the patch against trunk, which adds the fix for QueryMatcher.match() and a couple extra tests in TestQueryMatcher to confirm the fix.;;;","05/Nov/09 18:28;ghelmling;I've been running the ""v2"" patch (applied against the 0.20.1 release) in my development setup for a couple days with correct expiration behavior.  I should have the patched version deployed and tested against my live data early next week, at which point I should be able to completely verify the fix.

This could definitely use a good review by someone more familiar with the compaction process.  The actual code changes are very minor and the new and existing tests all pass.  But the changes eliminate early exits from the KeyValue iteration on rows in two places, so it would be good to assess any performance impact from the change.;;;","05/Nov/09 19:08;streamy;Reviewed patch.  Looks great.  Thanks Gary.

+1 for commit;;;","05/Nov/09 21:52;stack;Thanks for the patch Gary.  Applied branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unhandled exception at regionserver,HBASE-1946,12439548,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,dmitriy_l,dmitriy_l,dmitriy_l,30/Oct/09 16:39,12/Oct/12 06:13,01/Jul/25 07:49,30/Oct/09 17:26,0.20.1,,,,,0.20.2,0.90.0,,regionserver,,,,0,"While starting hbase I get following exception:
{quote}
java.lang.NullPointerException
  at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:459)
   at java.lang.Thread.run(Thread.java:619)
{quote}
in region server's log on the second machine whereas at first machine all going well.
We've discussed with larsgeorge this problem at IRC channel and seems problem is in HRegionServer implementation.
Patch which fixies that problem attached to this message, but it should be not a final variant, because I cannot stop hbase with this fix.","two centos 5.3 installations
hmaster + regionserver at first and regionserver at second.
zookeeper quorum consists of one machine and located at the same place as master.",,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/09 16:41;dmitriy_l;HRegionServer.patch;https://issues.apache.org/jira/secure/attachment/12423691/HRegionServer.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26074,,,,,Fri Oct 30 17:26:07 UTC 2009,,,,,,,,,,"0|i08s5j:",49161,,,,,,,,,,,,,,,,,,,,,"30/Oct/09 16:41;dmitriy_l;Patch which fixies NullPointer exception in HRegionServer;;;","30/Oct/09 17:26;stack;Applied branch and trunk.  Thanks for patch Dmitriy.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Put's copy feature has a bug.,HBASE-1941,12439426,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,hyunsik.choi,hyunsik.choi,29/Oct/09 16:49,12/Oct/12 06:13,01/Jul/25 07:49,30/Oct/09 19:29,0.20.1,,,,,0.20.2,,,Client,,,,0,"Put's copy feature has a bug. The copy does not consider the timestamp value.
In the following example, a put and its copied put prints out different timestamps.
{quote}
Put put = new Put(""abc"".getBytes());
put.setTimeStamp(1);
System.out.println(put.getTimeStamp());
Put put2 = new Put(put);
System.out.println(put2.getTimeStamp());
---------------------------
Above source code results in as follows:

1
9223372036854775807
{quote}",,,,,,,,,,,,,,,,,,,,,,,HBASE-1930,,,,,,"30/Oct/09 00:05;davelatham;1941-branch0.20.patch;https://issues.apache.org/jira/secure/attachment/12423646/1941-branch0.20.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26073,,,,,Fri Oct 30 19:29:41 UTC 2009,,,,,,,,,,"0|i08s8n:",49175,,,,,,,,,,,,"Put, timestamp",,,,,,,,,"29/Oct/09 16:52;hyunsik.choi;Consequently, this bug affects TableOutputFormat.class.;;;","29/Oct/09 18:11;davelatham;See https://issues.apache.org/jira/browse/HBASE-1930 for some related discussion.

It doesn't actually affect TableOutputFormat because the timestamp in Put only affects values added after it is set, and TableOutputFormat does not add any values to the Put object.

HBASE-1930 fixed this for trunk, but perhaps a separate patch that doesn't introduce incompatibility would be good for the 0.20 branch.;;;","30/Oct/09 00:05;davelatham;Patch for 0.20 branch updates the Put copy constructor to copy timestamp and writeToWAL.  Also adds a warning to the setTimeStamp javadoc to make its behavior more clear.;;;","30/Oct/09 19:29;stack;Committed.  Thanks for patch Dave.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in ClientScanner,HBASE-1934,12439039,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,dploeg,dploeg,25/Oct/09 21:01,12/Oct/12 06:13,01/Jul/25 07:49,26/Oct/09 05:31,0.20.1,,,,,0.20.2,0.90.0,,,,,,0,"The following stack trace was observed whilst loading a large volume of data into Hbase:

Caused by: java.lang.NullPointerException
 at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:2008)
 at org.apache.hadoop.hbase.client.HTable$ClientScanner$1.hasNext(HTable.java:2089)

It appears that lastResult is initialized to be null, however in the exception handling code there isn't any null checking before the field is accessed for get row:

            this.scan.setStartRow(this.lastResult.getRow());

There should be some additional null checking logic here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/09 21:34;apurtell;HBASE-1934.patch;https://issues.apache.org/jira/secure/attachment/12423153/HBASE-1934.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26072,,,,,Mon Oct 26 21:55:54 UTC 2009,,,,,,,,,,"0|i08s8v:",49176,,,,,,,,,,,,,,,,,,,,,"25/Oct/09 21:34;apurtell;Seems an NSRE on first next invocation could cause this. Thanks for the bug report Daniel.;;;","26/Oct/09 05:31;stack;Committed branch and trunk.  Thanks for the patch Daniel.  Removed unused method filterSaysStop just above it while committing...;;;","26/Oct/09 21:55;stack;Sorry, thanks Andrew for making the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Put.setTimeStamp misleading (doesn't change timestamp on existing KeyValues, not copied in copy constructor)",HBASE-1930,12438845,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,davelatham,davelatham,22/Oct/09 20:39,20/Nov/15 13:01,01/Jul/25 07:49,23/Oct/09 20:00,0.20.0,0.20.1,,,,0.90.0,,,Client,,,,0,"In the process of migrating some code from 0.19, and was changing BatchUpdate's to Put's.  I was hit by a bit of a gotcha.  In the old code, I populated the BatchUpdate, then set the timestamp.  However, this doesn't wotk for Put, because Put creates KeyValue's with the currently set timestamp when adding values.  Setting the timestamp at the end has no effect.  Also, the copy constructor doesn't copy the timestamp (or writeToWAL) setting.

One option would be to simply update the javadoc to make it clear that the timestamp needs to be set prior to adding values.  I'm attaching a proposed patch which moves the timestamp setting to constructor only so that it isn't possible to trigger the confusing case at all.",,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1941,,,,"23/Oct/09 15:51;davelatham;1930-trunk.patch;https://issues.apache.org/jira/secure/attachment/12423025/1930-trunk.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26070,Reviewed,,,,Fri Nov 20 13:01:30 UTC 2015,,,,,,,,,,"0|i0hfw7:",99837,,,,,,,,,,,,,,,,,,,,,"22/Oct/09 20:40;davelatham;proposed patch removes setTimestamp, adds timestamp as a constructor arg, modifies copy constructor to copy timestamp and writeToWAL;;;","22/Oct/09 20:44;davelatham;Realized this fix shouldn't go on branch, because it's an api change.;;;","22/Oct/09 21:53;streamy;Good catch.

Patch looks good, Dave.  Will run it through tests tomorrow and commit if no one else does before me.  Thanks.;;;","23/Oct/09 00:41;stack;Should the below be passing on the rowlock?  It doesn't seem to be?  (Might be just my reading of patch in isolation)
{code}

Index: src/java/org/apache/hadoop/hbase/client/Put.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/Put.java	(revision 828839)
+++ src/java/org/apache/hadoop/hbase/client/Put.java	(working copy)
@@ -75,6 +75,16 @@
    * @param rowLock previously acquired row lock, or null
    */
   public Put(byte [] row, RowLock rowLock) {
+      this(row, HConstants.LATEST_TIMESTAMP, null);
+  }

{code}

Otherwise +1;;;","23/Oct/09 01:44;davelatham;You're absolutely right, Stack.  That's what I get to revising the constructors too many times.  Here's a revised patch.;;;","23/Oct/09 15:51;davelatham;Updated the patch to match trunk, fix up some javadoc and tests.  All tests pass locally.;;;","23/Oct/09 20:00;stack;Thanks for the patch Dave.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ROOT and META tables stay in transition state (making the system not usable) if the designated regionServer dies before the assignment is complete,HBASE-1928,12438825,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ypavlidis,ypavlidis,22/Oct/09 17:32,12/Oct/12 06:13,01/Jul/25 07:49,07/Nov/09 23:43,0.20.0,0.20.1,,,,0.20.2,0.90.0,,master,,,,0,"During a ROOT or META table re-assignment if the designated regionServer dies before the assignment is complete then the whole cluster becomes unavailble since the ROOT or META tables cannot be accessed (and never recover since they are kept in a transition state).

These are the 4 steps to replicate this issue (this is the easiest way to replicate. You can imagine that the following can occur in any real system).

Pre condition
============
1. a cluster of 3 nodes (cache01, cache02, search01).
2. start the system (start-hbase)
3. cache02 has META, search01 has ROOT, cache01 has regionServer and Master.

Case 1:
=======
1. kill cache01
2. kill cache02
3. now search01 has both ROOT and META.
4. re-start RegionServers on cache01 and cache02
5. Tail the master logs and grep for ""Assigning region -ROOT-"" and also ""Assigning region .META."" (need to windows for easiness)
6. kill search01
7. wait to see to which server the ROOT will be assigned (from the tail)
8. quickly kill that server
9. you should notice that the ROOT server never gets re-assigned (because it is stuck in the regionsInTransitions)

The termination occurs through the ServerManager::removeServerInfo since the regionServer sends back to the master in a report that it is shutting down.

Case 2:
========
Repeat Case1 and in step 7 and 8 kill the server that has the META region assigned to it. Again the cluster becomes unavailble because the META region stays in the regionsInTransitions.
The termination occurs through the ServerManager::removeServerInfo since the regionServer sends back to the master in a report that it is shutting down.

Case 3:
========
Repeat Case1 and in step 7 and 8 kill the server with kill -9 instead of kill. This will not give the opportunity to the regionServer to send back the master in the report that it is terminating. The master will realize this because the znode will expire (but it is a different code path from before - it goes to the ProcessServerShutdown).

Case 4:
========
Repeat Case3 and in step 7 and 8 kill the server with kill -9 instead of kill. This will not give the opportunity to the regionServer to send back the master in the report that it is terminating. The master will realize this because the znode will expire (but it is a different code path from before - it goes to the ProcessServerShutdown).

The solution would be to check the in the ServerManager:removeServerInfo and in  ProcessServerShutdown::closeMetaRegions whether the server that has been terminated has been assigned either the ROOT or META table. And if they have make sure we make those table ready to be re-assigned again.
",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/09 22:43;stack;1928-branch.patch;https://issues.apache.org/jira/secure/attachment/12424235/1928-branch.patch","06/Nov/09 20:00;ypavlidis;HBASE-1928.patch;https://issues.apache.org/jira/secure/attachment/12424222/HBASE-1928.patch","22/Oct/09 18:14;ypavlidis;diff_ProcessServerShutdown.txt;https://issues.apache.org/jira/secure/attachment/12422923/diff_ProcessServerShutdown.txt","22/Oct/09 18:14;ypavlidis;diff_RegionManager.txt;https://issues.apache.org/jira/secure/attachment/12422924/diff_RegionManager.txt","22/Oct/09 18:14;ypavlidis;diff_ServerManager.txt;https://issues.apache.org/jira/secure/attachment/12422925/diff_ServerManager.txt","22/Oct/09 17:39;ypavlidis;master_cache01.txt;https://issues.apache.org/jira/secure/attachment/12422911/master_cache01.txt","22/Oct/09 17:39;ypavlidis;region_cache01.txt;https://issues.apache.org/jira/secure/attachment/12422912/region_cache01.txt","22/Oct/09 17:39;ypavlidis;region_cache02.txt;https://issues.apache.org/jira/secure/attachment/12422913/region_cache02.txt",,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26068,,,,,Sat Nov 07 23:43:24 UTC 2009,,,,,,,,,,"0|i08s5z:",49163,,,,,,,,,,,,,,,,,,,,,"22/Oct/09 17:37;ypavlidis;The above situation occurred in our environment when:

pre condition
===============
cache01 (is the backup master, runs a region server has the ROOT and META assigned to it)
cache02 (runs a region server)
search01 (runs the master and the region server)

scenario
=========
kill the master on search01
the master on cache01 resumes master duties
cache01 encounters a fatal error (FATAL org.apache.hadoop.hbase.regionserver.LogRoller: Log rolling failed with ioe) and has to exit
The ROOT is getting re-assigned to the region server on search01 and the META is getting re-assigned to the region server on cache02.

Now cache02 encounters the same fatal error (FATAL org.apache.hadoop.hbase.regionserver.LogRoller: Log rolling failed with ioe) and has to exit before it accepts the assignment for servicing the META region

post condition
===============
While the root is assigned to search01 the meta appears to have been left in limbo state (still in regionsInTransitions map of the RegionManager). The issue I believe is because of a race condition.
The region server in cache02 never gets the chance to complete the assignment of the meta region. When cache01 realizes that cache02 has died in the ProcessServerShutdown it never checks to see whether the server that died had a meta region assigned to it in transition (isMetaServer method in the RegionManager checks for that). The result of this is that when my client connects it gets the cache02 address for the meta server and of course it keeps failing to connect.

I have attached the logs for this scenario. Because of course it is difficult to replicate the above case please follow the steps on the first comment to simulate this problem with ROOT / META.
;;;","22/Oct/09 17:39;ypavlidis;Master indicates that is not assigned to any server
Cache01 terminates
Cache02 gets the assignment for the META region but never completes it.;;;","22/Oct/09 18:16;ypavlidis;stack.

I am re-assigning this too for further testing and approval. 

I performed all the tests described in the first comments without the patches to replicate the issue and then performed the same steps with the patches and everything worked as expected (at the end ROOT and META were assigned to a regionServer).

Thanks in advance,

Yannis.;;;","23/Oct/09 00:50;stack;Thanks Yannis for all the digging in.  Will include in 0.20.2.  Will apply soon.;;;","02/Nov/09 22:58;jdcryans;Yannis,

Could you rebase your patches on the latest trunk? 

Also you could make a single patch for all 3 files? Simply issue a ""svn diff src/ > HBASE-1928.patch"" from HBase's base folder. 

Finally please make sure that your lines aren't longer than 80 chars. 

Thanks a lot!;;;","06/Nov/09 20:00;ypavlidis;Hey Jean-Daniel,

I just posted the info you requested. Let me know if you have any questions.

Regards,

Yannis.;;;","06/Nov/09 22:43;stack;Version of patch that works for 0.20.  I'm running all tests now.;;;","07/Nov/09 23:43;stack;Committed branch and trunk.  Thank you for the patch Yannis (Passes all tests locally).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scanners not closed properly in certain circumstances (memory leak),HBASE-1927,12438783,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,dlrozendaal,dlrozendaal,22/Oct/09 07:31,12/Oct/12 06:13,01/Jul/25 07:49,22/Oct/09 18:54,0.20.1,,,,,0.20.2,0.90.0,,regionserver,,,,0,"Scanners are sometimes leaked by the KeyValueHeap class. The constructor adds each scanner to a heap, but only if the scanner's peek() method returns not null (line 58). Otherwise the scanner is dropped without being closed.

Unfortunately some scanners (like StoreScanner and MemStoreScanner) register themselves to some global list when constructed and only deregister on close(). This can cause a memory leak, for example with MemStoreScanners on an empty memory store.

The quick fix is to add an else clause to the if on line 58:

} else {
  scanner.close()
}

The root cause is that ownership of the scanners is transferred from the caller to the KeyValueHeap on construction. Maybe this should be made clear in the documentation or changed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/09 18:48;streamy;HBASE-1927-v1-trunk.patch;https://issues.apache.org/jira/secure/attachment/12422929/HBASE-1927-v1-trunk.patch","22/Oct/09 17:10;streamy;HBASE-1927-v1.patch;https://issues.apache.org/jira/secure/attachment/12422909/HBASE-1927-v1.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26067,Reviewed,,,,Thu Oct 22 18:54:12 UTC 2009,,,,,,,,,,"0|i08s93:",49177,,,,,,,,,,,,,,,,,,,,,"22/Oct/09 17:10;streamy;Thanks for finding this Erik.

Patch adds closing of empty KVScanners in KVHeap and adds comment in constructor javadoc.

Also adds a test in TestKeyValueHeap that uncovers the issue.  Test passes with patch.;;;","22/Oct/09 17:11;streamy;Bug exists in trunk as well.  Please review for commit.;;;","22/Oct/09 17:18;stack;+1 for branch and trunk.  Thanks Erik for the fix.;;;","22/Oct/09 18:09;dlrozendaal;Patch applies, compiles, and fixes the problem. Thanks!
;;;","22/Oct/09 18:48;streamy;Patch didn't apply cleanly to trunk, just want to run local tests before committing.;;;","22/Oct/09 18:54;streamy;Test passes on branch and trunk.  Committed to both.  Thanks Erik and Stack.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the Master's session times out and there's only one, cluster is wedged",HBASE-1921,12438514,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,20/Oct/09 03:08,12/Oct/12 06:13,01/Jul/25 07:49,29/Oct/09 02:14,0.20.1,,,,,0.20.2,0.90.0,,,,,,0,"On IRC, some fella had a session expiration on his Master and had only one. Maybe in this case the Master should first try to re-get the znode?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/09 01:06;jdcryans;HBASE-1921-trunk.patch;https://issues.apache.org/jira/secure/attachment/12423517/HBASE-1921-trunk.patch","20/Oct/09 18:07;jdcryans;HBASE-1921.patch;https://issues.apache.org/jira/secure/attachment/12422705/HBASE-1921.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26062,Reviewed,,,,Sat Apr 30 00:53:40 UTC 2011,,,,,,,,,,"0|i08s6f:",49165,,,,,,,,,,,,,,,,,,,,,"20/Oct/09 04:33;stack;Sounds good J-D.  We need to reenable the thing where RS does this too... Nitay had it up and working but we disabled it before 0.20.0 because it had some issues.;;;","20/Oct/09 16:34;jdcryans;Well in this case it's a different solution I have in mind. For sure, cleanly restarting the processes would be nice and would solve this problem but what I was thinking is that when the master loses its session, it should try to re-get the lock right away (no restart). This way you keep the internal state and if no other master took its place, then it's a win. Else, harakiri.;;;","20/Oct/09 17:01;stack;Sounds grand.;;;","20/Oct/09 18:07;jdcryans;Patch that does what I described and here's what you will see when it happens:

{code}2009-10-20 10:53:38,708 DEBUG org.apache.hadoop.hbase.master.HMaster: Got event None with path null
2009-10-20 10:53:39,997 INFO org.apache.zookeeper.ClientCnxn: Attempting connection to server /10.10.1.58:2181
2009-10-20 10:53:39,998 INFO org.apache.zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/10.10.1.58:56099 remote=/10.10.1.58:2181]
2009-10-20 10:53:39,998 INFO org.apache.zookeeper.ClientCnxn: Server connection successful
2009-10-20 10:53:40,000 WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x12472fd41f10004 to sun.nio.ch.SelectionKeyImpl@2afb6c5f
java.io.IOException: Session Expired
	at org.apache.zookeeper.ClientCnxn$SendThread.readConnectResult(ClientCnxn.java:589)
	at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:709)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:945)
2009-10-20 10:53:40,000 DEBUG org.apache.hadoop.hbase.master.HMaster: Got event None with path null
2009-10-20 10:53:40,000 INFO org.apache.hadoop.hbase.master.HMaster: Master lost its znode, trying to get a new one
2009-10-20 10:53:40,000 INFO org.apache.zookeeper.ZooKeeper: Closing session: 0x12472fd41f10004
2009-10-20 10:53:40,000 INFO org.apache.zookeeper.ClientCnxn: Closing ClientCnxn for session: 0x12472fd41f10004
2009-10-20 10:53:40,001 INFO org.apache.zookeeper.ClientCnxn: Disconnecting ClientCnxn for session: 0x12472fd41f10004
2009-10-20 10:53:40,001 INFO org.apache.zookeeper.ZooKeeper: Session: 0x12472fd41f10004 closed
2009-10-20 10:53:40,001 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Closed connection with ZooKeeper
2009-10-20 10:53:40,003 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=10.10.1.58:2181 sessionTimeout=60000 watcher=Thread[HMaster,5,main]
2009-10-20 10:53:40,003 INFO org.apache.zookeeper.ClientCnxn: Attempting connection to server /10.10.1.58:2181
2009-10-20 10:53:40,005 INFO org.apache.zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/10.10.1.58:56100 remote=/10.10.1.58:2181]
2009-10-20 10:53:40,006 INFO org.apache.zookeeper.ClientCnxn: Server connection successful
2009-10-20 10:53:40,009 DEBUG org.apache.hadoop.hbase.master.HMaster: Got event None with path null
2009-10-20 10:53:40,012 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Wrote master address 10.10.1.58:60000 to ZooKeeper
2009-10-20 10:53:40,016 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Read ZNode /hbase/master got 10.10.1.58:60000
2009-10-20 10:53:40,017 DEBUG org.apache.hadoop.hbase.master.HMaster: Checking cluster state...
2009-10-20 10:53:40,017 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.10.1.58:60020
2009-10-20 10:53:40,019 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Read ZNode /hbase/rs/1256061062528 got 10.10.1.58:60020
2009-10-20 10:53:40,019 INFO org.apache.hadoop.hbase.master.HMaster: This is a failover, ZK inspection begins...
2009-10-20 10:53:40,020 DEBUG org.apache.hadoop.hbase.master.HMaster: Inspection found server 10.10.1.58
2009-10-20 10:53:40,022 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Updated ZNode /hbase/rs/1256061062528 with data 10.10.1.58:60020
2009-10-20 10:53:40,028 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: SetData of ZNode /hbase/root-region-server with 10.10.1.58:60020
2009-10-20 10:53:40,029 INFO org.apache.hadoop.hbase.master.HMaster: Inspection found 3 regions, with -ROOT-
2009-10-20 10:53:40,029 INFO org.apache.hadoop.hbase.master.HMaster: Found log folder : 10.10.1.58,60020,1256061062528
2009-10-20 10:53:40,029 INFO org.apache.hadoop.hbase.master.HMaster: Log folder belongs to an existing region server
2009-10-20 10:53:40,029 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2009-10-20 10:54:38,601 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 3.0
2009-10-20 10:54:38,602 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 10.10.1.58:60020, regionname: -ROOT-,,0, startKey: <>}
2009-10-20 10:54:38,607 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 10.10.1.58:60020, regionname: .META.,,1, startKey: <>}
2009-10-20 10:54:38,611 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 10.10.1.58:60020, regionname: -ROOT-,,0, startKey: <>} complete
2009-10-20 10:54:38,615 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 1 row(s) of meta region {server: 10.10.1.58:60020, regionname: .META.,,1, startKey: <>} complete
2009-10-20 10:54:38,615 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
{code};;;","21/Oct/09 00:08;jdcryans;I asked _hp_ on IRC to run the patch and during the whole day it happened once and continued like if it was normal.

Would like a code review.;;;","22/Oct/09 01:02;stack;+1;;;","22/Oct/09 17:15;streamy;Patch looks good, JD.;;;","22/Oct/09 17:40;jdcryans;Thank you guys, I will try to write some tests (doesn't seem obvious with current tools) and then commit. ;;;","29/Oct/09 01:07;jdcryans;I will commit this patch to trunk with the new test and the other patch with the new test to branch.;;;","29/Oct/09 02:14;jdcryans;Committed to branch and trunk. Had to tweak ZKW a bit because the master was staying in safe mode (damn safe mode).;;;","30/Apr/11 00:53;hudson;Integrated in HBase-TRUNK #1888 (See [https://builds.apache.org/hudson/job/HBase-TRUNK/1888/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
code: HRS.delete seems to ignore exceptions it shouldnt,HBASE-1919,12438500,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,ryanobjc,ryanobjc,19/Oct/09 21:54,12/Oct/12 06:13,01/Jul/25 07:49,03/Nov/09 23:48,0.20.1,,,,,0.20.2,0.90.0,,,,,,0,"the code is:

      region.delete(delete, lid, writeToWAL);

      this.hlog.sync(region.getRegionInfo().isMetaRegion());
    } catch (WrongRegionException ex) {
    } catch (NotServingRegionException ex) {
      // ignore                                                                                                                                                                          
    } catch (Throwable t) {
      throw convertThrowableToIOE(cleanup(t));
    }

we ignore those 2 exceptions... weird... should not be!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/09 22:11;jdcryans;HBASE-1919.patch;https://issues.apache.org/jira/secure/attachment/12423958/HBASE-1919.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26060,Reviewed,,,,Tue Nov 03 23:48:32 UTC 2009,,,,,,,,,,"0|i08s2f:",49147,,,,,,,,,,,,,,,,,,,,,"03/Nov/09 22:11;jdcryans;Here's a patch that just removes the exceptions non-catching just like the other stuff works (single put, get, etc).;;;","03/Nov/09 22:23;stack;+1 (Have you tried dropping a table... maybe this hackery is necessary there?);;;","03/Nov/09 23:37;jdcryans;Passes tests and can drop tables;;;","03/Nov/09 23:48;jdcryans;Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When adding a secondary index to an existing table, it will cause NPE during re-indexing. ",HBASE-1912,12438285,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,ray_liao,ray_liao,16/Oct/09 10:08,12/Oct/12 06:13,01/Jul/25 07:49,18/Oct/09 16:30,0.20.0,0.20.1,,,,0.20.2,0.90.0,,,,,,0,"When adding a secondary index to an existing table, an IndexSpecification must be constructed.
If we construct a simple index using the following constructor: IndexSpecification(String indexId, byte[] indexedColumn), then the program will cause NPE during re-indexing. 
    
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.tableindexed.IndexMaintenanceUtils.createIndexUpdate(IndexMaintenanceUtils.java:57)
        at org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.reIndexTable(IndexedTableAdmin.java:144)
        at org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.addIndex(IndexedTableAdmin.java:132)
        at MyIndexedTable.addSecondaryIndexToExistingTable(MyIndexedTable.java:256)
        at MyIndexedTable.main(MyIndexedTable.java:276)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/09 10:18;ray_liao;IndexSpecification.patch;https://issues.apache.org/jira/secure/attachment/12422341/IndexSpecification.patch","16/Oct/09 10:18;ray_liao;IndexedTableTest.java;https://issues.apache.org/jira/secure/attachment/12422339/IndexedTableTest.java","16/Oct/09 10:18;ray_liao;hbase-site.xml;https://issues.apache.org/jira/secure/attachment/12422340/hbase-site.xml",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26054,,,,,Sun Oct 18 16:30:46 UTC 2009,,,,,,,,,,"0|i08s33:",49150,,,,,,,,,,,,,,,,,,,,,"16/Oct/09 10:18;ray_liao;IndexedTableTest.java is the application for triggering the problem.
hbase-site.xml is the configuration used.


The cause of the NPE is that inside IndexMaintenanceUtils.createIndexUpdate function, 
it attempts to iterate through IndexSpecification's additionalColumns without first
    checking for null assignment. 
    i.e. 
    for (byte[] col : indexSpec.getAdditionalColumns()) 
    { /* will NPE if additionalColumns = null }
    
Solution:
    There are many places that will attempts to get additionalColumns from the IndexSpecification.
    It is best to assign a zero length byte[][] inside the constructor, if the additionalColumns is null.;;;","16/Oct/09 16:08;apurtell;+1

Thanks for the bug report and patch Ray. Much appreciated. ;;;","18/Oct/09 16:30;apurtell;Committed to trunk and 0.20 branch. All local tests pass.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ROOT not reassigned if only one regionserver left,HBASE-1908,12438225,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,15/Oct/09 18:23,12/Oct/12 06:13,01/Jul/25 07:49,20/Oct/09 00:08,0.20.1,,,,,0.20.2,0.90.0,,,,,,0,"Yannis on the list uncovered an assignment bug:

{quote}
I performed additional testing with some alternate configurations and the problem arises (ONLY) when there is only one regionserver left which has the META table already assigned to it. 

In this case the ROOT table does not get assigned to the last regionserver (which holds the META table).

Interestingly enough though when there is only one regionserver left that has the ROOT table already assign to it then it can also have the META table re-assigned to it (if again is the only server - i.e. in this scenario you can have one regionserver holding both the META and ROOT tables).

Unless I am missing something I cannot find any reason why we cannot assign the ROOT table to the regionserver that manages the META table if it is the only one remaining (again it is an extreme case I agree that this can happen).

I applied and tested a fix (at the hbase-0.20.0 codebase) in the RegionManager::regionsAwaitingAssignment where I add the root table in the regionstoAssign set if the it is the metaServer and also the only server.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/09 18:56;streamy;HBASE-1908-v1.patch;https://issues.apache.org/jira/secure/attachment/12422259/HBASE-1908-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26051,Reviewed,,,,Tue Oct 20 00:08:34 UTC 2009,,,,,,,,,,"0|i08s3b:",49151,,,,,,,,,,,,,,,,,,,,,"15/Oct/09 18:38;streamy;Yannis proposed fix:

{noformat}
diff RegionManager.java.FIXES RegionManager.java
414c414
<       if ((!isMetaServer) || (isMetaServer && isSingleServer)) {
---
>       if (!isMetaServer) {
{noformat};;;","15/Oct/09 18:51;davelatham;How about:

{{if (!isMetaServer || isSingleServer) {}}
;;;","15/Oct/09 18:52;streamy;Binary logic ftw!;;;","15/Oct/09 18:56;streamy;Trying to reproduce and then will test with this patch.;;;","15/Oct/09 19:14;streamy;Reproduced on clean branch as described.  If you have two regionservers, one with META and one with ROOT, if you kill the regionserver hosting ROOT it doesn't ever get reassigned.

Odd behavior though.  The META region gets reassigned to the regionserver tho it was never unassigned.

Master log:  http://pastebin.com/mb4fed51

RegionServer log: http://pastebin.com/m3e079b36;;;","15/Oct/09 20:55;streamy;Patch fixes the problem.  It does re-assign META, but doesn't seem to matter (since it's being reassigned to regionserver already hosting it).

Also tested killing server hosting META instead of ROOT and all seems fine.  Did uncover a client issue though.;;;","19/Oct/09 23:21;streamy;Will commit after someone reviews;;;","19/Oct/09 23:25;stack;+1

What about issue where client would not go elsewhere after a connection refused exception?  Is that a different issue?;;;","20/Oct/09 00:06;streamy;Opened HBASE-1920 for that issue, it is completely distinct I just uncovered it while testing for this.;;;","20/Oct/09 00:08;streamy;Committed to branch and trunk, thanks for review stack.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WhileMatchFilter.reset should call encapsulated filter reset  ,HBASE-1896,12437671,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,ykulbak,ykulbak,09/Oct/09 03:40,12/Oct/12 06:13,01/Jul/25 07:49,15/Oct/09 06:48,0.20.1,,,,,0.20.2,0.90.0,,Filters,,,,0,"Bumped into this when trying to encapsulate a SingleValueColumnFilter in a WhileMatchFilter. 
A scanner would grab all the rows after the first matched row in the table 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/09 06:37;stack;1896.patch;https://issues.apache.org/jira/secure/attachment/12422190/1896.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26046,,,,,Thu Oct 15 06:37:50 UTC 2009,,,,,,,,,,"0|i08s3r:",49153,,,,,,,,,,,,,,,,,,,,,"09/Oct/09 04:32;stack;Will take a look at this in morning.

Yoram, you need this in 0.20.1 hbase release?  Or can it wait till 0.20.2?;;;","09/Oct/09 15:09;stack;Moved to 0.20.2...;;;","15/Oct/09 06:37;stack;Fix.  Testing now...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HConstants.MAX_ROW_LENGTH is incorrectly 64k, should be 32k",HBASE-1895,12437648,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,amansk,ryanobjc,ryanobjc,08/Oct/09 21:33,12/Oct/12 06:13,01/Jul/25 07:49,21/Oct/09 23:40,0.20.0,,,,,0.20.2,0.90.0,,,,,,0,"the max length of a row has to fit into a signed short. That is 32k.  The constant is incorrect, but in the depths of KeyValue it does the right thing:

    if (rlength > Short.MAX_VALUE) {
      throw new IllegalArgumentException(""Row > "" + Short.MAX_VALUE);
    }

so your Put wont fail and instead it will fail in the KeyValue constructor.  So far only that 1 line in Put uses this constant, but we should have a correct value here. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/09 14:31;amansk;HBASE-1895.patch;https://issues.apache.org/jira/secure/attachment/12421875/HBASE-1895.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26045,,,,,Wed Oct 21 23:40:07 UTC 2009,,,,,,,,,,"0|i08sa7:",49182,,,,,,,,,,,,,,,,,,,,,"09/Oct/09 21:51;stack;You want to make a patch ak?;;;","12/Oct/09 14:30;amansk;Submitting patch. No test case yet.;;;","12/Oct/09 23:09;streamy;0.20.1 is being released, bumping to 0.20.2;;;","12/Oct/09 23:16;ryanobjc;so far looking good, a test case would be great thanks!;;;","21/Oct/09 23:40;streamy;Committed to branch, somehow this was already fixed in trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException on trunk for REST,HBASE-1889,12437403,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,06/Oct/09 15:15,20/Nov/15 13:01,01/Jul/25 07:49,11/Oct/09 08:51,0.90.0,,,,,0.90.0,,,master,,,,0,"Reported by Zheng Shao on list:

{noformat}
java.lang.ClassNotFoundException: org.apache.hadoop.hbase.rest.Dispatcher
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:276)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
{noformat}

related to

{noformat}
        at org.apache.hadoop.http.HttpServer.start(HttpServer.java:460)
        at org.apache.hadoop.hbase.master.HMaster.startServiceThreads(HMaster.java:641)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:410)
{noformat}

REST was removed completely in 0.21.  Investigate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/09 15:21;streamy;HBASE-1889-v1.patch;https://issues.apache.org/jira/secure/attachment/12421433/HBASE-1889-v1.patch","06/Oct/09 17:45;streamy;HBASE-1889-v2.patch;https://issues.apache.org/jira/secure/attachment/12421446/HBASE-1889-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26041,,,,,Fri Nov 20 13:01:32 UTC 2015,,,,,,,,,,"0|i0d40v:",74428,,,,,,,,,,,,,,,,,,,,,"06/Oct/09 15:21;streamy;Removes addition of REST server for master web ui.;;;","06/Oct/09 17:11;stack;+1 on patch.

Remove it from bin/hbase too:

{code}
stack@aron:~/checkouts/workspace/trunk$ ./bin/hbase
Usage: hbase <command>
where <command> is one of:
  shell            run the HBase shell
  master           run an HBase HMaster node
  regionserver     run an HBase HRegionServer node
  rest             run an HBase REST server
  thrift           run an HBase Thrift server
  zookeeper        run a Zookeeper server
  migrate          upgrade an hbase.rootdir
 or
  CLASSNAME        run the class named CLASSNAME
{code};;;","06/Oct/09 17:45;streamy;Removes rest from /bin/hbase.

Also removes src/webapps/rest;;;","09/Oct/09 22:15;stack;Marking patch available.  Remove the xmlenc jar too from lib Jon.  Only REST used it IIRC.;;;","11/Oct/09 08:51;apurtell;Committed.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyValue methods throw NullPointerException instead of IllegalArgumentException during parameter sanity check,HBASE-1888,12437400,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,stack,mpodsiadlowski,mpodsiadlowski,06/Oct/09 14:27,20/Nov/15 13:02,01/Jul/25 07:49,07/Dec/10 22:36,0.20.0,,,,,0.92.0,,,,,,,0,"Methods of org.apache.hadoop.hbase.KeyValue
public static int getDelimiter(final byte [] b, int offset, final int length, final int delimiter)
public static int getDelimiterInReverse(final byte [] b, final int offset, final int length, final int delimiter)
throw NullPointerException instead of IllegalArgumentException when byte array b is check for null  - which is very bad practice!
Please refactor this because this can be very misleading.  
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/10 22:35;stack;1888.txt;https://issues.apache.org/jira/secure/attachment/12465748/1888.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26040,,,,,Fri Nov 20 13:02:01 UTC 2015,,,,,,,,,,"0|i0hfsf:",99820,,,,,,,,,,,,,,,,,,,,,"07/Dec/10 22:35;stack;Small patch to do as Michal suggests (head of patch has some fix up of class javadoc).;;;","07/Dec/10 22:36;stack;Committed to TRUNK.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRegion passes the wrong minSequenceNumber to doReconstructionLog,HBASE-1883,12437162,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,clint.morgan,clint.morgan,clint.morgan,02/Oct/09 22:14,20/Nov/15 13:02,01/Jul/25 07:49,03/Oct/09 05:07,0.20.0,,,,,0.20.1,0.90.0,,regionserver,,,,0,HRegion initializes by opens up all store files which may recover from the WAL. It then calls protected doReconstructionLog which THBase uses to go through the log and look for pending transactions that may need to be recovered. Currently HRegion is passing down the minSequenceNumber *after* WAL recovery. What we want is the lowest sequence number before the wal recovery.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/09 22:19;clint.morgan;1883.patch;https://issues.apache.org/jira/secure/attachment/12421163/1883.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26038,,,,,Fri Nov 20 13:02:03 UTC 2015,,,,,,,,,,"0|i0hfrb:",99815,,,,,,,,,,,,,,,,,,,,,"02/Oct/09 22:19;clint.morgan;this fixed it for me;;;","03/Oct/09 05:07;stack;Applied branch and trunk.  Thanks for the patch Clint.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeleteColumns are not recovered properly from the write-ahead-log,HBASE-1880,12437083,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,,clint.morgan,clint.morgan,01/Oct/09 22:50,20/Nov/15 13:01,01/Jul/25 07:49,24/May/10 16:56,0.20.0,0.20.1,0.90.0,,,0.90.0,,,regionserver,,,,0,"I found a couple of issues:
 - The timestamp is being set to now after it has been written to the wal. So if the WAL was flushed on that write, it gets in with ts of MAX_INT and is effectively lost.

 - Even after that fix, I had issues getting the delete to apply properly. In my case, the WAL had a put to a column, then a DeleteColumn for the same column. The DeleteColumn KV had a later timestamp, but it was still lost on recovery. I traced around a bit, and it looks like the current approach of just using an HFile.writer to write the set of KVs read from the log will not work. There is special logic in MemStore for deletes that needs to happen before writing. I got around this by just adding to memstore in the log recovery process. Not sure if there are other implications of this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/09 00:00;clint.morgan;1880-v2.patch;https://issues.apache.org/jira/secure/attachment/12421086/1880-v2.patch","02/Oct/09 18:38;clint.morgan;1880-v3.patch;https://issues.apache.org/jira/secure/attachment/12421135/1880-v3.patch","02/Oct/09 18:42;clint.morgan;1880-v4.patch;https://issues.apache.org/jira/secure/attachment/12421137/1880-v4.patch","01/Oct/09 23:00;clint.morgan;1880.patch;https://issues.apache.org/jira/secure/attachment/12421078/1880.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26036,,,,,Fri Nov 20 13:01:34 UTC 2015,,,,,,,,,,"0|i0hfqv:",99813,,,,,,,,,,,,,,,,,,,,,"01/Oct/09 23:00;clint.morgan;This fixes both issues as described above. 

New approach of filling the memstore with WAL entries needs review.;;;","02/Oct/09 00:00;clint.morgan;Looks like putting in memstore and not flushing is bad. If we crash again, then we will loose the edits.

This adds a flush if we read any edits.

This fixes my failure tests, running hbase tests now.;;;","02/Oct/09 00:54;clint.morgan;TestClient.testDeletes fails:  Expected 1 key but received 2
	at org.apache.hadoop.hbase.client.TestClient.testDeletes(TestClient.java:1266)

I'll look into it tomorrow...;;;","02/Oct/09 04:01;stack;Patch looks good.  Moving writeToWAL till after the timestamp gets set seems like a no duh kinda thing (Can you fix the formatting -- its anti-easy-read at moment obviously product of a machine formatter).

So, no more reconstructeCache, just insert straight into memstore?  That solves another problem I was worried about in that we keep an eye on size of memstore but not on this reconstructionCache.  Removing it looks like a good move. 

So, these new edits are not going into a WAL at all?  They should?  (Make a new issue? My sense is when a working flush all of our recovery will need to come out of the dark and get a spotlight shone on it).;;;","02/Oct/09 18:38;clint.morgan;Last patch broke the case when we have two deletes with LATEST timestamp. If so, then we need for the first delete to get into the store, so it removes the first version, then the second delete will get the timestamp of the second version to remove.

Good thing for unit tests, as the change did seem harmless. TestClient now passes. I'll run the full suite again...

This patch also removes the delete KVs that get discarded because there is nothing to delete. So they don't get in the WAL.

@Stack
Yeah, writing to memstore directly. Currently its wired up to flush memstore during log recovery, but this would be a good idea. 

""these new edits are not going into a WAL at all?"" Yeah thats right, we talk to the memstore directly rather than go through HRegion which does the WALing. This is ok because those edits were coming from a WAL anyway. The key is we need to flush after the recovery so that we no longer need that WAL we just read to recover in case of subsequent crash.

;;;","02/Oct/09 18:42;clint.morgan;Last patch had a bit from 1877 in it, this removes it.;;;","22/May/10 00:34;streamy;Does this issue still exist?;;;","22/May/10 00:47;clint.morgan;Looks like this was applied to trunk. ;;;","22/May/10 22:05;stack;I don't see this in trunk or banch.;;;","24/May/10 16:44;clint.morgan;Well looking at Store.java, those changes sure seem to be in there to me: Apply edits to memstore and flush if appropriate. Store.runReconstructionLog has the same comments introduced/changed in this patch...;;;","24/May/10 16:56;stack;OK Clint.  I see the HStore edits now.  I was looking for the HRegion changes.  HRegion seems to have evolved significantly from when your patch was applied.  I can see ghost outlines of what you added in HRegion.  Resolving.  We can open new issue if this still an issue.  Thanks.;;;","24/May/10 17:36;kannanm;Just to make sure... this issue should have been fixed as part of HBASE-2338, right?;;;","25/May/10 00:30;ryanobjc;this was not fixed in 0.20 branch as of today, I ported what was done on trunk and massaged it a bit for post HBASE-2248 and it is now in.;;;","25/May/10 01:41;tlipcon;Can we add a unit test for this? It seems clearly unit testable.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadOnly transactions generate WAL activity.,HBASE-1879,12437070,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,clint.morgan,clint.morgan,clint.morgan,01/Oct/09 21:03,20/Nov/15 13:01,01/Jul/25 07:49,03/Oct/09 05:17,0.20.0,,,,,0.20.1,0.90.0,,,,,,0,"Currently we write a start entry in the WAL each time a transaction starts, and a commit/abort at the end of the transaction. This means read-only transactions unnecessarily generate two WAL entries per region.

Can avoid this by removing the start entry from the WAL (this is implicit in the first trx OP entry we see), and only writing commit/abort when the transaction has a write.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/09 21:50;clint.morgan;1879-v2.patch;https://issues.apache.org/jira/secure/attachment/12421160/1879-v2.patch","01/Oct/09 21:15;clint.morgan;1879.patch;https://issues.apache.org/jira/secure/attachment/12421067/1879.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26035,,,,,Fri Nov 20 13:01:40 UTC 2015,,,,,,,,,,"0|i0hfqn:",99812,,,,,,,,,,,,,,,,,,,,,"01/Oct/09 21:15;clint.morgan;This fixes it. All transactional tests pass.;;;","02/Oct/09 05:23;stack;Committed branch and trunk.  Thanks for patch Clint.;;;","02/Oct/09 21:50;clint.morgan;I missed a line. ;;;","02/Oct/09 21:50;clint.morgan;please apply v2 to branch and trunk;;;","03/Oct/09 04:59;stack;Done.  Thanks for patch Clint.;;;","03/Oct/09 05:17;stack;Resolving.  Committed second patch.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong type used in TableMapReduceUtil.initTableReduceJob(),HBASE-1871,12436670,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,larsgeorge,larsgeorge,larsgeorge,26/Sep/09 20:23,20/Nov/15 13:01,01/Jul/25 07:49,26/Sep/09 21:23,0.20.0,,,,,0.20.1,0.90.0,,,,,,0,"Since we changed it so that TableOutputFormat can handle Put and Delete it is necessary to set the output value class to Writable.

{code}
  public static void initTableReducerJob(String table,
    Class<? extends TableReducer> reducer, Job job, Class partitioner)
  throws IOException {
    job.setOutputFormatClass(TableOutputFormat.class);
    if (reducer != null) job.setReducerClass(reducer);
    job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, table);
    job.setOutputKeyClass(ImmutableBytesWritable.class);
    job.setOutputValueClass(Put.class);
   ....
{code} 

The last line should be 

{code}
    job.setOutputValueClass(Writable.class);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/09 20:28;larsgeorge;HBASE-1871.patch;https://issues.apache.org/jira/secure/attachment/12420635/HBASE-1871.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26030,Reviewed,,,,Fri Nov 20 13:01:20 UTC 2015,,,,,,,,,,"0|i0hfpj:",99807,,,,,,,,,,,,,,,,,,,,,"26/Sep/09 20:28;larsgeorge;HBASE-1871.patch - Fixes class being set and removes obsolete import.;;;","26/Sep/09 21:23;stack;Applied trunk and branch.  Thanks for the patch Lars.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexedTable delete fails when used in conjunction with RowLock(),HBASE-1869,12436634,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,keiththomas,keiththomas,25/Sep/09 19:46,20/Nov/15 13:01,01/Jul/25 07:49,25/Sep/09 23:22,0.20.0,0.20.1,0.90.0,,,0.90.0,,,,,,,0,"Created the following test in TestIndexedTable,

  public void testLockedRowDelete() throws IOException {
    writeInitalRows();
    // Delete the first row;
    byte[] row = PerformanceEvaluation.format(0);
    RowLock lock = table.lockRow(row);
    table.delete(new Delete(row, HConstants.LATEST_TIMESTAMP, lock));
    table.unlockRow(lock);    

    assertRowDeleted(NUM_ROWS - 1);  
  }
}

which fails and throws the following exception,

java.io.IOException: java.io.IOException: Invalid row lock
	at org.apache.hadoop.hbase.regionserver.HRegion.getLock(HRegion.java:1621)
	at org.apache.hadoop.hbase.regionserver.HRegion.delete(HRegion.java:1094)
	at org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.delete(IndexedRegion.java:269)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:2014)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:648)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

Patch coded for the latest version in SVN (looks like 0.21.0) , just going through final testing and packaging. Will attach shortly.","Ubunto 9.04 Desktop under VmWare Fusion, Java 1.6.0_16",,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/09 20:53;keiththomas;HBASE-1869.patch;https://issues.apache.org/jira/secure/attachment/12420602/HBASE-1869.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26028,,,,,Fri Nov 20 13:01:39 UTC 2015,,,,,,,,,,"0|i0hfp3:",99805,,,,,,,,,,,,hbase indexedtable rowlock delete,,,,,,,,,"25/Sep/09 20:53;keiththomas;Patch attached, including extra unit test. Have run the (minor) fix through all tests by invoking 'ant clean test javadoc'. ;;;","25/Sep/09 22:48;keiththomas;Patch attached to Jira.;;;","25/Sep/09 23:22;stack;It looks like HBASE-1840 fixed this for the branch.

Thanks for the patch Keith (good to meet you the last night at HUG).;;;","25/Sep/09 23:55;keiththomas;My previous fix HBASE-1840 only stooped this exception being thrown upon updates, unfortunately I didn't think to test deletes. Maybe 1840 was improved upon by others after I submitted it to make it also handle deletions with a row locked. However if my fix was taken as-is then this later fix HBASE-1869 is required to fix both updates and deletions. I hope this makes sense and apologize if my problem descriptions have caused unnecessary confusion.

P.S. It was great to meet you too 'Stack', thanks for your feedback, much appreciated.

;;;","26/Sep/09 21:52;stack;I backported this fix to 0.20 branch.  Thanks Keith.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan(Scan) copy constructor does not copy value of cacheBlocks,HBASE-1866,12436544,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,24/Sep/09 22:49,20/Nov/15 13:01,01/Jul/25 07:49,24/Sep/09 23:20,0.20.0,,,,,0.20.1,0.90.0,,Client,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/09 22:56;streamy;HBASE-1866-v1.patch;https://issues.apache.org/jira/secure/attachment/12420506/HBASE-1866-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26026,Reviewed,,,,Fri Nov 20 13:01:29 UTC 2015,,,,,,,,,,"0|i0hfon:",99803,,,,,,,,,,,,,,,,,,,,,"24/Sep/09 22:56;streamy;Simple patch.  Quick review and I will commit.;;;","24/Sep/09 23:02;stack;+1;;;","24/Sep/09 23:20;streamy;Thanks for review stack.

Also added this on commit:

{code}
@@ -380,6 +381,8 @@
     sb.append("""" + this.maxVersions);
     sb.append("", caching="");
     sb.append("""" + this.caching);
+    sb.append("", cacheBlocks="");
+    sb.append("""" + this.cacheBlocks);
     sb.append("", timeRange="");
     sb.append(""["" + this.tr.getMin() + "","" + this.tr.getMax() + "")"");
     sb.append("", families="");
{code};;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.20.0 TableInputFormatBase NPE,HBASE-1865,12436539,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,cheddar,cheddar,24/Sep/09 21:02,20/Nov/15 13:01,01/Jul/25 07:49,24/Sep/09 23:06,0.20.0,,,,,0.20.1,0.90.0,,,,,,0,"Spot the bug in this code:

public List<InputSplit> getSplits(JobContext context) throws IOException {
    byte [][] startKeys = table.getStartKeys();
    if (startKeys == null || startKeys.length == 0) {
      throw new IOException(""Expecting at least one region."");
    }
    if (table == null) {
      throw new IOException(""No table was provided."");
    }
...
}

Should check if the table is null before calling a method on it.

Admittedly, this isn't the worst bug in the world, it's really just more of a nuisance in that the ""No table was provided"" message becomes an NPE

This bug is in both

org.apache.hadoop.hbase.mapred.TableInputFormatBase
org.apache.hadoop.hbase.mapreduce.TableInputFormatBase",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/09 22:40;stack;1865.patch;https://issues.apache.org/jira/secure/attachment/12420505/1865.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26025,,,,,Fri Nov 20 13:01:37 UTC 2015,,,,,,,,,,"0|i0hfof:",99802,,,,,,,,,,,,,,,,,,,,,"24/Sep/09 22:40;stack;Thanks Eric.  Would this fix it?;;;","24/Sep/09 22:51;cheddar;It actually exists in both the mapreduce package and the mapred package.  But yes, that would fix it :).;;;","24/Sep/09 23:06;stack;Applied branch and trunk (Thanks Eric).;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master can't split logs created by THBase,HBASE-1858,12436324,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,clint.morgan,clint.morgan,clint.morgan,22/Sep/09 17:51,20/Nov/15 13:02,01/Jul/25 07:49,26/Sep/09 21:20,0.20.0,,,,,0.20.1,0.90.0,,master,,,,0,"When master tries to split logs created by THbase, it fails because it tries to read in the wrong key type. (THBase subclasses HLogKey to add fields to the key).

2009-09-16 09:03:01,943 WARN org.apache.hadoop.hbase.regionserver.HLog:
Exception processing
hdfs://domU-12-31-39-07-CC-A2.compute-1.internal:9000/hbase/.logs/domU-12-31-39-07-CC-A2.compute-1.internal,60020,1253103101743/hlog.dat.1253103102168
-- continuing. Possible DATA LOSS!
java.io.IOException: wrong key class:
org.apache.hadoop.hbase.regionserver.HLogKey is not class
org.apache.hadoop.hbase.regionserver.transactional.THLogKey
       at
org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1824)
       at
org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1876)
       at org.apache.hadoop.hbase.regionserver.HLog.splitLog(HLog.java:880)
       at org.apache.hadoop.hbase.regionserver.HLog.splitLog(HLog.java:802)
       at
org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:274)
       at
org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:492)
       at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:426)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/09 23:27;clint.morgan;1858-v2.patch;https://issues.apache.org/jira/secure/attachment/12420511/1858-v2.patch","24/Sep/09 21:01;clint.morgan;1858.patch;https://issues.apache.org/jira/secure/attachment/12420487/1858.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26021,,,,,Fri Nov 20 13:02:00 UTC 2015,,,,,,,,,,"0|i0hfnb:",99797,,,,,,,,,,,,,,,,,,,,,"22/Sep/09 23:29;stack;Do you have a fix for this Clint?;;;","23/Sep/09 01:00;clint.morgan;I have a fix for this (define the HLogKey class in the conf), but I've found some other issues with restoring from trx log that I'm working on. 

Should have something by tomorrow.;;;","24/Sep/09 21:01;clint.morgan;This patch clean up the transaction WALing, and makes it work. 

- Introduce config property ""hbase.regionserver.hlog.keyclass"" which is used to instantiate keys.

- When we commit a transaction, we let the puts/deletes go into the normal WAL. This way, they are handled normally during recovery. The only time we need to do special WAL recovery in the transactional layer is when we find a transaction that started, but does not have an commit or abort message. In this case, its status should be in the ""global"" trx log.

- Clean up the use of the ""global"" transaction log. This holds the state of a transaction while the transaction is still in-process. This state can be forgotten after a successful commit/abort. This state is only used when we recover from the WAL and don't know what actually happened to the transaction.

- Ports HbaseBackTransactionalLogger to the new API.

- fixes a bug: when multiple puts in the same transaction to the same cell, make the last put should be used for a trx-local get.

I tested the WAL recovery and it works for me.;;;","24/Sep/09 23:27;clint.morgan;Forgot to update the test. All tests now pass.;;;","26/Sep/09 00:13;apurtell;+1 will commit if all tests pass. ;;;","26/Sep/09 01:58;apurtell;Committed to trunk only. TestTHLogRecovery fails for me on 0.20:
\\
{code}
Testcase: testWithoutFlush took 30.077 sec
Caused an ERROR
java.io.IOException: Only Puts in BU as of 0.20.0
java.lang.RuntimeException: java.io.Exception: Only Puts in BU as of 0.20.0
at org.apache.hadoop.hbase.client.transactional.HBaseBackedTransactionLogger.forgetTransaction
(HBaseBackedTransactionLogger.java:136)
at org.apache.hadoop.hbase.client.transactional.TransactionManager.doCommit
(TransactionManager.java:192)
at org.apache.hadoop.hbase.client.transactional.TransactionManager.tryCommit
(TransactionManager.java:154)
at org.apache.hadoop.hbase.regionserver.transactional.TestTHLogRecovery.testWithoutFlush
(TestTHLogRecovery.java:118)
{code};;;","26/Sep/09 20:26;stack;Backporting Jon's fixes to TestTHLogRecover in trunk where he updates to new API seems to make it pass the test.  I'm running all tests now.;;;","26/Sep/09 21:20;stack;Applied to branch.  Closing.  Thanks for the patch Clint.;;;","08/Sep/10 15:38;gstathis;Folks, unless there is something special that needs to be done when configuring transactional tables, it unfortunately seems that this problem is still occurring in 0.20.3:

{noformat}
2010-08-27 15:53:06,030 DEBUG regionserver.HLog - Splitting hlog 2 of 31: hdfs://domU-12-31-39-18-19-65.compute-1.internal:9000/hbase/.logs/domU-12-31-39-18-15-24.compute-1.internal,60020,1282782970585/
hlog.dat.1282798940318, length=9831249
2010-08-27 15:53:06,033 DEBUG regionserver.HLog - IOE Pushed=0 entries from hdfs://domU-12-31-39-18-19-65.compute-1.internal:9000/hbase/.logs/domU-12-31-39-18-15-24.compute-1.internal,60020,128278297058
5/hlog.dat.1282798940318
2010-08-27 15:53:06,033 WARN  regionserver.HLog - Exception processing hdfs://domU-12-31-39-18-19-65.compute-1.internal:9000/hbase/.logs/domU-12-31-39-18-15-24.compute-1.internal,60020,1282782970585/hlo
g.dat.1282798940318 -- continuing. Possible DATA LOSS!
java.io.IOException: wrong key class: org.apache.hadoop.hbase.regionserver.HLogKey is not class org.apache.hadoop.hbase.regionserver.transactional.THLogKey
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1824)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1876)
	at org.apache.hadoop.hbase.regionserver.HLog.splitLog(HLog.java:966)
	at org.apache.hadoop.hbase.regionserver.HLog.splitLog(HLog.java:872)
	at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:286)
	at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)
{noformat}

I'm not sure though that the ""define the HLogKey class in the conf"" part means, so I might be missing that...;;;","13/Sep/10 19:09;jk-public@troove.net;You do need to specify the HLogKey instance in hour config:

<property>
	    <name>hbase.regionserver.hlog.keyclass</name>
	    <value>org.apache.hadoop.hbase.regionserver.transactional.THLogKey</value>
</property>

I apologize for the poor documentation in the hbase-transactional-indexed project.  It is not mature yet though we have significant updates going in soon that
are compatible with HBase 0.89.  Note that in that version you will NOT need to specify THLogKey in the config.

;;;","13/Sep/10 21:21;gstathis;James, do you know if I can make the hbase.regionserver.hlog.keyclass config change on a cluster that already has transactional tables with data in them or do I need some sort of data conversion process now?;;;","14/Sep/10 00:09;jk-public@troove.net;Well the THLog.class does overwrite the read/write serialization methods to add it's own fields. So if you've already serialized HLogs you may run into trouble.

I can say for sure though what will happen without digging further.
I suggest you try without conversion and see what happens.
Backup your data first though.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WrongRegionException when setting region online after .META. split,HBASE-1857,12436318,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,clehene,clehene,clehene,22/Sep/09 16:25,20/Nov/15 13:02,01/Jul/25 07:49,22/Sep/09 19:23,0.20.0,,,,,0.20.1,,,master,,,,0,"After splitting .META. when updating region information in .META. (e.g. ProcessRegionOpen) the wrong .META. region was retrieved in RegionManager from onlineMetaRegions map. 
This is due to a bug in RegionManager.getFirstMetaRegionForRegion that was using the wrong key to get data out of the map (the table name instead of the region name) 

return onlineMetaRegions.get(onlineMetaRegions.headMap(newRegion.getTableDesc().getName()).lastKey());

and when adding the region to the map it was added with 

onlineMetaRegions.put(metaRegion.getStartKey(), metaRegion);

so it's supposed to be taken out with: 
return onlineMetaRegions.get(onlineMetaRegions.headMap(newRegion.getRegionName()).lastKey());
",OSX/Linux,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,"22/Sep/09 16:27;clehene;0001-fixed-WrongRegionException-bug-when-splitting-.META.patch;https://issues.apache.org/jira/secure/attachment/12420282/0001-fixed-WrongRegionException-bug-when-splitting-.META.patch","22/Sep/09 16:27;clehene;0002-Adding-tests-for-WrongRegionException-bug-when-split.patch;https://issues.apache.org/jira/secure/attachment/12420281/0002-Adding-tests-for-WrongRegionException-bug-when-split.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,5,Reviewed,,,,Fri Nov 20 13:02:05 UTC 2015,,,,,,,,,,"0|i00xbb:",3329,,,,,,,,,,,,,,,,,,,,,"22/Sep/09 16:27;clehene;Adding a unit test and the patch. 

The unit test is semi-functional actually. It doesn't split the .META. table, but rather using the RegionManager methods to put and get regions in and out of the map. ;;;","22/Sep/09 19:23;stack;Committed to trunk and branch.  Thanks for the patch Cosmin.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-1765 broke MapReduce when using Result.list(),HBASE-1856,12436283,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,larsgeorge,larsgeorge,larsgeorge,22/Sep/09 08:31,20/Nov/15 13:01,01/Jul/25 07:49,22/Sep/09 18:49,0.20.1,0.90.0,,,,0.20.1,,,,,,,0,"Not sure if it is just me, but using MR over HBase employing a TableReducer is not working. After the first row is read all subsequent rows get the same Result's of that very first row. After tracing this from the Map phase I found the culprit in Result and the HBASE-1765 delayed field parsing change.

This is the code I use in the reduce():

{code}
   @Override
    protected void reduce(ImmutableBytesWritable key, Iterable<Result> values,
        Context context) throws IOException, InterruptedException {
      String skey = Bytes.toString(key.get());
      context.getCounter(CountersTotals.ROWS).increment(1);
      for (Result result : values) {
        for (KeyValue kv: result.list()) {
          try {
            if (LOG.isDebugEnabled()) LOG.debug(""reduce: key -> "" + skey + "", kv -> "" + kv);
            ...
{code}

Here is the current list() implementation:

{code}
  public List<KeyValue> list() {
    if(this.kvs == null) {
      readFields();
    }
    return isEmpty()? null: Arrays.asList(sorted());
  }
{code}

The problem is that readFields(DataInput) does not clear kvs!

{code}
  public void readFields(final DataInput in)
  throws IOException {
    familyMap = null;
    row = null;
    int totalBuffer = in.readInt();
    if(totalBuffer == 0) {
      bytes = null;
      return;
    }
    byte [] raw = new byte[totalBuffer];
    in.readFully(raw, 0, totalBuffer);
    bytes = new ImmutableBytesWritable(raw, 0, totalBuffer);
  }
{code}

The above is called by the MR framework's WritableSerialization for each map output. But since ""kvs"" is already set ""list()"" returns the old data!

I assume the only change needed is clearing kvs as well:

{code}
  public void readFields(final DataInput in)
  throws IOException {
    familyMap = null;
    row = null;
    kvs = null;
    ....
{code}

I'll test that now and report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/09 16:29;stack;1856.patch;https://issues.apache.org/jira/secure/attachment/12420283/1856.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26020,Reviewed,,,,Fri Nov 20 13:01:23 UTC 2015,,,,,,,,,,"0|i0hfn3:",99796,,,,,,,,,,,,,,,,,,,,,"22/Sep/09 14:21;stack;Good man Lars.  I found this too and have the fix you describe above in as part of hbase-1815 IIRC.;;;","22/Sep/09 15:21;larsgeorge;Hrmm, can't see it in HBASE-1815 though. Did you mean a different issue?;;;","22/Sep/09 16:29;stack;Assigning Lars (he found it -- I can't find my 'fix' in hbase-1815 and its not in any local repo here...).;;;","22/Sep/09 16:44;larsgeorge;+1 with a minor comment on using ""this."". famliyMap and row is used without it, so I would do the same for kvs?;;;","22/Sep/09 18:49;stack;Committed branch and trunk (with Lars's suggestion).;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete latest of a null qualifier when non-null qualifiers exist throws a RuntimeException,HBASE-1847,12435898,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,17/Sep/09 00:44,20/Nov/15 13:01,01/Jul/25 07:49,17/Sep/09 21:17,0.20.0,,,,,0.20.1,0.90.0,,regionserver,,,,0,Bug in delete latest code when deleting the null qualifier column when other columns also exist.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/09 00:55;streamy;HBASE-1847-v1.patch;https://issues.apache.org/jira/secure/attachment/12419835/HBASE-1847-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26013,Reviewed,,,,Fri Nov 20 13:01:53 UTC 2015,,,,,,,,,,"0|i0hflj:",99789,,,,,,,,,,,,,,,,,,,,,"17/Sep/09 00:55;streamy;Adds a few lines in TestClient.testDeletes that reproduces bug.

Patch fixes bug and test now passes.;;;","17/Sep/09 00:55;streamy;Please review.;;;","17/Sep/09 01:13;stack;+1 but use empty byte array constant from hconstants on commit;;;","17/Sep/09 01:19;streamy;Good point.  Unfortunately I put new byte[0] all over HBASE-1822... it's shorter ;);;;","17/Sep/09 21:17;streamy;Committed to branch and trunk.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowLock fails when used with IndexTable,HBASE-1840,12435749,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,keiththomas,keiththomas,keiththomas,15/Sep/09 19:29,20/Nov/15 13:02,01/Jul/25 07:49,15/Sep/09 20:23,0.20.0,,,,,0.20.1,0.90.0,,,,,,0,"The following exception is thrown when using RowLock to update a row in an IndexedTable:

    [junit] java.io.IOException: java.io.IOException: Invalid row lock
    [junit] at org.apache.hadoop.hbase.regionserver.HRegion.getLock(HRegion.java:1640)
    [junit] at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1244)
    [junit] at org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.put(IndexedRegion.java:97)
    [junit] at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1216)
    [junit] at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1818)
    [junit] at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
    [junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
    [junit] at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

NOTE #1: Line numbers in stacktrace may not make sense because I've been hacking in loads of debug info. 

NOTE #2: I attaching a fix which includes unit tests
","Ubuntu desktop 9.0.4, Sun Java 1.6.0_16, HBase 0.20.0, Hadoop 0.20.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/09 19:32;keiththomas;HBASE-1840.patch;https://issues.apache.org/jira/secure/attachment/12419671/HBASE-1840.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26010,,,,,Fri Nov 20 13:02:05 UTC 2015,,,,,,,,,,"0|i0hfk7:",99783,,,,,,,,,,,,RowLock IndexedTable IndexedRegion TestIndexedTable,,,,,,,,,"15/Sep/09 19:32;keiththomas;Patch attached.;;;","15/Sep/09 19:48;stack;Bringing into 0.20.1.  Patch looks good to me.  Will commit in a bit.;;;","15/Sep/09 20:03;streamy;Patch looks good to me as well.  Thanks Keith!;;;","15/Sep/09 20:23;stack;Committed branch and trunk.  Thanks for the patch Keith.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scanning API must be reworked to allow for fully functional Filters client-side,HBASE-1831,12435494,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,streamy,streamy,12/Sep/09 01:06,20/Nov/15 13:01,01/Jul/25 07:49,06/Oct/09 03:26,0.20.0,,,,,0.20.1,0.90.0,,,,,,1,"Right now, a client replays part of the Filter locally by calling filterRowKey() and filterAllRemaining() to determine whether it should continue to the next region.

A number of new filters rely on filterKeyValue() and other calls to alter state.  It's also a false assumption that all rows/keys affecting a filter returning true for FAR will be seen client-side (what about those that failed the filter).

This issue is about dealing with Filters properly from the client-side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/09 00:32;stack;1831-v2.patch;https://issues.apache.org/jira/secure/attachment/12420516/1831-v2.patch","27/Sep/09 22:54;stack;1831-v3.patch;https://issues.apache.org/jira/secure/attachment/12420667/1831-v3.patch","02/Oct/09 04:22;stack;1831-v4.patch;https://issues.apache.org/jira/secure/attachment/12421097/1831-v4.patch","02/Oct/09 05:07;stack;1831-v5.patch;https://issues.apache.org/jira/secure/attachment/12421098/1831-v5.patch","05/Oct/09 23:36;apurtell;1831-v6.patch;https://issues.apache.org/jira/secure/attachment/12421353/1831-v6.patch","23/Sep/09 20:44;stack;1831.patch;https://issues.apache.org/jira/secure/attachment/12420403/1831.patch",,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26006,,,,,Fri Nov 20 13:01:47 UTC 2015,,,,,,,,,,"0|i0hfi7:",99774,,,,,,,,,,,,,,,,,,,,,"23/Sep/09 04:35;streamy;Filters should not be run on the client-side at all.

Server needs to be able to tell whether Scan should continue to the next region or not.;;;","23/Sep/09 05:04;stack;Chatted with Jon.

1. Need to flag client to STOP.  HTable#next internally uses the batch version of next (so can do prefetching of rows).  An empty list of Results is always sent -- never null.   We'll add passing null as the flag that filter is done; do not move to next region (Client has code to handle null list so if an old hbase version connects, it won't break; it'll just not do the STOP properly).
2. There is a non-batch next in the ipc interface.  I was thinking of deprecating it and moving internals to use the batch interface only, but these internal uses of scanners do not carry filters so will just leave them for 0.20.1.
3. Filters carry state.   How do we get the state across region transitions?  Again chatting with Jon, will do the following.  If a Scanner has a filter, and we got back a non-empty list, its time to move to the next region.  Just before we move to the next region, we'll make another call to the old server -- Scanner.getFilter -- whose result is the deserialized filter.  The deserialized filter will be passed then to the next region.  In this manner filters will be able to carry their state forward.  Downside is extra RPC call IF scanning with a filter.;;;","23/Sep/09 19:32;stack;On 3., above, carrying stateful filters across regions, it can't be done easily in 0.20.x because we can get a NotServingRegionException at any time.  Also, if nothing goes wrong and we just exhaust a scanner on a particular region, the last thing done over in RegionServer is cleanup of scanner so a subsequent getFilter call would have nothing to pick up on once it'd arrived at the RegionServer.  Statefulness has to be done client-side; filters need to allow specifying a client-component.;;;","23/Sep/09 19:42;stack;@jgray You say, ""Filters should not be run on the client-side at all.""

I don't know how else we can do stateful scanners that ride over regions.;;;","23/Sep/09 19:57;stack;So, chatting w/ Ryan on this topic, he suggests that support for stateful filters is going to be a bear -- there's splitting and then what if regionserver crashes, etc.  At the moment we figure a filter wants to stop the scan by testing the last regions endrow against the filter run client-side.   Would it be enough my removing this and just adding the flag above which allows filters server-side to stop the scan (by passing back null result out of batch next)?  There'd be no getFilter to pick up a filters state and pass it from one region to the next.

There is going to be a problem though when fellas want to do filters that return 20 row results only or that want to have a filter skip 1000 rows; to do this, we'd something to run client-side.;;;","23/Sep/09 20:44;stack;Here is a bit of a start.  It adds javadoc to the ipc interfaces about new meaning of null.  It then adds a new flag to the client-side nextScanner method.  It renames the method that checks for the end row in Scan....  Now to work on server-side.;;;","24/Sep/09 00:06;streamy;I agree that this is not going to be easy.  Still think that filters should not be run client-side (even if they are, it will require additional/new information to be sent from client to server).

Running an offset of 1000 rows client-side completely negates the value in using filters in the first place... to not have to send back all that data.  That's the reason client-side filters don't really work, they require you to send back otherwise filtered-out data.

What I would like to see as a long-term solution to this:

1st: Add ability for server to say STOP and not go to next region
2nd: Correctness for stateful scanners under non-split, non-failure scenarios (less correctness / fail-fast if encountering issues)
3rd: Correctness/robustness for stateful scanners under splits and failures;;;","25/Sep/09 00:32;stack;v2... complete but for test.  Working on that now.;;;","27/Sep/09 22:54;stack;Still not done;;;","02/Oct/09 04:22;stack;Adds two ugly tests.  One under filter that puts up three regions and then checks at the Region level that filters are doing right thing (Why can't i instantiate an HRegionServer and test from its interface -- its currently way too hard to put one of these up... requires there be a master.. .it shouldn't).  Other test if ugly from client side.  Splits table then makes sure RowFilter is returning right results around the row boundary.  I can assert counts but I can't assert that only a subset of regions are being accessed with asserts.  To do the latter, I added logging and it required eyeballing but you can see in the logs that yes we do not go to next region if filter says we're done.

A few tests seem to be failing.... Looking into it.;;;","02/Oct/09 05:07;stack;This version passes all tests.;;;","03/Oct/09 05:18;stack;Needs review.;;;","05/Oct/09 23:36;apurtell;Testing this now. There were two rejects against latest SVN 0.20, one in HTable, one in HConnectionManager. I fixed them up and attached the result as -v6;;;","06/Oct/09 00:30;apurtell;+1 all tests pass here;;;","06/Oct/09 03:26;stack;Thanks for review Andrew.  Committed branch and trunk.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompareFilters are broken from client-side,HBASE-1828,12435387,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,11/Sep/09 03:10,20/Nov/15 13:01,01/Jul/25 07:49,12/Sep/09 01:02,0.20.0,,,,,0.20.1,0.90.0,,Filters,,,,0,Some filters pass region-level tests but seem to freeze client-side.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/09 03:12;streamy;HBASE-1828-FailTest.patch;https://issues.apache.org/jira/secure/attachment/12419260/HBASE-1828-FailTest.patch","12/Sep/09 00:57;streamy;HBASE-1828-v2.patch;https://issues.apache.org/jira/secure/attachment/12419361/HBASE-1828-v2.patch","11/Sep/09 15:55;jdcryans;HBASE-1828.patch;https://issues.apache.org/jira/secure/attachment/12419314/HBASE-1828.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26005,,,,,Fri Nov 20 13:01:14 UTC 2015,,,,,,,,,,"0|i0hfhr:",99772,,,,,,,,,,,,,,,,,,,,,"11/Sep/09 03:12;streamy;Test fails.  Freezes up and throws:

{noformat}
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 192.168.249.50:3928 for region testFilters,,1252638503066, row '', but failed after 3 attempts.
Exceptions:
java.io.IOException: Call to /192.168.249.50:3928 failed on local exception: java.io.EOFException
java.io.IOException: Call to /192.168.249.50:3928 failed on local exception: java.io.EOFException
java.io.IOException: Call to /192.168.249.50:3928 failed on local exception: java.io.EOFException
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:994)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1880)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1826)
	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:372)
	at org.apache.hadoop.hbase.client.TestClient.testFilters(TestClient.java:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
....
{noformat};;;","11/Sep/09 15:44;jdcryans;The real exception was eaten in HBaseServer.Listener.doRead:

{code}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:365)
	at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:347)
	at org.apache.hadoop.hbase.filter.CompareFilter.readFields(CompareFilter.java:133)
	at org.apache.hadoop.hbase.client.Scan.readFields(Scan.java:523)
	at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:418)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invocation.readFields(HBaseRPC.java:175)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:873)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:841)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:424)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.run(HBaseServer.java:319)
{code};;;","11/Sep/09 15:55;jdcryans;With this the test is green, I copied what was done in ColumnValueFilter.;;;","11/Sep/09 17:52;streamy;So what seems to be broken are the changes I made to HOW to allow null HBCs to be passed.;;;","12/Sep/09 00:57;streamy;I want to get this working in branch and trunk.  Will open new issue to deal with properly.  This patch should fix for now.  Instantiates a single HBC in a static so we don't have to redo each time.;;;","12/Sep/09 01:02;streamy;Opened HBASE-1830 to deal with the real issue here.

Committed to branch and trunk.  Thanks for investigation JD.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseClient can get stuck in an infinite loop while attempting to contact a failed regionserver,HBASE-1815,12434787,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,justinlynn,justinlynn,03/Sep/09 22:25,20/Nov/15 13:01,01/Jul/25 07:49,22/Sep/09 16:34,0.20.0,,,,,0.20.1,,,Client,,,,0,"While using HBase Thrift server, if a regionserver goes down due to shutdown or failure clients will timeout because the thrift server cannot contact the dead regionserver.","Ubuntu Linux (Linux <elided> 2.6.24-23-generic #1 SMP Wed Apr 1 21:43:24 UTC 2009 x86_64 GNU/Linux), java version ""1.6.0_06"", Java(TM) SE Runtime Environment (build 1.6.0_06-b02), Java HotSpot(TM) 64-Bit Server VM (build 10.0-b22, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/09 22:04;stack;hbaseclient-v3.patch;https://issues.apache.org/jira/secure/attachment/12420087/hbaseclient-v3.patch","18/Sep/09 00:13;stack;ipctimeout.patch;https://issues.apache.org/jira/secure/attachment/12419948/ipctimeout.patch","03/Sep/09 22:30;justinlynn;thrift_server_log_excerpt;https://issues.apache.org/jira/secure/attachment/12418552/thrift_server_log_excerpt","03/Sep/09 22:30;justinlynn;thrift_server_threaddump;https://issues.apache.org/jira/secure/attachment/12418551/thrift_server_threaddump","03/Sep/09 22:39;justinlynn;thrift_server_threaddump_1;https://issues.apache.org/jira/secure/attachment/12418554/thrift_server_threaddump_1",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25999,Reviewed,,,,Fri Nov 20 13:01:49 UTC 2015,,,,,,,,,,"0|i0hfev:",99759,,,,,,,,,,,,,,,,,,,,,"03/Sep/09 22:30;justinlynn;These are the thrift server threaddumps and log files from the time when the failure was noticed.;;;","03/Sep/09 22:39;justinlynn;Another thread dump.;;;","03/Sep/09 22:57;stack;Working w/ JSharp, looking in the thread dumps, it looks like each thread has to do ten retries sleeping a second between each retry.  When many threads, we get a lot of messages in the log about the failure to connect.  Need to recognize dead-remote-side and handle it promptly.;;;","03/Sep/09 23:05;apurtell;Clients can monitor RS liveness via ZK and respond quickly via watches? ;;;","15/Sep/09 22:42;stack;HBaseClient also has this issue from list:

Yeah, this is down in guts of the hadoop rpc we use.  Around connection setup it has its own config. that is not well aligned with ours (ours being the retries and pause settings)

The maxretriies down in ipc is

this.maxRetries = conf.getInt(""ipc.client.connect.max.retries"", 10);

Thats for an IOE other than timeout.  For timeout, it does this:

          } catch (SocketTimeoutException toe) {
            /* The max number of retries is 45,
             * which amounts to 20s*45 = 15 minutes retries.
             */
            handleConnectionFailure(timeoutFailures++, 45, toe);

Let me file an issue to address the above.  The retries should be our retries... and in here it has a hardcoded 1000ms that instead should be our pause.... Not hard to fix.;;;","18/Sep/09 00:13;stack;This patch might be a bit radical, but here it goes.

High-level motivation is undo retrying and sleeps down in ipc; let retrying be done at a higher level up in the hbase client.

In ipc, socket setup had a timeout of 20 seconds.  Ipc then retries the socket setup ten times with a 1 second sleep in between.  Thats 210seconds  or so before we timeout down in the guts of RPC.  We then go up to the retry logic in hbase (usually, not always), and then do ten retries with a 2 second retry in between (If a SocketTimeoutException exception setting up the connection, we'd retry a hard-coded 45 times; i.e. 15 minutes).

In Justin's case, I don't think we were doing SocketTimeoutException going by the stack trace.  It was more the 210 seconds per thread but my guess is  that his thrift client had probably timed out already.

This patch turns off retry down in the ipc client (let the upper-layers do retry), changes hard-coded sleep times to be hbase.client.pause time (2 seconds), and removes the 45 hard-coding,   It also adds an hbase prefix to the ipc configuration parameters in case we want different values from hadoop.

Let me try out this patch.  My guess is that there are places in hbase where we don't retry because we were dependent on ipc doing retry for us.  Let me find those.;;;","18/Sep/09 22:04;stack;This version adds cleanup.

In HRegionServer main run loop, wait before retrying rather than just run all retries without pause.

Changed the HBaseRPC RetriesExhaustedException so its about failure to get proxy instead of a wonky message about unknown row.

Move the get of a regionserver connection into the try/catch so if fails, its retried.

This patch changes how our retrying from client and from servers works.  I tested up on a cluster and it seems more regular and 'live' now than previous but I may have missed cases where we used to rely on the rpc retry.  I'm not sure how to find those other than to commit and wait till someone complains.

Review appreciated.;;;","19/Sep/09 16:05;stack;I had this patch installed in my overnight loading.  The upload worked about same as usual so this patch doesn't seem to change basic workings.;;;","19/Sep/09 16:15;stack;All unit tests pass.;;;","22/Sep/09 15:04;jdcryans;+1 patch seems good. Apart from unit tests and loading, did you try killing some region servers?;;;","22/Sep/09 15:53;stack;Yes.  I should have so.  I killed master and watched what regionservers did.  I also killed the cluster and watched client.   It all seems to run more regularly.    Less weird retrying.   


Thanks for review. ;;;","22/Sep/09 16:34;stack;Committed branch and trunk.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in region assignment,HBASE-1810,12434610,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,herberts,larsgeorge,larsgeorge,02/Sep/09 10:39,13/Sep/09 22:24,01/Jul/25 07:49,02/Sep/09 16:40,0.20.0,,,,,0.20.0,,,,,,,0,"{code}
2009-09-01 11:28:24,106 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 16 on 60000, call regionServerReport(address: 192.168.105.39:60020, startcode: 1251828463043, load: (requests=0, regions=21, usedHeap=135, maxHeap=4093), [Lorg.apache.hadoop.hbase.HMsg;@6556c280, [Lorg.apache.hadoop.hbase.HRegionInfo;@22fb957a) from 192.168.105.39:60281: error: java.io.IOException: java.util.ConcurrentModificationException
java.io.IOException: java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
        at org.apache.hadoop.hbase.master.RegionManager.isMetaServer(RegionManager.java:837)
        at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:405)
        at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:202)
        at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:481)
        at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:415)
        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:324)
        at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:722)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
{code}

and 

{code}
2009-09-01 11:28:25,713 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 23 on 60000, call regionServerReport(address: 192.168.105.37:60020, startcode: 1251828463132, load: (requests=0, regions=14, usedHeap=141, maxHeap=4093), [Lorg.apache.hadoop.hbase.HMsg;@68345260, [Lorg.apache.hadoop.hbase.HRegionInfo;@430c5212) from 192.168.105.37:35432: error: java.io.IOException: java.util.ConcurrentModificationException
java.io.IOException: java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
        at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:428)
        at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:202)
        at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:481)
        at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:415)
        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:324)
        at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:722)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
{code}

As discussed with Mathias on IRC:

<hbs> regionsInTransition is a synchronized map so accesses to it are synchronized but not looping over it
<hbs> as stated in http://java.sun.com/j2se/1.5.0/docs/api/java/util/Collections.html#synchronizedMap%28java.util.Map%29
<hbs> so there is a missing synchronized(regionsInTransition) wherever regionsInTransition is iterated over.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/09 10:43;herberts;HBASE-1810.patch;https://issues.apache.org/jira/secure/attachment/12418366/HBASE-1810.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25996,Reviewed,,,,Wed Sep 02 16:40:26 UTC 2009,,,,,,,,,,"0|i0hfdz:",99755,,,,,,,,,,,,,,,,,,,,,"02/Sep/09 10:43;herberts;Enclosed iteration of regionsInTransition in 'synchronized(regionsInTransition)' block.;;;","02/Sep/09 14:39;stack;hmm.... Looking at the regionsInTransistion set, I see that its synchronized sometimes but not always (I'm talking about before Mathias's patch is applied).  Mathias's patch would seem to make sense.... Let me check java src to ensure that same monitor is effected when you do synchronized(regionsInTransistion) as is used when you do Collections.synchronizedSortedMap get /put;;;","02/Sep/09 16:40;stack;I looked at java src.  Internally, Collections.synchronizedSet, etc., use the same mutex as is used when you do synchronized(object) where object is the synchronizedSet, etc.

I reviewed the rest of RegionManager.  regionsInTransition seems to be safely synchronized with Mathias's patch.

Committed branch and trunk.  Thanks for the patch Mathias.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puts are permitted (and stored) when including an appended colon,HBASE-1804,12434449,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,31/Aug/09 19:52,13/Sep/09 22:24,01/Jul/25 07:49,02/Sep/09 18:58,0.20.0,,,,,0.20.0,0.90.0,,,,,,0,"If I have a table with family ""testFamily"", currently I can do Puts using the new API by specifying the family name with or without a colon.  The KV is then stored w/ or w/o depending on how the Put was done.

If you try to Put.add(""testFamily:"", ""qualifier"", ""value"") this should throw a NoSuchColumnFamilyException",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/09 06:06;stack;1804-v2.patch;https://issues.apache.org/jira/secure/attachment/12418342/1804-v2.patch","02/Sep/09 18:12;stack;1804-v3.patch;https://issues.apache.org/jira/secure/attachment/12418418/1804-v3.patch","31/Aug/09 20:27;streamy;HBASE-1804-v1.patch;https://issues.apache.org/jira/secure/attachment/12418174/HBASE-1804-v1.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25993,Reviewed,,,,Wed Sep 02 18:58:51 UTC 2009,,,,,,,,,,"0|i0hfcn:",99749,,,,,,,,,,,,,,,,,,,,,"31/Aug/09 20:19;streamy;Issue is because we use a special FAMILY_COMPARATOR for the HTD map... It currently ignores the delimiter when doing the comparison.  IMO, we should not ignore it, if we support old API stuff with delimiters, we should be stripping them before we do any family checking.

{code}
  public static final RawComparator<byte []> FAMILY_COMPARATOR =
    new RawComparator<byte []> () {
      public int compare(byte [] a, int ao, int al, byte [] b, int bo, int bl) {
        int indexa = KeyValue.getDelimiter(a, ao, al, COLUMN_FAMILY_DELIMITER);
        if (indexa < 0) {
          indexa = al;
        }
        int indexb = KeyValue.getDelimiter(b, bo, bl, COLUMN_FAMILY_DELIMITER);
        if (indexb < 0) {
          indexb = bl;
        }
        return Bytes.compareTo(a, ao, indexa, b, bo, indexb);
      }

      public int compare(byte[] a, byte[] b) {
        return compare(a, 0, a.length, b, 0, b.length);
      }
    };
{code};;;","31/Aug/09 20:27;streamy;Removes usage of FAMILY_COMPARATOR.  Replaces with straight binary compare, Bytes.BYTES_COMPARATOR.

Needs to be tested and discussed.  I don't see why or where we expect or need to be tolerant of a family with a colon.  It should be stripped from old api stuff, and is not even part of the equation of the new api.

This does fix the original bug.  If you run a new API put with family: it will throw no such column family exception.;;;","31/Aug/09 20:28;streamy;Please review.;;;","31/Aug/09 20:32;streamy;This breaks stuff.;;;","31/Aug/09 21:53;stack;Does your patch work?  Seems odd that it would.  Is family comparator used elsewhere?  Should it be purged?;;;","31/Aug/09 22:21;streamy;As I said, it breaks stuff.

There doesn't seem to be a fixed contract here.  Sometimes we pass it ""family"" other times we pass it ""family:qualifier"".

We should only ever be passing it ""family"" and should never pass it with a colon... We don't store the colon, we don't use the colon in the API, it's leftover from the old API.

I will spend more time digging in where we'd need to change things.

Family comparator is used in the HRegion map of Stores and the HTD map of Families... in both cases, we should be able to deal strictly with ""family"", there just seems to be some old API calls making it's way down to HR and HTD.;;;","01/Sep/09 15:35;jdcryans;Shouldn't we just strip the colon in Put, Delete, Get, Scan when passed-in in a family,qualifier method? Or even throw an exception?;;;","01/Sep/09 16:06;streamy;Yes.  I'm working on a patch that does that, just need to run a lot of tests because it's hard to track down all the old API instances internally and in the tests themselves.;;;","02/Sep/09 06:06;stack;Here's a v2.  Running tests now.  Its Jon's patch only it purges the family comparator from KV and it adds checks to Put, Get, Scan and Delete that throw exception if family is 'illegal'.;;;","02/Sep/09 06:12;streamy;Is it necessary to add these client-side checks in the new API?  Should have checks when creating HTD/HCD but if you put something wonky in normal methods you'll get NoSuchFam.

The change of comparator does break some of the old API tests in the client package.  In calls like getClosest... we need to run KV.parseColumn() and remove the : and qualifier from family.;;;","02/Sep/09 06:20;streamy;hmm... tests are passing for me now.;;;","02/Sep/09 14:28;stack;The old api tests failed for me w/ NPEs.   Will remove the additions to Put/Get/Scan/Delete that I added.  Shouldn't be double checking.;;;","02/Sep/09 15:00;streamy;I must have missed the patch somehow.  You should be able to add a few KV.parseColumn calls inside old-API HTable code so that we don't send something that's supposed to just be ""family"" with a color or qualifier;;;","02/Sep/09 18:12;stack;Adds test to TestHRegion that checks whether Put does right thing when family has a colon.

Adds handler to old api getclosest to handle case where passed a ':' on family.

Most of rest of patch is like jgray's original.  I removed the check in Put/Get/Scan/Delete.;;;","02/Sep/09 18:19;streamy;+1.  thanks for cleaning up the patch, stack;;;","02/Sep/09 18:21;jdcryans;+1;;;","02/Sep/09 18:58;stack;Thanks for review lads.  Committed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Regression] Unable to delete a row in the future,HBASE-1798,12434074,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,26/Aug/09 18:20,13/Sep/09 22:24,01/Jul/25 07:49,27/Aug/09 00:58,0.20.0,,,,,0.20.0,0.90.0,,,,,,0,Deleting in the future doesn't work because KV resets everything to now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/09 22:02;jdcryans;HBASE-1798-2.patch;https://issues.apache.org/jira/secure/attachment/12417818/HBASE-1798-2.patch","27/Aug/09 00:52;jdcryans;HBASE-1798-3.patch;https://issues.apache.org/jira/secure/attachment/12417835/HBASE-1798-3.patch","26/Aug/09 21:02;ryanobjc;HBASE-1798-ryan.patch;https://issues.apache.org/jira/secure/attachment/12417797/HBASE-1798-ryan.patch","26/Aug/09 19:23;streamy;HBASE-1798-v1.patch;https://issues.apache.org/jira/secure/attachment/12417775/HBASE-1798-v1.patch","26/Aug/09 18:21;jdcryans;HBASE-1798.patch;https://issues.apache.org/jira/secure/attachment/12417765/HBASE-1798.patch",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25988,Reviewed,,,,Thu Aug 27 00:58:08 UTC 2009,,,,,,,,,,"0|i0hfbb:",99743,,,,,,,,,,,,,,,,,,,,,"26/Aug/09 18:21;jdcryans;This patch solves the first problem and demonstrates the second.;;;","26/Aug/09 19:23;streamy;Unit test passes.  Shows the behavior.

If you delete at a timestamp, new puts to that same timestamp do not show up.;;;","26/Aug/09 21:02;ryanobjc;this pair of modified tests pass.;;;","26/Aug/09 22:01;jdcryans;Changing the description of the issue, it's only about deleting in the future.;;;","26/Aug/09 22:02;jdcryans;This patch does 2 things:

 - First it changes KV to only reset to now if LATEST_TIMESTAMP.
 - In HRegion, if you delete a row without specifying families, it was eating the timestamp you may have set.;;;","26/Aug/09 22:59;ryanobjc;looking good JD, this is a patch i can heartily approve.

+1;;;","27/Aug/09 00:52;jdcryans;I will commit this patch with a new test for HRegion.;;;","27/Aug/09 00:58;jdcryans;Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"log recovery doesnt reset the max sequence id, new logfiles can get tossed as 'duplicates'",HBASE-1795,12434001,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,26/Aug/09 01:02,20/Nov/15 13:01,01/Jul/25 07:49,11/Sep/09 19:53,0.20.0,0.90.0,,,,0.20.1,0.90.0,,,,,,0,"during log recovery, we dont reset Store.maxSeqId, thus new log entries are stamped starting off from the old files.  This can cause a problem if we fail and recover again, since the new mutations are deemed ""old"" and shouldnt be applied in a subsequent recovery scenario.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/09 23:33;ryanobjc;HBASE-1795.patch;https://issues.apache.org/jira/secure/attachment/12419239/HBASE-1795.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25986,,,,,Fri Nov 20 13:01:58 UTC 2015,,,,,,,,,,"0|i0hfan:",99740,,,,,,,,,,,,,,,,,,,,,"10/Sep/09 23:33;ryanobjc;also fixes 1794;;;","10/Sep/09 23:37;jdcryans;+1;;;","11/Sep/09 19:20;stack;+1 committing....;;;","11/Sep/09 19:53;stack;Committed branch and trunk;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Regression] HTable.get/getRow with a ts is broken,HBASE-1793,12433973,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,25/Aug/09 19:37,13/Sep/09 22:24,01/Jul/25 07:49,26/Aug/09 00:05,0.20.0,,,,,0.20.0,0.90.0,,,,,,0,"If using the old API with 0.20, the behavior of get and getRow is changed when setting a timestamp. Previously, setting a ts was working like a time range and now it works like an exact time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 23:07;jdcryans;HBASE-1793-2.patch;https://issues.apache.org/jira/secure/attachment/12417675/HBASE-1793-2.patch","25/Aug/09 19:50;jdcryans;HBASE-1793.patch;https://issues.apache.org/jira/secure/attachment/12417647/HBASE-1793.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25984,Reviewed,,,,Wed Aug 26 00:05:08 UTC 2009,,,,,,,,,,"0|i0hfa7:",99738,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 19:39;streamy;Spoke with JD on this.  +1 on making 0.20 old API behavior consistent with 0.19 behavior across the board.;;;","25/Aug/09 19:50;jdcryans;Patch that fixes the gets and scans. I will test it with some legacy code.;;;","25/Aug/09 19:53;jdcryans;The code that was failing before now passes its unit tests with this patch.;;;","25/Aug/09 21:35;jdcryans;New patch coming, I forgot that the max timestamp was exclusive.;;;","25/Aug/09 23:07;jdcryans;Patch that respects the exclusion of maxtimestamp.;;;","25/Aug/09 23:45;streamy;Patch looks good.  +1 for commit in branch and trunk after jd testing.;;;","26/Aug/09 00:05;jdcryans;Committed to branch and trunk., tests now work great.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Regression] Cannot save timestamp in the future,HBASE-1792,12433967,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,25/Aug/09 18:32,13/Sep/09 22:24,01/Jul/25 07:49,26/Aug/09 00:05,0.20.0,,,,,0.20.0,0.90.0,,,,,,0,"0.20, compared to previous versions, doesn't let you save with a timestamp in the future and will set it to current time without telling you. This is really bad for users upgrading to 0.20 that were using those timestamps.

Example:

 hbase(main):004:0> put 'testtable', 'r1', 'f1:c1', 'val', 5373965335336911168
 0 row(s) in 0.0070 seconds
 hbase(main):005:0> scan 'testtable'
 ROW                          COLUMN+CELL                                                                      
  r1                          column=f1:c1, timestamp=1251223892010, value=val                                 
 1 row(s) in 0.0380 seconds",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 20:40;jdcryans;HBASE-1792.patch;https://issues.apache.org/jira/secure/attachment/12417652/HBASE-1792.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25983,Reviewed,,,,Wed Aug 26 00:05:57 UTC 2009,,,,,,,,,,"0|i0hf9z:",99737,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 19:02;stack;Yeah, I suppose old days we'd leave the ts alone and only change it to current time if it was HConstants.LATEST_TIMESTAMP (IIRC).;;;","25/Aug/09 19:07;streamy;There was a number of long discussions related to this.  Unfortunately, I forgot where we ended up.

We tried to keep certain axioms, like ""deletes only apply to older storefiles"", but manually setting of stamps (future or past)  breaks that.  That means, if you modify stamps manually, your Gets can have indeterminate behavior.

I'd be more than happy to remove the strictness here, and allow anything < Long.MAX_VALUE (if equal, rewrite as now()), with a warning in Put that manually setting of stamp can cause indeterminate behavior, especially in Gets.

In any case, I'd like to see a unit test that verifies the behavior that we do have.  There is already a bit of it in o.a.h.h.c.TestClient, but there should be more, perhaps another method or part of testVersions().  It should do insertions with past, present, future stamps, in all different orders, with lots of flushing in between (all issues with stamps / indeterminate behavior are related to multiple storefiles + memstore interaction).

If we can create a new test, with comments explaining what the behavior is, and all prior tests still pass, then I'll be +1 on allowing future stamps.

Stack, Erik, any recollection on why it was left this way?

This should also not have an impact on META where determinate behavior of Gets is quite important.;;;","25/Aug/09 20:13;jdcryans;Just so I don't forget it, the timestamp is replaced silently here in HRegion.updateKeys:

{code}
  /**
   * Checks if any stamps are > now.  If so, sets them to now.
   * <p>
   * This acts to be prevent users from inserting future stamps as well as
   * to replace LATEST_TIMESTAMP with now.
   * @param keys
   * @param now
   * @return <code>true</code> when updating the time stamp completed.
   */
  private boolean updateKeys(List<KeyValue> keys, byte [] now) {
    if(keys == null || keys.isEmpty()) {
      return false;
    }
    for(KeyValue key : keys) {
      key.updateLatestStamp(now);
    }
    return true;
  }
{code};;;","25/Aug/09 20:28;ryanobjc;we should move back to Long.MAX_VALUE as the thing to replace to 'now' i think... 

The problem with deletes would be if the delete is set too far in the past or future, depending on the time range of the files that end up bracketing them. Solved with minor compaction, and works via scans.

;;;","25/Aug/09 20:40;jdcryans;Currently testing this patch.;;;","25/Aug/09 23:43;streamy;+1 for commit on branch and trunk after your testing, jd.;;;","26/Aug/09 00:05;jdcryans;Committed to branch and trunk, test passes and it fixes the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeout in IndexRecordWriter,HBASE-1791,12433956,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,bstephens,bstephens,bstephens,25/Aug/09 16:27,20/Nov/15 13:01,01/Jul/25 07:49,25/Aug/09 18:33,0.20.0,0.20.1,0.90.0,,,0.20.1,0.90.0,,,,,,0,"A MapReduce job to generate Lucene Indexes from HBase will fail on sufficiently large tables. After the indexing finished, the close() method of IndexRecordWriter is called.  The  writer.optimize() call in this method can take many minutes, forcing most MapReduce tasks to timeout. There is a HeartBeatsThread, but it does not seem to send progress updates. 

A suggested fix may be to add context.progress(); in the HeardbeatsThread run() method, after the context.setStatus call. Not sure why context.setStatus is not ""good enough"". ","19 HBase nodes, 8 cores, 8 GB RAM, CentOS",10800,10800,,0%,10800,10800,,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 17:03;bstephens;hbase-1791.patch;https://issues.apache.org/jira/secure/attachment/12417630/hbase-1791.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25982,,,,,Fri Nov 20 13:01:44 UTC 2015,,,,,,,,,,"0|i0hf9r:",99736,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 17:02;bstephens;Added context.progress() to HeartbeatsThread, moved context.setStatus to more useful place. ;;;","25/Aug/09 17:10;larsgeorge;+1

I attribute this to the new Hadoop mapreduce package and the new context class layout. There is an old documentation that refers to setting the status to report progress but I am not surprised that the new classes rather use the proper progress() call. The unit test is too small to notice this change, so good that Bradford found it!
;;;","25/Aug/09 18:33;apurtell;Committed to branch and trunk. Thanks for the patch Bradford!

Stack, please set up Bradford as a contributor so we can assign this issue to him. ;;;","25/Aug/09 18:37;jdcryans;Assigned. Every committer can change the roles.;;;","25/Aug/09 18:46;stack;OK that this ain't going to be in 0.20.0?  If its needed, need to sink the current RC.  Otherwise it'll be in 0.20.1.  Good stuff.;;;","25/Aug/09 18:52;apurtell;Sorry, set fix version to 0.20.1.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
filters are not working correctly,HBASE-1790,12433824,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,backslash,backslash,24/Aug/09 07:24,13/Sep/09 22:24,01/Jul/25 07:49,02/Sep/09 23:24,0.20.0,0.90.0,,,,0.20.0,0.90.0,,Filters,,,,0,"Filters used in Scanning the table are not working correctly. For example a table with three rows:
1. rowkey = adminbackslash-nb0, desc:temp = ""temp""
2. rowkey = adminbackslash-nb1, desc:temp = ""temp""
3. rowkey = adminkleptoman, desc:temp = ""temp""

If I scan all rows in the table without filter I get all the rows as expected. But applying a simple prefixfilter with parameter ""adminbackslash"" will return only first row. I searched it down to HRegion::nextInternal method, which will not output one passed row before denied row(by filter). ",,,,,,,,,,,,,,,,,,,,HBASE-1353,HBASE-1710,HBASE-1807,,,,,,,"29/Aug/09 16:57;backslash;1790-3.patch;https://issues.apache.org/jira/secure/attachment/12418067/1790-3.patch","29/Aug/09 14:25;streamy;HBASE-1790-v2.patch;https://issues.apache.org/jira/secure/attachment/12418061/HBASE-1790-v2.patch","02/Sep/09 02:17;streamy;HBASE-1790-v4.patch;https://issues.apache.org/jira/secure/attachment/12418329/HBASE-1790-v4.patch","02/Sep/09 20:53;streamy;HBASE-1790-v5.patch;https://issues.apache.org/jira/secure/attachment/12418431/HBASE-1790-v5.patch","02/Sep/09 22:11;streamy;HBASE-1790-v6.patch;https://issues.apache.org/jira/secure/attachment/12418444/HBASE-1790-v6.patch","02/Sep/09 23:00;streamy;HBASE-1790-v7.patch;https://issues.apache.org/jira/secure/attachment/12418446/HBASE-1790-v7.patch","24/Aug/09 09:15;backslash;hbase-1790.patch;https://issues.apache.org/jira/secure/attachment/12417456/hbase-1790.patch","29/Aug/09 05:26;stack;testfilter.patch;https://issues.apache.org/jira/secure/attachment/12418048/testfilter.patch",,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25981,Reviewed,,,,Wed Sep 02 23:24:23 UTC 2009,,,,,,,,,,"0|i0hf9j:",99735,,,,,,,,,,,,,,,,,,,,,"24/Aug/09 12:07;stack;Move to 0.20.1;;;","28/Aug/09 08:37;apurtell;Why not pull this into 0.20.0 since it seems like filters do not work at all really?;;;","28/Aug/09 09:12;backslash;I would ask you to pay attention to my post from few days:

Is anybody using filters when scanning table? I recently created ""(HBASE-1790) filters are not working correctly"" and submitted a patch, but latter I found that stateful filters are still not working. I have already rewrited the HRegion::nextInternal function and filters(not submitted patch yet). As I see it, there were more problems.
1. filters are applyed before deleted rows were discarted(so they were count in when scaning, this is problem for PageFilter)
2. filters like PageFilter reset their state in reset function, but this function is called in HRegion::nextInternal, so it would be good to mention in javadoc for Filter::reset, where in the process of filtering is this method called.
3. last problem is a question, why there is Filter::filterRow called in HRegion::nextInternal, when it should be called after Filter::filterKeyValue(KeyValue) which is not called in HRegion::nextInternal.
So my last question is, is anybody using filters, because their are seriously not working correctly at all. And if there is someone, which filter do you use?


Basically, the patch is not working for statefull filters, but I could make one, which apparently work, but I would be very grateful if somebody tests it. ;;;","28/Aug/09 15:42;stack;Bringing into 0.20.0.;;;","28/Aug/09 15:49;stack;Matus: Any chance of unit tests?  Please add a patch for stateful filters.  As to whether folks are using filters, the answer must be no... or at least not in any seriious way.;;;","28/Aug/09 16:27;davelatham;We are using filters extensively in our deployment on 0.19 HEAD.  ;;;","28/Aug/09 19:41;clint.morgan;We use filters (mostly FilterList and ValueFilter) on 0.20 to answer AND/OR type criteria. Not much use of staful filters except as needed in ValueFilter.

For prefixing/start/stop rows we adjust the Scan.

For paging, we use a wrapper on the Scan which skips over the first results then stops scanning after pageNum results. I realize thats not ideal as we unnecessarily pay transport on the first skipped rows, then pay the cost to get the N+1 row. I'd love to see PageFilter working!;;;","29/Aug/09 00:10;backslash;Clint, thank you for comment. I am glad, that filters are something, which is used atleast somewhere. Stack, I will look into unit tests tomorow and I will also submit the patch. Dave, which filters are you using? Is it possible, that filters are working correctly in 0.19?;;;","29/Aug/09 05:26;stack;Matus: Here is a TestFilter template that does the testing at the HRegion interface.  It might help you get going (I was afraid you'd try to figure how best to test this and wander into the weeds and we'd never hear from you again).

Regards filters, the API and implementation changed in 0.20.0 so yeah, they work for Dave and partially for others (such as Clint) but seems to be issues here.

Thanks for helping out.;;;","29/Aug/09 13:31;davelatham;Matus, we use several custom filter implementations that commonly deserialize some of the data in the region server and filters out rows based on data in the row, column, or cell.  They work great for us, and are a definite performance gain.  It will definitely be important for them to be working in 0.20 before we migrate.;;;","29/Aug/09 14:25;streamy;This adds a prefix test to stack's framework and then makes a change inside of HRegion.nextInternal() to return the current result if there is one (once we hit a filtered row key).

What was happening was we put the second row in the result (List<KV>), iterated to the third, it said filter it, so we iterated the scanner, cleared the result, and returned.  So even though the second KV should have been returned, the third KV forced a full clear of the result.

I added a check that if we just hit a new row (currentRow != row) and there is something still in the result (!result.isEmpty()), then return w/o clearing.

I also modified prefix filter to be more stateful so that it knows once it has passed it's prefix.  Once passed the prefix, it returns filterAllRemaining() as true, so this further cleans up the test case.  Without that, when running this at the Region level, we end up returning an empty result list on the third call rather than the second call returning false (that we are done).  Since we actually can determine we are done in this case, because the prefix has been passed, adding it cleans that up so we return the correct result in the second call, and that call also returns false letting up know that there will be no more matches.

I had to update one test (I think I actually wrote it) that tested FilterList MUST_PASS_ONE because it was running against a key passed the prefix, and then reversing back to the prefix.  That's impossible behavior so I slightly modified the test.

Not sure whether this is totally right but in debugging it made sense.  All tests I've run pass.;;;","29/Aug/09 16:57;backslash;Jonathan I see what you tried to do, but if we need to make the Prefix filter stateful(or any other filter) than the HRegion.nextInternal method is basicaly depending on filter to be able to tell, if there will be no more accepted rows, before filtering them. This is possible for alfanumeric filters like prefix filter, but I think, it will break down with ValueFilter and a situation on last passed row, because ValueFilter cant tell if there would be any acceptable rows, before aplying filter. So I rewrite the nextInternal and next method. Also I rewrite the test file TestFilter. There are 4 test methods now: without filter, prefixfilter(dont need to apply jonathans patch), pagefilter and valuefilter. And I also add new filter StartPageFilter, which is the same as PageFilter, but you can specify how many rows should be skipped before the paging.

I am developing on windows, so I dont have the best conditions for running tests. Although this nextInternal implementation passed TestFilter tests, I am not sure, if it will passed all the others tests. If anybody can try this it would be great.

As I was searching for all usages of InternalScanner.next function, which basicaly take the result of nextInternal in case of HRegion and thus should return false if this is the last row, I came accross HMerge:

while(rootScanner.next(results)) {
          for(KeyValue kv: results) {
            HRegionInfo info = Writables.getHRegionInfoOrNull(kv.getValue());
            if (info != null) {
              metaRegions.add(info);
            }
          }
        }

It looks like, it expects rootScanner.next(results) to be true for every row and not for every row except the last. So Stack, should we correct this?;;;","31/Aug/09 20:31;stack;The piece you pasted from HMerge looks broke to me Matus.  Please add a fix to your patch please.

In pagefilter, please just remove rather than comment-out:

{code}
-    rowsAccepted = 0;
+    //rowsAccepted = 0;
{code}

You do the same commenting out elsewhere in your patch.  Please just remove the replaced code.

The StartPageFilter needs to have in its class javadoc why its different from PageFilter.  Or, better, can we not just have PageFilter do what StartPageFilter does (pass 0 for pageStart if you want PageFilter behavior and N if you want to do StartPageFilter actions?)

Just by way of FYI, fellas usually make the patch in the $HBASE_HOME dir.  Yours was made  in $HBASE_HOME/src. ... just for the future.

Unfortunately, it seems as though this patch breaks other hbase tests.  I'll try taking a look why in a few hours.





;;;","01/Sep/09 05:55;stack;The tests that are failing are the following:

{code}
    [junit] Test org.apache.hadoop.hbase.TestEmptyMetaInfo FAILED (timeout)
    [junit] Test org.apache.hadoop.hbase.TestHBaseCluster FAILED (timeout)
    [junit] Test org.apache.hadoop.hbase.TestInfoServers FAILED (timeout)
{code}

Its always a timeout.  Must be getting wrong answers back.  Will take a look in morning.;;;","01/Sep/09 16:36;streamy;Resuming work on this with stack;;;","02/Sep/09 02:17;streamy;Adds new tests.  Adds new filters.  Makes a few changes to HRegion code and how filters are run.  Modifies some existing tests to match updated semantics.  Add ""early-out"" to a few existing filters so that we don't require an additional round-trip when we know we are done.

Also includes a modified version of Andrew's patch from HBASE-1807.

More work to be done, not ready for review...  Just checkpointing.;;;","02/Sep/09 18:35;streamy;Patch will deal with these other issues as well.;;;","02/Sep/09 20:53;streamy;Completely changes the KeyFilter stuff from apurtell patch over in HBASE-1807 but retains all the functionality besides family checking (after clarifying with user on list that he was in fact not looking for that).

Adds more tests.  New tests are in TestFilter.  There are lots of them, though could use more FilterList tests.

Adds a couple new classes to filters... BinaryComparator which is a WritableBAComparable implementing wrapper of Bytes.compareTo().  CompareFilter is a superclass for RowFilter, QualifierFilter, and ValueFilter classes.  

Existing ValueFilter has become SingleColumnValueFilter which is what it actually does.  I actually removed the filterRowIfColumnMissing stuff and instead created a wrapping filter SkipFilter.  You wrap a KV checking filter with it, and if any KVs in the row fail, the entire row is filtered out.

Couple changes in HRegion code to actually fix filters.;;;","02/Sep/09 20:55;streamy;Please review.

Excuse the small extra-line formatting error in HRegion.;;;","02/Sep/09 21:22;apurtell;Nice Jon.

Put CompareOp enum in CompareFilter now?

SkipFilter javadoc is confusing. I think you are missing a ""not"" in ""Any row which did not have the given value for the specified column will be emitted. "" Should be ""Any row which did not have the given value for the specified column will NOT be emitted. ""?

There are some classes missing in the HbaseObjectWritable map? BinaryComparator, CompareFilter, QualifierFilter, RowFilter, SkipFilter?

Tests look great. ;;;","02/Sep/09 21:40;stack;Should CompareFilter be abstract (Its useless standalone).   Others?

Ugh. Do we have to make a new HBaseConfiguration serializing?  That looks way broke.  Can we fix?  We should use HBaseObjectWritable anyways? If I look in HOW, its just using it to make a NullWritable.... do we even do this?  I can change HOW so we create a HBC if we need to do a nullwritable?







;;;","02/Sep/09 22:03;streamy;Changed to a different example for SkipFilter, ran myself in circles with the one that was there :)  Thanks for review andrew.

Made CompareFilter abstract.  Moved CompareOp to CompareFilter.  Added new classes to HOW.

Not sure what to do about HBC.  Definitely agree we should not instantiate new ones in serialization.  I'm open to whatever will work :);;;","02/Sep/09 22:11;streamy;As described.  Addresses everything but HBC/HOW issue.;;;","02/Sep/09 23:00;streamy;Addresses all issues.

HOW read/write can now take null HBCs.  Only issue is if you pass it a Configurable you need to pass it the HBC in order to have the configuration set on it automatically.  Does not impact existing code outside scope of this filter patch.;;;","02/Sep/09 23:17;stack;+1 on patch.  Didn't apply properly... last lines of patch but looks good.;;;","02/Sep/09 23:24;streamy;Committed to trunk and branch.  Thanks for review stack and andrew.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Missing rows after medium intensity insert,HBASE-1784,12433723,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,jdcryans,jdcryans,21/Aug/09 19:52,13/Sep/09 22:24,01/Jul/25 07:49,01/Sep/09 19:56,0.20.0,,,,,0.20.0,,,,,,,0,"This bug was uncovered by Mathias in his mail ""Issue on data load with 0.20.0-rc2"". Basically, somehow, after a medium intensity insert a lot of rows goes missing. Easy way to reproduce : PE. Doing a PE scan or randomRead afterwards won't uncover anything since it doesn't bother about null rows. Simply do a count in the shell, easy to test (I changed my scanner caching in the shell to do it faster).

I tested some light insertions with force flush/compact/split in the shell and it doesn't break.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/09 04:54;stack;1784-v2.patch;https://issues.apache.org/jira/secure/attachment/12418218/1784-v2.patch","27/Aug/09 19:28;stack;1784.patch;https://issues.apache.org/jira/secure/attachment/12417918/1784.patch","24/Aug/09 14:52;herberts;DataLoad.java;https://issues.apache.org/jira/secure/attachment/12417480/DataLoad.java","28/Aug/09 16:44;apurtell;HBASE-1784-StoreFileScanner-hack.patch;https://issues.apache.org/jira/secure/attachment/12418005/HBASE-1784-StoreFileScanner-hack.patch","26/Aug/09 07:11;herberts;HBASE-1784.log;https://issues.apache.org/jira/secure/attachment/12417710/HBASE-1784.log","26/Aug/09 23:14;herberts;META.log;https://issues.apache.org/jira/secure/attachment/12417824/META.log","31/Aug/09 21:45;herberts;dbl-assignment-20090831;https://issues.apache.org/jira/secure/attachment/12418181/dbl-assignment-20090831","26/Aug/09 21:42;herberts;double-assignment;https://issues.apache.org/jira/secure/attachment/12417803/double-assignment","01/Sep/09 19:02;herberts;post-1784v2.log;https://issues.apache.org/jira/secure/attachment/12418282/post-1784v2.log","26/Aug/09 22:27;herberts;processSplitRegion-check-regionIsOpening.patch;https://issues.apache.org/jira/secure/attachment/12417822/processSplitRegion-check-regionIsOpening.patch",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25978,Reviewed,,,,Tue Sep 01 19:56:20 UTC 2009,,,,,,,,,,"0|i0hf87:",99729,,,,,,,,,,,,,,,,,,,,,"21/Aug/09 22:22;jdcryans;Testing this issue is not as easy as I thought it was. I was doing randomWrites and not sequentialWrites... that's very different, with the random one you don't write as many rows. So I never really ""lost"" thoses rows, they were never written.

But there seems to be something in Mathias's case, Ryan also got this issue as he told me during the hackathon.;;;","24/Aug/09 14:52;herberts;This is a stripped down version of the MR job I use to import data into HBase.

The input consists of SequenceFiles with byte[] keys and byte[] values, respectively 19 and 320 bytes long.

;;;","24/Aug/09 18:06;streamy;Current thinking is that this is caused by failed compaction(s).  Mathias is turning on DEBUG and running smaller jobs, increasing load each run, until it triggers missing rows.;;;","26/Aug/09 07:11;herberts;I reran my test to try to corner the problem.

My last run *only* lost around 2 million rows out of 866 millions. Interestingly the logs (attached) only show one compaction failure.

A side effect observed was that inserting the rows this the WB disabled was almost twice as fast as with the WB set to 1Mb.;;;","26/Aug/09 08:02;herberts;A scan of the table shows that rows from the region 0TE181121109t\x90_\x7F\xFF\xFF\xFE,1251257023139 whose compaction failed are indeed gone, so this clearly shows that missing rows are indeed linked to the failed compaction.;;;","26/Aug/09 16:11;streamy;@Mathias, one thing JD found during some import testing we did over the hackathon was to use large write buffers.  Experiment with 10-20MB write buffers rather than 1MB.  And disable the WAL.

Are you running with WAL disabled on these Puts already?;;;","26/Aug/09 16:15;streamy;Looking at your code, seems that you are disabling WAL.

Given that in your testing you don't generally see full-on cluster breakage, just occasional compactions failing, that if the failed compactions weren't erradicating data you would probably survive the import to the end.

That's a good argument for fixing this in 0.20.0, but it might not be as easy as we'd hope.

One other note... At least in the Mathias logs I've seen, the compactions that are running, and failing, seem to have at least some half readers / referenced storefiles.;;;","26/Aug/09 19:47;stack;@Mathias If you grep /hdfs/hadoop/hbase/domirama/377086364/batchid/5233728315379016023 in your namenode log or 7352946847213562224 (from blk_7352946847213562224_926536) do you see anything interesting?  The file looks like its been deleted by another process (doubly assigned region?  But this is RC2, right?).;;;","26/Aug/09 21:42;herberts;The masterlog. show a scenario which seems to have led to a double assignment.

At 05:20:16,074 RS0 opens region domirama,0TE181121109t\x90_\x7F\xFF\xFF\xFE,125125681462

At 05:23:43,138 RS0 initiates a split of this region, the split will complete at 05:24:01,315 after 18s.

At 05:23:50,516 MASTER discovers one of the daughter regions and decides its current assignment is not valid

At 05:23:50,905 MASTER assigns this daughter region to RS1

At 05:23:53,957 MASTER receives PROCESS_OPEN / OPEN messages from RS1

At 05:24:02,389 MASTER receives REPORT_SPLIT message from RS0

At 05:24:02,390 MASTER assigns the daughter region to RS0

At 05:24:05,466 MASTER receives PROCESS_OPEN / OPEN messages from RS0

the daughter region is then doubly assigned.

This is happening because the split starts by writing info about the daughter regions in META, then if the split takes quite some time (it took 18s in this case), the master might discover the daughter regions prior to being notified of the split. It then acts as if the regions were not assigned and assigns it, but later when it receives the split report, it assigns again the region (to the RS that did the split).

Shouldn't a splitting RS assume it is serving the daughters and record this in historian or in META?
Shouldn't the master check that the RS that is doing the split is already serving the region?
Shouldn't the master check when it receives the split report that it has not already assigned the daughters?;;;","26/Aug/09 22:27;herberts;The attached patch checks that the daughter regions are not in state 'isOpening' prior to setting them unassigned.;;;","26/Aug/09 23:02;ryanobjc;so a slow split opens us up a race condition with the META scanner, good find.

any thoughts on why the split is so slow? it should be fast?

I call this another vote for cutting the master out of the split loop. ;;;","26/Aug/09 23:14;herberts;The split was slow because META and another region were being compacted, as soon as META was scheduled for compaction, IPC were blocked, and they remained blocked during the compaction of ther other region which took 15s and happened BEFORE compaction of META.

;;;","27/Aug/09 06:41;stack;I think the patch won't fix what I see in the logs.  What I am seeing is that, as you describe Mathias, the new daughter region has been added to the .META. table, noticed by the scanner and then assigned AHEAD of the master getting the split message that includes said daughter.  The daughter has been successfully opened out on the regionserver.  On successful open, the master no longer carries internal state on a region since its no longer 'transitioning'.  The split comes in and the region is reassigned IF !s.isPendingOpen() && !s.isOpen().  The daughter region will be in neither state.  It won't be pending since its already opened.  isOpen is a bit of a misnomer in that its a temporary state a region is in after we get the open message from regionserver but before its open details -- the server it was opened on -- has been added to the .META. table.  After the .META. has been updated, the region is no longer transistioning and is removed from the master inner state tables.  So, the daughter won't be in the isOpen state either.

Lets chat in morning.  Maybe a reordering of the order in which we send events would make things work well enough in the 0.20 timeframe till we get a chance to redo this state-transition stuff properly in 0.21.



;;;","27/Aug/09 16:35;stack;Bringing into 0.20.0;;;","27/Aug/09 19:28;stack;Here is a patch that before it adds new daughter region to unassigned, it checks that region has not already got a server and startcode in the .META. table: i.e that it has not already been assigned.  Its messy and expensive -- e.g. more .META. queries -- but in the scheme of things splits are relatively infrequent.

I do not see an alternative given the current toolset used keeping region state.  Even in new regime, will need to check for this condition but perhaps in the new context we can make the check more lightweight.;;;","27/Aug/09 21:55;stack;hbs is running patch overnight on his cluster.  Will have news for us in morning.;;;","28/Aug/09 15:14;herberts;I ran my import job against the head of the 0.20 branch + patch for 1784 and unfortunately I am still missing some rows.

The logs don't show similar messages as the one that lead to discover the double assignment problem. But they show a few

java.lang.RuntimeException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column

4 of the 5 reducers were restarted due to timeout being reached when attempting to contact region servers, the batch therefore ran for more than 15 hours.

Will rerun it once more on an empty table to have a double test, but for now it seems dataloss still occur.;;;","28/Aug/09 16:25;apurtell;bq. java.lang.RuntimeException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column

That error means that edits for one family are still ending up in store files for another. I remember how to hack the code to let the store scanner continue around this condition. Otherwise it is in effect data loss because the store scanner cannot continue, though data is there. Let me put up a patch for this.

There is a separate hack for healing the problem as it is discovered during compaction by moving the edits from the inappropriate store to the appropriate one. Should we do this too as a workaround until the root cause is found and fixed? ;;;","28/Aug/09 16:29;stack;@Mathias Any regionserver crashes during upload?;;;","31/Aug/09 21:45;herberts;This is with a head of 0.20 with St^Ack's fix and Andrew's hack.

lines of interest have been prefixed with '@@' in file 'dbl-assignment-20090831'

This double assignment is happening after a split, as in the previous case.

The double assignement takes place after

2009-08-31 17:03:08,664

The attachement has logs from Master and 2 RS (the ones involved in the double assignment).;;;","31/Aug/09 21:58;stack;This is like the last hole we plugged only it's in basescanner this time.   It's an interesting one.  Patch co
ing;;;","31/Aug/09 23:29;stack;From the log posted by Mathias, this is the pertinent bit:
{code}
2009-08-31 17:03:11,689 INFO org.apache.hadoop.hbase.master.RegionServerOperation: domirama,00AZRPXURYW7\x13\x7B_\x7F\xFF\xFF\xFE,1251730985136 open on 10.154.99.183:60020
2009-08-31 17:03:11,690 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row domirama,00AZRPXURYW7\x13\x7B_\x7F\xFF\xFF\xFE,1251730985136 in region .META.,,1 with startcode=1251704407620, server=10.154.99.183:60020
2009-08-31 17:03:11,749 INFO org.apache.hadoop.hbase.master.ServerManager: 5 region servers, 0 dead, average load 50.0
@@2009-08-31 17:03:11,851 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of domirama,00AZRPXUPUF7\xFC\xD4\xDF\x7F\xFF\xFF\xFC,1251730985136 is not valid;  serverAddress=, startCode=0 unknown.
{code}

Here, the split has been opened successfully out on the regionserver, so its state in the master has been cleared... but BaseScanner which has been running during the update sees old state of this row, not the state had been updated 150ms earlier.

Let me go through all places where we set a region unassigned and add in a get so we get current state rather than a stale state of a row.  I did this for the special case of splits in the previous patch applied against this issue but looks like it needs to be done more generally.
;;;","01/Sep/09 00:52;stack;@Mathias Have you upped hbase.client.scanner.caching in hbase-site.xml?;;;","01/Sep/09 04:54;stack;So, this patch adds distrust of the view returned by BaseScanner (Scanners do not respect row locks in 0.20.0).   Inside in checkAssign, if server address is null, we'll do a new Get to ensure its still null just before we decide to set a region as unassigned.  It also adds an explicit set of the BaseScanner caching to 1 in case caching is changed in hbase-site.xml to avoid client-side configurations effecting the BaseScanner running in the Master.

Patch includes the earlier patch for handling the split message not assigning a region already assigned.

It does not include Andrews' change (Looks like that can be applied independently).

Its tough testing for this scenario.  Patch logs if it comes across the issue where Scan sees null but the Get actually gets value.

Any chance of a review and if it looks good to you, trying it on  your upload Mathias?

Thanks boss.;;;","01/Sep/09 05:06;stack;Tests pass.;;;","01/Sep/09 07:17;herberts;hbase.client.scanner.caching was set to the default of 1 during all my tests.

Launching a new run right now with you latest patch + Andrew's change.;;;","01/Sep/09 14:21;apurtell;However it is done, I don't think having ScanWildcardColumnTracker.checkColumn throw an exception which kills the store scanner is the right thing to do.;;;","01/Sep/09 16:29;stack;In this latest run, Mathias is seeing:

{code}
13:06 <hbs_> hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:34:30,214 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
13:06 <hbs_> hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:34:31,473 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
13:06 <hbs_> hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:36:29,485 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
13:06 <hbs_> hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:36:29,494 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
{code}

These are cases where the content of the row has changed between the scan of the row and inside in the method where we check for assignment.  Each instance above would have made for a double-assignment, if my understanding is correct.;;;","01/Sep/09 18:46;apurtell;Reopened HBASE-1715 to address the orthogonal ScanWildcardColumnTracker issue.;;;","01/Sep/09 19:02;herberts;My latest run encountered only the errors included in the log I attached to the issue.

23 occurrences of the log message ""GET got ...."" appear in the file, meaning that the latest patch St^Ack put together is working and avoided 23 possible double assignements.

The row count I did after my import found N-1 records, I need to check the counting job as the offset of 1 is probably due to it.

The multiple cases of double assignments seem at least gone, thanks St^Ack ! ;;;","01/Sep/09 19:05;streamy;1 row a bit better than 2M :);;;","01/Sep/09 19:26;herberts;The row counter (an MR job using TableInputFormat) got N-1 Map Input Records (as reported by Hadoop), so the missing record is either not in the table or lost by TableInputFormat.;;;","01/Sep/09 19:32;streamy;Where did you determine N from?  Possible two things ended up in the same row, or you have an off-by-one error on the original N?;;;","01/Sep/09 19:38;herberts;The import job has the following counters:

Map Input Records: N
Reduce Input Records: N

I know all N records have different keys (by construction).

The counting job (using TableInputFormat) reports:

Map Input Records: N-1
;;;","01/Sep/09 19:46;herberts;+1 on 1784-v2, fixes the double assignment issues.;;;","01/Sep/09 19:56;stack;Committed branch and trunk.  Thanks for testing and review Mathias.;;;",,,,,,,,,,,,,,,,
"Weird behavior of WildcardColumnTracker.checkColumn(), looks like recursive loop",HBASE-1781,12433585,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,larsgeorge,larsgeorge,20/Aug/09 16:41,12/Oct/12 06:13,01/Jul/25 07:49,04/Nov/09 05:25,0.20.0,,,,,0.20.2,0.90.0,,,,,,0,"I got a weird error twice on a MR job which eventually though completed

{code}
...
09/08/19 11:28:31 INFO mapreduce.TableInputFormatBase: split: 2591->foo.bar.net:1fff99f02088fe,1fffdcbdb0476b
09/08/19 11:28:31 INFO mapreduce.TableInputFormatBase: split: 2592->foo.bar.net:1fffdcbdb0476b,
09/08/19 11:28:31 INFO mapred.JobClient: Running job: job_200908120615_0015
09/08/19 11:28:32 INFO mapred.JobClient:  map 0% reduce 0%
09/08/19 11:35:43 INFO mapred.JobClient:  map 1% reduce 0%
09/08/19 11:39:53 INFO mapred.JobClient:  map 2% reduce 0%
09/08/19 11:42:58 INFO mapred.JobClient:  map 3% reduce 0%
09/08/19 11:47:02 INFO mapred.JobClient:  map 4% reduce 0%
09/08/19 11:50:41 INFO mapred.JobClient:  map 5% reduce 0%
09/08/19 11:54:25 INFO mapred.JobClient:  map 6% reduce 0%
09/08/19 11:58:31 INFO mapred.JobClient:  map 7% reduce 0%
09/08/19 12:02:36 INFO mapred.JobClient:  map 8% reduce 0%
09/08/19 12:06:12 INFO mapred.JobClient:  map 9% reduce 0%
09/08/19 12:10:01 INFO mapred.JobClient:  map 10% reduce 0%
09/08/19 12:13:40 INFO mapred.JobClient:  map 11% reduce 0%
09/08/19 12:17:04 INFO mapred.JobClient:  map 12% reduce 0%
09/08/19 12:21:07 INFO mapred.JobClient:  map 13% reduce 0%
09/08/19 12:24:46 INFO mapred.JobClient:  map 14% reduce 0%
09/08/19 12:29:27 INFO mapred.JobClient:  map 15% reduce 0%
09/08/19 12:33:42 INFO mapred.JobClient:  map 16% reduce 0%
09/08/19 12:38:04 INFO mapred.JobClient:  map 17% reduce 0%
09/08/19 12:44:16 INFO mapred.JobClient:  map 18% reduce 0%
09/08/19 12:50:20 INFO mapred.JobClient:  map 19% reduce 0%
09/08/19 12:55:44 INFO mapred.JobClient:  map 20% reduce 0%
09/08/19 13:01:11 INFO mapred.JobClient:  map 21% reduce 0%
09/08/19 13:06:21 INFO mapred.JobClient:  map 22% reduce 0%
09/08/19 13:11:24 INFO mapred.JobClient:  map 23% reduce 0%
09/08/19 13:15:39 INFO mapred.JobClient:  map 24% reduce 0%
09/08/19 13:20:33 INFO mapred.JobClient:  map 25% reduce 0%
09/08/19 13:25:58 INFO mapred.JobClient:  map 26% reduce 0%
09/08/19 13:29:52 INFO mapred.JobClient:  map 27% reduce 0%
09/08/19 13:34:44 INFO mapred.JobClient:  map 28% reduce 0%
09/08/19 13:38:19 INFO mapred.JobClient:  map 29% reduce 0%
09/08/19 13:41:53 INFO mapred.JobClient:  map 30% reduce 0%
09/08/19 13:45:09 INFO mapred.JobClient:  map 31% reduce 0%
09/08/19 13:49:06 INFO mapred.JobClient:  map 32% reduce 0%
09/08/19 13:52:47 INFO mapred.JobClient:  map 33% reduce 0%
09/08/19 13:56:37 INFO mapred.JobClient:  map 34% reduce 0%
09/08/19 13:59:48 INFO mapred.JobClient:  map 35% reduce 0%
09/08/19 14:04:14 INFO mapred.JobClient:  map 36% reduce 0%
09/08/19 14:09:32 INFO mapred.JobClient:  map 37% reduce 0%
09/08/19 14:14:00 INFO mapred.JobClient:  map 38% reduce 0%
09/08/19 14:17:42 INFO mapred.JobClient:  map 39% reduce 0%
09/08/19 14:21:50 INFO mapred.JobClient:  map 40% reduce 0%
09/08/19 14:25:17 INFO mapred.JobClient:  map 41% reduce 0%
09/08/19 14:28:57 INFO mapred.JobClient:  map 42% reduce 0%
09/08/19 14:33:03 INFO mapred.JobClient:  map 43% reduce 0%
09/08/19 14:36:51 INFO mapred.JobClient:  map 44% reduce 0%
09/08/19 14:40:49 INFO mapred.JobClient:  map 45% reduce 0%
09/08/19 14:44:44 INFO mapred.JobClient:  map 46% reduce 0%
09/08/19 14:48:37 INFO mapred.JobClient:  map 47% reduce 0%
09/08/19 14:52:15 INFO mapred.JobClient:  map 48% reduce 0%
09/08/19 14:55:57 INFO mapred.JobClient:  map 49% reduce 0%
09/08/19 14:59:21 INFO mapred.JobClient:  map 50% reduce 0%
09/08/19 15:02:58 INFO mapred.JobClient:  map 51% reduce 0%
09/08/19 15:07:23 INFO mapred.JobClient:  map 52% reduce 0%
09/08/19 15:10:19 INFO mapred.JobClient:  map 53% reduce 0%
09/08/19 15:13:19 INFO mapred.JobClient:  map 54% reduce 0%
09/08/19 15:16:38 INFO mapred.JobClient:  map 55% reduce 0%
09/08/19 15:19:36 INFO mapred.JobClient:  map 56% reduce 0%
09/08/19 15:22:41 INFO mapred.JobClient:  map 57% reduce 0%
09/08/19 15:25:35 INFO mapred.JobClient:  map 58% reduce 0%
09/08/19 15:30:07 INFO mapred.JobClient:  map 59% reduce 0%
09/08/19 15:37:41 INFO mapred.JobClient:  map 60% reduce 0%
09/08/19 15:42:04 WARN zookeeper.ClientCnxn: Exception closing session 0x422d8cc8cbb310e to sun.nio.ch.SelectionKeyImpl@4737371
java.io.IOException: TIMED OUT
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:858)
09/08/19 15:42:05 INFO zookeeper.ClientCnxn: Attempting connection to server tr-bt-dal-03/10.12.205.194:2181
09/08/19 15:42:05 INFO zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/10.16.182.238:41125 remote=tr-bt-dal-03/10.12.205.194:2181]
09/08/19 15:42:05 INFO zookeeper.ClientCnxn: Server connection successful
09/08/19 15:43:08 INFO mapred.JobClient:  map 61% reduce 0%
09/08/19 15:49:24 INFO mapred.JobClient:  map 62% reduce 0%
09/08/19 15:54:04 INFO mapred.JobClient:  map 63% reduce 0%
09/08/19 15:57:01 INFO mapred.JobClient:  map 64% reduce 0%
09/08/19 16:00:42 INFO mapred.JobClient:  map 65% reduce 0%
09/08/19 16:03:20 INFO mapred.JobClient:  map 66% reduce 0%
09/08/19 16:06:53 INFO mapred.JobClient:  map 67% reduce 0%
09/08/19 16:09:20 INFO mapred.JobClient:  map 68% reduce 0%
09/08/19 16:12:02 INFO mapred.JobClient:  map 69% reduce 0%
09/08/19 16:14:35 INFO mapred.JobClient:  map 70% reduce 0%
09/08/19 16:17:26 INFO mapred.JobClient:  map 71% reduce 0%
09/08/19 16:20:19 INFO mapred.JobClient:  map 72% reduce 0%
09/08/19 16:23:14 INFO mapred.JobClient:  map 73% reduce 0%
09/08/19 16:25:56 INFO mapred.JobClient:  map 74% reduce 0%
09/08/19 16:30:16 INFO mapred.JobClient:  map 75% reduce 0%
09/08/19 16:34:14 INFO mapred.JobClient:  map 76% reduce 0%
09/08/19 16:37:45 INFO mapred.JobClient:  map 77% reduce 0%
09/08/19 16:41:38 INFO mapred.JobClient:  map 78% reduce 0%
09/08/19 16:44:33 INFO mapred.JobClient:  map 79% reduce 0%
09/08/19 16:47:54 INFO mapred.JobClient:  map 80% reduce 0%
09/08/19 16:52:20 INFO mapred.JobClient:  map 81% reduce 0%
09/08/19 16:56:44 INFO mapred.JobClient:  map 82% reduce 0%
09/08/19 17:00:50 INFO mapred.JobClient:  map 83% reduce 0%
09/08/19 17:02:57 INFO mapred.JobClient: Task Id : attempt_200908120615_0015_m_002231_0, Status : FAILED
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row '7c78ca95-4d82-420f-ab2f-495fe139bad0', but failed after 10 attempts.
Exceptions:
java.io.IOException: java.io.IOException: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:847)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:837)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1770)
        at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:136)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracke
09/08/19 17:04:01 INFO mapred.JobClient:  map 84% reduce 0%
09/08/19 17:07:16 INFO mapred.JobClient:  map 85% reduce 0%
09/08/19 17:10:38 INFO mapred.JobClient:  map 86% reduce 0%
09/08/19 17:13:44 INFO mapred.JobClient:  map 87% reduce 0%
09/08/19 17:16:12 INFO mapred.JobClient:  map 88% reduce 0%
09/08/19 17:19:29 INFO mapred.JobClient:  map 89% reduce 0%
09/08/19 17:23:00 INFO mapred.JobClient:  map 90% reduce 0%
09/08/19 17:29:25 INFO mapred.JobClient: Task Id : attempt_200908120615_0015_m_002231_1, Status : FAILED
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row '7c78ca95-4d82-420f-ab2f-495fe139bad0', but failed after 10 attempts.
Exceptions:
java.io.IOException: java.io.IOException: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:847)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:837)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1770)
        at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:136)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracke
09/08/19 17:31:22 INFO mapred.JobClient:  map 91% reduce 0%
09/08/19 17:43:27 INFO mapred.JobClient:  map 92% reduce 0%
09/08/19 17:57:27 INFO mapred.JobClient:  map 93% reduce 0%
09/08/19 18:09:43 INFO mapred.JobClient:  map 94% reduce 0%
09/08/19 18:19:16 INFO mapred.JobClient:  map 95% reduce 0%
09/08/19 18:21:49 INFO mapred.JobClient: Task Id : attempt_200908120615_0015_m_002231_2, Status : FAILED
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row '7c78ca95-4d82-420f-ab2f-495fe139bad0', but failed after 10 attempts.
Exceptions:
java.io.IOException: java.io.IOException: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:847)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:837)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1770)
        at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:136)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)
        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracke
09/08/19 18:31:22 INFO mapred.JobClient:  map 96% reduce 0%
09/08/19 18:42:54 INFO mapred.JobClient:  map 97% reduce 0%
09/08/19 18:43:02 INFO mapred.JobClient: Job complete: job_200908120615_0015
09/08/19 18:43:02 INFO mapred.JobClient: Counters: 11
09/08/19 18:43:02 INFO mapred.JobClient:   Job Counters
09/08/19 18:43:02 INFO mapred.JobClient:     Rack-local map tasks=30
09/08/19 18:43:02 INFO mapred.JobClient:     Launched map tasks=2530
09/08/19 18:43:02 INFO mapred.JobClient:     Data-local map tasks=2500
09/08/19 18:43:02 INFO mapred.JobClient:     Failed map tasks=1
09/08/19 18:43:02 INFO mapred.JobClient:   net.foo.hadoop.hbase.mapred.SetCurrentData$SetCurrentMapper$Counters
09/08/19 18:43:02 INFO mapred.JobClient:     ROWS=22692113
09/08/19 18:43:02 INFO mapred.JobClient:     NETWORK=101481740
09/08/19 18:43:02 INFO mapred.JobClient:     ASSOC=231471790
09/08/19 18:43:02 INFO mapred.JobClient:     CURR_NETWORK=23944682
09/08/19 18:43:02 INFO mapred.JobClient:     CURR_PROVIDER=1801150
09/08/19 18:43:02 INFO mapred.JobClient:   Map-Reduce Framework
09/08/19 18:43:02 INFO mapred.JobClient:     Map input records=22692113
09/08/19 18:43:02 INFO mapred.JobClient:     Spilled Records=0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1951,,,,"04/Nov/09 03:12;jdcryans;HBASE-1781-test.patch;https://issues.apache.org/jira/secure/attachment/12423998/HBASE-1781-test.patch","04/Nov/09 04:10;jdcryans;HBASE-1781.patch;https://issues.apache.org/jira/secure/attachment/12424002/HBASE-1781.patch","24/Aug/09 18:38;larsgeorge;TestGetvsScan.java;https://issues.apache.org/jira/secure/attachment/12417501/TestGetvsScan.java","24/Aug/09 18:39;larsgeorge;output.txt;https://issues.apache.org/jira/secure/attachment/12417503/output.txt","30/Oct/09 17:44;stack;wct.patch;https://issues.apache.org/jira/secure/attachment/12423698/wct.patch",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25975,Reviewed,,,,Wed Nov 04 14:44:52 UTC 2009,,,,,,,,,,"0|i08s47:",49155,,,,,,,,,,,,,,,,,,,,,"20/Aug/09 16:44;larsgeorge;The row in question has versions set to 1 but 18885 columns in one column family. The table also only has that one family.;;;","20/Aug/09 16:49;larsgeorge;Oh, and one more important fact, this is not the table that is scanned but a secondary table. Based on what I get from the scanned table I look up a referenced value from a different table with a straight forward get call, i.e. 

{code}
...
    tblSegmentData = new HTable(new HBaseConfiguration(conf), TBL_SEGDATA);

...
    Get get = new Get(Bytes.toBytes(cid));
    get.addColumn(CF_DATA);
    Result result = tblSegmentData.get(get);
...
{code};;;","20/Aug/09 19:20;streamy;Those three lines of Get are definitely the lines that are executed for this case?  What is the value of CF_DATA?

Get.addColumn(byte[]) is an old-api compliant way of doing things.  There could be a bug somewhere in the mapping from that style to the new style. (family:qualifier -> family, qualfier).

It might also be completely unrelated.

Since you know the row in question, have you tested a script that runs the same Get on the same row but not within this long running MR?  Does it have the same looping issue?  If not, is it slower than you expect?;;;","20/Aug/09 20:01;larsgeorge;Yes, that was an oversight, should have been addFamily(), will change. But the legacy code does parse this proper and assigns the family internally.;;;","24/Aug/09 18:38;larsgeorge;Jon suggested trying to do a test on the row and compare scan vs. get. Attached is a TestGetvsScan application that can be used to iterate over getting a row using a a Get and a Scan n times. I thought I attach it here in case someone else wants to try.;;;","24/Aug/09 18:39;larsgeorge;Attached output.txt as a sample how to use and what is output. Here Scan vs Get is a tie.;;;","29/Oct/09 05:34;stack;Moving to 0.20.2... a few fellas are seeing this now.  Can we artificially manufacture this?;;;","29/Oct/09 19:19;stack;Is the issue that we're doing exact match for index when it should be >= in places like the below:

{code}
      // Specified column is bigger than current column
      // Move down current column and check again
      if(ret <= -1) {
        if(++index == columns.size()) {
          // No more, add to new and include (new was empty prior to this)
          newColumns.add(new ColumnCount(bytes, offset, length, 1));
          this.newColumn = newColumns.get(newIndex);
          this.column = null;
          return MatchCode.INCLUDE;
        }
        this.column = columns.get(index);
        return checkColumn(bytes, offset, length);
      }
{code}

I say this because above is not only place that index is incremented.  Its also incremeted at #191 in same method but then its also incremented in the update method.  The increment is a pre-increment so I can postulate that we'd leave the update with the index set to columns.size and then in the test up in checkColumn, the pre-increment would send us over columns.size().  Now we're in infinite recursion?;;;","30/Oct/09 17:44;stack;Patch that tests for >= rather than == for size.  See if this gets rid of recursion (I do not have a test that manufactures the recursion).;;;","04/Nov/09 03:12;jdcryans;Patch that adds a test to trigger the stack overflow. Stack's patch doesn't fix it (sorry).;;;","04/Nov/09 04:10;jdcryans;Patch that removes the 4 recursive calls with a combination of continue and do-while. It passes my new test as well as the others in that file. Running the other tests now. 

I'm not really fond of this fix but the code there is already very hard to follow. At first I tried to refactor but it didn't seem really doable...;;;","04/Nov/09 04:12;stack;+1 

Thanks for figuring it.;;;","04/Nov/09 05:09;jdcryans;Passes all tests exception for TestTHLog which is unstable currently on Hudson.;;;","04/Nov/09 05:25;jdcryans;Committed to branch and trunk.;;;","04/Nov/09 14:44;larsgeorge;Thanks JD!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThriftServer logged error if getVer() result is empty,HBASE-1779,12433435,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,skamio,skamio,19/Aug/09 09:36,20/Nov/15 13:01,01/Jul/25 07:49,04/Sep/09 05:02,0.20.0,,,,,0.20.1,,,Thrift,,,,0,"Null pointer exception is logged by thrift server process if a client calls getVer() through thrift server and its result is empty.
The easiest fix is to check if result is empty or not.

09/08/18 15:58:30 ERROR server.TThreadPoolServer: Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.getVer(ThriftServer.java:281)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.getVer(ThriftServer.java:269)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getVer.process(Hbase.java:2096)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor.process(Hbase.java:1859)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/09 09:43;skamio;hbase-0.20.0rc1-ThriftServer.patch;https://issues.apache.org/jira/secure/attachment/12417001/hbase-0.20.0rc1-ThriftServer.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25973,,,,,Fri Nov 20 13:01:53 UTC 2015,,,,,,,,,,"0|i0hf7b:",99725,,,,,,,,,,,,,,,,,,,,,"19/Aug/09 09:43;skamio;patch to fix the problem;;;","04/Sep/09 05:02;stack;Committed to branch and trunk;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
column length is not checked before saved to memstore,HBASE-1777,12433368,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,amansk,viper799,viper799,18/Aug/09 19:10,12/Oct/12 06:13,01/Jul/25 07:49,21/Oct/09 23:38,0.20.0,,,,,0.20.2,0.90.0,,Client,,,,0,"I added some debuging to line 511 in HFile.java and found that the column is causing my problem it was > max size
we should check this before saving the record to memstore

As of 0.20.0-RC2 the server dies and cause the hlogs to be read again by the next region server that gets the region in the end it cause the whole cluster to go down sense the bad data is in the hlog at this point.

{code}
2009-08-18 12:54:16,572 FATAL 
org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog 
required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: 
webdata,http:\x2F\x2Fanaal-genomen.isporno.nl\x2F,1250569930062
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:950)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:843)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:241)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
Caused by: java.io.IOException: Key length 183108 > 65536
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkKey(HFile.java:511)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:479)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:447)
        at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:525)
        at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:489)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:935)
        ... 3 more
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/09 14:30;amansk;HBASE-1777.patch;https://issues.apache.org/jira/secure/attachment/12421874/HBASE-1777.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25972,,,,,Wed Oct 21 23:38:02 UTC 2009,,,,,,,,,,"0|i08s7j:",49170,,,,,,,,,,,,,,,,,,,,,"18/Aug/09 19:19;stack;Good one Billy.  Lets fix in 0.20.1.;;;","22/Sep/09 21:12;amansk;Moving it to 0.21 since I didnt get time to look into it yet.;;;","22/Sep/09 21:13;stack;Moving out of 0.20.1 after chatting with Aman.  Its rare that we run into this issue -- though its bad when it happens.

A while back, looking at this issue with Aman, it looks like there are some interesting mismatches in how length checking is done as you go down from client through server and out to hfile so its needs a bit of careful study.;;;","08/Oct/09 21:07;ryanobjc;this one is not as straightforward as it seems, the mainline code in Put and KeyValue checks the row length, thus this is ""impossible"". Must be one of the other code paths somewhere...;;;","08/Oct/09 21:23;ryanobjc;ah looks like its a minor bug in HFile, the code is:

      if (length > MAXIMUM_KEY_LENGTH) {
        throw new IOException(""Key length "" + length + "" > "" +
          MAXIMUM_KEY_LENGTH);
      }


MAXIMUM_KEY_LENGTH is 64*1024, it should be really (2^31-1), since a HFile key can be up to that sized.  The actual row portion can only be 32/64k.;;;","12/Oct/09 14:25;amansk;Attaching a patch. Havent got a chance to write a test case for this yet.;;;","21/Oct/09 23:22;streamy;Moving back into 0.20.2... This got moved out and never committed;;;","21/Oct/09 23:38;streamy;Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ICV has a subtle race condition only visible under high load,HBASE-1740,12432106,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,04/Aug/09 00:00,20/Nov/15 13:02,01/Jul/25 07:49,11/Sep/09 21:46,0.20.0,,,,,0.20.1,,,,,,,0,"ICV demonstrates a race condition under high load.  The result is a duplicate KeyValue with the same timestamp, at first in the memcache, and in hfile, then both in hfile.  The get/scan code doesnt know which one to read, and picks one arbitrarily.  One of the keyvalues is correct, one is incorrect.

What happens at a deeper level:
- we start an ICV
- a snapshot happens and moves the memstore to the snapshot
- the ICV code puts a key-value into memstore that has the same timestamp as a keyvalue in the snapshot.

This is a deep race condition and several attempts to fix it failed in production here at SU.  This issue is about a more permanent fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/09 23:37;ryanobjc;HBASE-1740-test.patch;https://issues.apache.org/jira/secure/attachment/12419240/HBASE-1740-test.patch","04/Aug/09 00:01;ryanobjc;HBASE-1740.patch;https://issues.apache.org/jira/secure/attachment/12415427/HBASE-1740.patch","10/Sep/09 23:08;ryanobjc;HBASE-icv.patch;https://issues.apache.org/jira/secure/attachment/12419234/HBASE-icv.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25951,Reviewed,,,,Fri Nov 20 13:02:08 UTC 2015,,,,,,,,,,"0|i0hf07:",99693,,,,,,,,,,,,,,,,,,,,,"04/Aug/09 00:01;ryanobjc;here is a prelim potential fix, but no test updates and probably doesnt compile in the wider codebase (the snippet doesnt have errors).;;;","13/Aug/09 03:51;stack;Move to 0.20.1;;;","10/Sep/09 23:08;ryanobjc;here is a patch that works for us. Highly recommended, but also very intrusive.  It does do ICV ""the right way"":
- log to HLog
- then make in-ram changes
- dont end up with duplicate timestamps in memstore and hfiles
- dont create too many versions

enjoy;;;","10/Sep/09 23:37;ryanobjc;i forgot the tests! oops!;;;","11/Sep/09 21:27;stack;Reviewed and ran unit tests.  All pass but the broken ITHBase test.  Committed branch and trunk.;;;","11/Sep/09 21:46;stack;Committed branch and trunk.  Ran tests.  Tests passed but for the ones in contrib currently failing up on hudson.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Scanner doesnt reset when a snapshot is created, could miss new updates into the 'kvset' (active part)",HBASE-1738,12432103,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,ryanobjc,ryanobjc,03/Aug/09 23:15,13/Sep/09 22:24,01/Jul/25 07:49,17/Aug/09 22:22,0.20.0,,,,,0.20.0,,,,,,,0,"when a Scanner is created, it creates 2 MemStoreScanners on the kvset and the snapshot (internal names of Memstore)... if the snapshot is originally empty, it only creates the 1, for kvset.  When the snapshot is created, the outstanding Scanners now have a pointer to the tree that is now the snapshot, but no pointer to the kvset.

When the flush completes, the scanner will reset the memstore scanners and 'see' the new values again.

If there is a large delay between snapshot and finalization of the flush, there can be a large period of time a scanner doesnt see 'new' values that are being inserted. the canonical 'bad' case where this can do things is the META scanner, and we end up with double assignment.

The snapshot is really lightweight, it only takes out a small lock in memstore, so im not sure there is an easy mechanism to hook to without building out a bit more code or restructuring the memstore scanner.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/09 20:39;stack;1738-v2.patch;https://issues.apache.org/jira/secure/attachment/12416606/1738-v2.patch","17/Aug/09 21:26;streamy;1738-v3-withheapsizefix.patch;https://issues.apache.org/jira/secure/attachment/12416802/1738-v3-withheapsizefix.patch","17/Aug/09 20:15;stack;1738-v3.patch;https://issues.apache.org/jira/secure/attachment/12416795/1738-v3.patch","07/Aug/09 05:54;stack;1738.patch;https://issues.apache.org/jira/secure/attachment/12415819/1738.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25949,Reviewed,,,,Mon Aug 17 22:22:04 UTC 2009,,,,,,,,,,"0|i0hezr:",99691,,,,,,,,,,,,,,,,,,,,,"07/Aug/09 03:37;stack;I notice that the MemStoreScanner keeps in its result data member all KVs ever seen.  There is no clear as it crosses rows.  The internal index just keeps incrementing over the life of the MemStoreScanner.;;;","07/Aug/09 05:54;stack;Not fully there yet.  Test failing.  Will check in morning.

This patch sets up a MemStoreScanner that goes against the kvset and snapshot always.  They can change under it.  It takes out a MemStore read lock while its caching the row so snapshot ain't cleared while reading row.  Fixes fact that we were keeping around every KV ever read by the scanner.;;;","13/Aug/09 03:50;stack;Moving to 0.20.0.  Bad bug and patch is almost there.;;;","15/Aug/09 00:56;stack;I think I need to fix the small hole here... in 0.20.0 after all.. thinking on it.;;;","17/Aug/09 20:15;stack;Here is v3:

{code}
M src/test/org/apache/hadoop/hbase/regionserver/TestMemStore.java
  Reset memstore scanners at end of each full scan.
  Reenable part of test that was broken because we weren't getting
  all values.
M src/java/org/apache/hadoop/hbase/regionserver/MemStore.java
  Have MemStoreScanners become Observers on MemStore.  If MemStore
  snapshots or clears snapshots, have Scanners update themselves.
  Changes MemStore to have a single scanner for both kvset and for
  snapshot rather than one for each.  Now we meld the results from
  both in the one scanner.
  New inner FirstOnRow class that is updated atomically.
{code};;;","17/Aug/09 21:26;streamy;Same v3 patch but fixes heapsize stuff.  Adds update to deep overhead as well.;;;","17/Aug/09 21:47;streamy;+1 for commit after confirmation that tests pass from stack;;;","17/Aug/09 22:22;stack;Thanks for the help Jon.

Committed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regions unbalanced when adding new node,HBASE-1737,12432090,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,03/Aug/09 21:10,13/Sep/09 22:24,01/Jul/25 07:49,25/Aug/09 23:18,0.20.0,,,,,0.20.0,0.90.0,,,,,,0,"When adding a new RegionServer to a cluster, the new RS will receive some regions but not enough to actually be considered balanced.

To recreate, just take an RS offline, allow regions to be reassigned, and then bring it back up.

Master will get itself into a broken, stuck state where it continuously outputs a line like this:

{noformat}
2009-08-03 12:54:57,812 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server dn4,60020,1249329081079 will be unloaded for balance. Server load: 341 avg: 318.0, regions can be moved: 55
{noformat}

This line is output every 3 seconds and never stops until another RS joins/leaves the cluster.

Making this a blocker because when your new RS only gets some regions (in my case, about half as many as it should have), then all new regions will be assigned to that RS.  This basically destroys any possibility for good load distribution with new data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/09 21:41;streamy;HBASE-1737-v1.patch;https://issues.apache.org/jira/secure/attachment/12415413/HBASE-1737-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25948,Reviewed,,,,Tue Aug 25 23:18:28 UTC 2009,,,,,,,,,,"0|i0hezj:",99690,,,,,,,,,,,,,,,,,,,,,"03/Aug/09 21:35;streamy;The errant section of code was introduced with HBASE-1017;;;","03/Aug/09 21:41;streamy;Fixes small bug in RegionManager.

When looking at the most loaded server, we check to see if there is another server that is underloaded.  When we determine we should unassign from it (numRegionsToClose = 0) then we determine how many to unassign.  However, we don't re-set numRegionsToClose to the number determine for reassignment (so it stays at 0, and thus 0 are reassigned).

Also has a few small formatting changes and an extra variable in log line.;;;","03/Aug/09 21:42;streamy;Newly added regionserver does not reach the exact level as others, but with this patch all RS load are within slop.;;;","03/Aug/09 21:45;stack;+1 on patch.;;;","03/Aug/09 21:49;streamy;Committed.  Thanks for review stack.;;;","03/Aug/09 23:22;stack;Backported to 0.20 branch.;;;","25/Aug/09 22:41;streamy;Commit of HBASE-1743 undid this patch, need to reapply.;;;","25/Aug/09 23:18;streamy;Recommitted to trunk and branch.

This patch was not included in RC2.  I'm going to -1 RC2 as I consider this a blocker.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse of KeyValue during log replay could cause the wrong data to be used,HBASE-1718,12431642,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,28/Jul/09 23:46,13/Sep/09 22:24,01/Jul/25 07:49,29/Jul/09 00:59,0.20.0,,,,,0.20.0,,,regionserver,,,,0,"Our meta table got a row key of METAROW in it.  Hard to explain how it happened, but under code inspection stack found that we are reusing the same KV instance for each replayed key.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 23:49;streamy;HBASE-1718-v1.patch;https://issues.apache.org/jira/secure/attachment/12414819/HBASE-1718-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25937,Reviewed,,,,Wed Jul 29 01:32:31 UTC 2009,,,,,,,,,,"0|i0hevj:",99672,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 23:49;streamy;Instantiates a new KeyValue at the end of the while loop, meaning we only re-instantiate once we've passed forward the KV (we can reuse the times we do continue).;;;","28/Jul/09 23:53;jdcryans;+1 seems fine.;;;","29/Jul/09 00:33;stack;+1 on patch.

Its hard to test.

Here is my supposition as to why this might be responsible for hbase-1638.

Server crashes, its logs are split.  The crashed servers regions are opened anew and there are logs for them to replay.

The inner loop playing the reconstruction logs is this:

{code}
     while (logReader.next(key, val)) {
        maxSeqIdInLog = Math.max(maxSeqIdInLog, key.getLogSeqNum());
        .....
        // Check this edit is for me. Also, guard against writing the special
        // METACOLUMN info such as HBASE::CACHEFLUSH entries
        if (/* commented out for now - stack via jgray key.isTransactionEntry() || */
            val.matchingFamily(HLog.METAFAMILY) ||
          !Bytes.equals(key.getRegionName(), regioninfo.getRegionName()) ||
          !val.matchingFamily(family.getName())) {
          continue;
        }
        // Add anything as value as long as we use same instance each time.
        reconstructedCache.add(val);
        ....
      }
{code}

So, a value might clear the family checks and get added to the reconstructionCache.

We call next again.  The 'val' instance is up in reconstructionCache.  The next deserializes a new KV into same instance.   The deserialized value might not make it past family checks but its already in the reconstructionCache.

This would account for our adding a single value. ;;;","29/Jul/09 00:34;stack;Single bad value.;;;","29/Jul/09 00:59;streamy;Committed.  Upon further thought, this _could_ create cross-family scenario we've seen in HBASE-1638 / HBASE-1715 (but apurtell says no replay).  It does, however, seem to be the case that this bug completely broke log replay as you would always end up only having the last seen KV.;;;","29/Jul/09 01:32;stack;So, at a minimum, this patch fixes replaying of logs:

Below I added logging.  Replay had 17k edits.  Only one was added, the last one.

{code}
....
2009-07-29 01:27:15,274 [regionserver/208.76.44.139:60020.worker] INFO org.apache.hadoop.hbase.regionserver.Store: ADDED TO RECON CACHE: \x00\x00\x00\x03\x00\x01\x07\x02\x08\x05/info:data/1248830494586/Put/vlen=1000
2009-07-29 01:27:15,274 [regionserver/208.76.44.139:60020.worker] DEBUG org.apache.hadoop.hbase.regionserver.Store: Applied 17286, skipped 0 because sequence id <= 22830124
2009-07-29 01:27:15,274 [regionserver/208.76.44.139:60020.worker] DEBUG org.apache.hadoop.hbase.regionserver.Store: flushing reconstructionCache: 1
2009-07-29 01:27:15,274 [regionserver/208.76.44.139:60020.worker] INFO org.apache.hadoop.hbase.regionserver.Store: DUMP \x00\x00\x00\x03\x00\x01\x07\x02\x08\x05/info:data/1248830494586/Put/vlen=1000
{code}

I thought this was hbase-1483 but that was in splitLog.  This is something else.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Put on client-side uses passed-in byte[]s rather than always using copies,HBASE-1717,12431622,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,28/Jul/09 19:36,13/Sep/09 22:24,01/Jul/25 07:49,28/Jul/09 20:12,0.20.0,,,,,0.20.0,,,Client,,,,0,"During review of Put with ryan, found that we are using a passed in reference to family in add() rather than a local copy.  If the backing array changed values, this could cause trouble.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 19:41;streamy;HBASE-1717-v1.patch;https://issues.apache.org/jira/secure/attachment/12414786/HBASE-1717-v1.patch","28/Jul/09 19:47;streamy;HBASE-1717-v2.patch;https://issues.apache.org/jira/secure/attachment/12414787/HBASE-1717-v2.patch","28/Jul/09 19:58;streamy;HBASE-1717-v3.patch;https://issues.apache.org/jira/secure/attachment/12414788/HBASE-1717-v3.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25936,Reviewed,,,,Tue Jul 28 20:12:17 UTC 2009,,,,,,,,,,"0|i0hevb:",99671,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 19:41;streamy;Uses local copy when using put(byte[], ...).  Makes a clone of KV when using put(KV).;;;","28/Jul/09 19:43;streamy;Review for commit.  I think KV cloning is the way to go.  If we don't, then we need to be very explicit in the javadocs about when you need to be sure not to touch passed values.  With this patch, we don't have to say anything because it copies everything.;;;","28/Jul/09 19:45;streamy;Should copy row passed in as well.;;;","28/Jul/09 19:47;streamy;Makes a copy of row during instantiation.  This couldn't cause issues we're seeing, but while we're being safe...;;;","28/Jul/09 19:56;stack;+1 on getting fresh instance of family to add to familymap but -1 on cloning KV.  KV is immutable.  If client makes KVs backed by a byte array of their own construction and then reuse,  then they will have issue.  You suggest javadoc'ing... That seems good to me.;;;","28/Jul/09 19:58;streamy;Removes cloning of KV, adds javadoc to explain the passed KV must be immutable.;;;","28/Jul/09 20:12;streamy;v3 Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convenience functions in Scan and the thrift API along with a few other bug fixes,HBASE-1714,12431545,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,27/Jul/09 22:30,20/Sep/12 22:10,01/Jul/25 07:49,27/Jul/09 23:48,0.20.0,,,,,0.20.0,,,Client,Thrift,,,0,a number of handy things i've added to my own repo recently,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/09 22:56;ryanobjc;HBASE-1714.patch;https://issues.apache.org/jira/secure/attachment/12414678/HBASE-1714.patch","27/Jul/09 22:30;ryanobjc;HBASE-ryan.patch;https://issues.apache.org/jira/secure/attachment/12414672/HBASE-ryan.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25933,,,,,Mon Jul 27 23:48:34 UTC 2009,,,,,,,,,,"0|i0heun:",99668,,,,,,,,,,,,,,,,,,,,,"27/Jul/09 22:34;stack;+1 on all but the Scan class changes where you add in String convenience methods.  If we add them here, expectation is that they should be every where and they are not and its too late to add that.;;;","27/Jul/09 22:56;ryanobjc;here is the stuff without the scan api changes;;;","27/Jul/09 23:48;ryanobjc;moved the scan stuff out;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CME out of HStore.getStorefilesIndexSize while deploying region,HBASE-1708,12431471,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,apurtell,apurtell,26/Jul/09 20:15,11/Jun/22 20:58,01/Jul/25 07:49,21/Nov/09 19:18,0.19.1,,,,,,,,,,,,0,"This is from a Trend 0.19.3 cluster:

2009-07-23 10:44:49,789 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: content,092501240723072009-0000000000000000308-10.3.134.228-a3e3be27683ba154539d1c055890e781,1248345884988: safeMode=false
2009-07-23 10:44:49,789 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting  compaction on region content,092501240723072009-0000000000000000308-10.3.134.228-746d6fcc693a34536215d76dee0fabee,1248345884988
2009-07-23 10:44:49,863 INFO org.apache.hadoop.hbase.regionserver.HRegion: region content,092501240723072009-0000000000000000308-10.3.134.228-a3e3be27683ba154539d1c055890e781,1248345884988/694893409 available

2009-07-23 10:44:52,736 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: Processing message (Retry: 0)
java.util.ConcurrentModificationException 
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
        at org.apache.hadoop.hbase.regionserver.HStore.getStorefilesIndexSize(HStore.java:2187)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doMetrics(HRegionServer.java:948)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:338)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1707,,,,"27/Jul/09 03:03;apurtell;master+regionserver224.rar;https://issues.apache.org/jira/secure/attachment/12414577/master%2Bregionserver224.rar",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25929,,,,,Sat Nov 21 19:18:26 UTC 2009,,,,,,,,,,"0|i0hetb:",99662,,,,,,,,,,,,,,,,,,,,,"27/Jul/09 03:03;apurtell;I'm told this is a 0.19.1 cluster and have suggested an upgrade.;;;","21/Nov/09 19:18;apurtell;Old issue not seen in a while. Probably fixed by 0.20. Reopen if necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
master fails to reassign region after failed deployment,HBASE-1707,12431470,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,apurtell,apurtell,26/Jul/09 20:14,11/Jun/22 20:58,01/Jul/25 07:49,21/Nov/09 19:19,0.19.1,,,,,,,,,,,,0,"From a Trend 0.19.3 cluster:

All the crawlers kept on printing log like this around 11:00 (UTC).

================================================

09/07/23 11:02:06 ERROR HBaseWriter:112 - org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server Some server for region , row '092501240723072009-0000000000000000308-10.3.134.228-d822aab6a9fdf4c216e397f00cf89d77', but failed after 11 attempts.

I think this is caused by regionserver problem; from regionserver log the key should be online after 10:44

=========================================

2009-07-23 10:44:49,789 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: content,092501240723072009-0000000000000000308-10.3.134.228-a3e3be27683ba154539d1c055890e781,1248345884988: safeMode=false
2009-07-23 10:44:49,789 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting  compaction on region content,092501240723072009-0000000000000000308-10.3.134.228-746d6fcc693a34536215d76dee0fabee,1248345884988
2009-07-23 10:44:49,863 INFO org.apache.hadoop.hbase.regionserver.HRegion: region content,092501240723072009-0000000000000000308-10.3.134.228-a3e3be27683ba154539d1c055890e781,1248345884988/694893409 available
2009-07-23 10:44:52,736 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: Processing message (Retry: 0)
java.util.ConcurrentModificationException 
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
        at org.apache.hadoop.hbase.regionserver.HStore.getStorefilesIndexSize(HStore.java:2187)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doMetrics(HRegionServer.java:948)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:338)
        at java.lang.Thread.run(Thread.java:619)

=========================================
",,,,,,,,,,,,,,,,,,,,,,,HBASE-1708,,,,,,"27/Jul/09 03:04;apurtell;master+regionserver224.rar;https://issues.apache.org/jira/secure/attachment/12414578/master%2Bregionserver224.rar",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25928,,,,,Sat Nov 21 19:19:08 UTC 2009,,,,,,,,,,"0|i0het3:",99661,,,,,,,,,,,,,,,,,,,,,"26/Jul/09 20:57;apurtell;Full master log should be available soon.;;;","27/Jul/09 03:03;apurtell;I'm told this is a 0.19.1 cluster and have suggested an upgrade.;;;","21/Nov/09 19:19;apurtell;Old issue not seen in a while. Probably fixed by 0.20. Reopen if necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ICVs across /during a flush can cause multiple keys with the same TS (bad),HBASE-1703,12431407,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ryanobjc,ryanobjc,ryanobjc,24/Jul/09 20:43,13/Sep/09 22:24,01/Jul/25 07:49,28/Jul/09 08:51,0.20.0,,,,,0.20.0,,,,,,,0,"We noticed a bug whereby the value in a hbase ICV'ed counter was lower, and the bug turned out to be that during a flush, the ICV will grab the KeyValue from 'memcache' and reuse the timestamp... if we grab the KeyValue from the _snapshot_ we end up with 2 key values, one in memcache, one in a hfile, both with the same timestamp, but one with a lower value than the other.

The fix is to not reuse timestamps if they come out of the snapshot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 08:49;ryanobjc;HBASE-1703.patch;https://issues.apache.org/jira/secure/attachment/12414727/HBASE-1703.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25924,,,,,Tue Jul 28 08:51:31 UTC 2009,,,,,,,,,,"0|i0hesf:",99658,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 08:49;ryanobjc;new test to cover the edge case;;;","28/Jul/09 08:51;ryanobjc;commited the change myself w/o review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web UI is extremely slow / freezes up if you have many tables,HBASE-1692,12431308,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,23/Jul/09 22:42,13/Sep/09 22:24,01/Jul/25 07:49,27/Jul/09 17:19,0.20.0,,,,,0.20.0,,,,,,,0,"I have 40 tables and every time the web ui loads (or i refresh it) the browser locks up and it takes 5-10 seconds to render the page.

In a dev cluster I had 3 tables and it was fast.

I thought there was a jira for this already but I couldn't find it... please correct me if there is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/09 10:40;streamy;HBASE-1692-v1.patch;https://issues.apache.org/jira/secure/attachment/12414602/HBASE-1692-v1.patch","27/Jul/09 12:06;streamy;HBASE-1692-v3.patch;https://issues.apache.org/jira/secure/attachment/12414608/HBASE-1692-v3.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25920,Reviewed,,,,Mon Jul 27 17:19:42 UTC 2009,,,,,,,,,,"0|i0heq7:",99648,,,,,,,,,,,,,,,,,,,,,"24/Jul/09 21:18;streamy;Tried this on two different OSes... still has problems.

Clicking expand all actually crashed my firefox.

Lars may be on vacation... I'm going to look at changing this back to the old format for now;;;","27/Jul/09 10:40;streamy;Changes master web ui back to old style listing.  Removes jquery files from static.;;;","27/Jul/09 12:06;streamy;This should apply cleanly, Eclipse did not want to behave.

Tested, this reverts back to what we had.  Talked to Lars, whenever we can make a better interface that is also fast we will change it again.;;;","27/Jul/09 12:39;streamy;Confirmed that this works on two systems.;;;","27/Jul/09 17:19;stack;Applied.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"major compaction can create empty store files, causing AIOOB when trying to read",HBASE-1686,12431199,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,ryanobjc,ryanobjc,23/Jul/09 00:39,13/Sep/09 22:24,01/Jul/25 07:49,27/Jul/09 17:22,0.20.0,,,,,0.20.0,,,,,,,0,"here is the backtrace:

Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.getFirstKey(HFile.java:991)
        at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.getStoreFile(StoreFileGetScan.java:84)
        at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.get(StoreFileGetScan.java:65)
        at org.apache.hadoop.hbase.regionserver.Store.get(Store.java:1548)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2263)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2252)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1739)

This can happen if your table only has deletes, and everything evaporates during a major compaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/09 21:17;stack;1686.patch;https://issues.apache.org/jira/secure/attachment/12414473/1686.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25916,,,,,Mon Jul 27 17:22:03 UTC 2009,,,,,,,,,,"0|i0heov:",99642,,,,,,,,,,,,,,,,,,,,,"23/Jul/09 13:11;jdcryans;Deletes biting our ass again... good catch!;;;","23/Jul/09 17:33;stack;Manish at rapleaf ran into this issue too only in his case the stacktrace was real ugly:

{code}
2009-07-21 00:00:29,996 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 1 on 60020, call get([B@346e6b9c, row=^\^U^D^U^N^@^\^U^D^X^Parrana88@aol.com^@^@, maxVersions=1, timeRange=[0,9223372036854775807), families={(family=piston, columns=ALL}) from 10.100.48.4:35231: error: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException
    at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:832)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:822)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1755)
    at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.ArrayIndexOutOfBoundsException
{code}

... and nothing else.;;;","23/Jul/09 17:38;stack;Oh, I know its the same issue because Manish did the work and added in printfs.;;;","23/Jul/09 17:44;ryanobjc;He has the client stack trace, you need to visit the server to get the one I
posted. Its probably the same cause.

On Jul 23, 2009 10:35 AM, ""stack (JIRA)"" <jira@apache.org> wrote:


   [
https://issues.apache.org/jira/browse/HBASE-1686?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12734655#action_12734655]

stack commented on HBASE-1686:
------------------------------

Manish at rapleaf ran into this issue too only in his case the stacktrace
was real ugly:

{code}
2009-07-21 00:00:29,996 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server
handler 1 on 60020, call get([B@346e6b9c, row=^\^U^D^U^N^@^\^U^D^X^
Parrana88@aol.com^@^@, maxVersions=1, timeRange=[0,9223372036854775807),
families={(family=piston, columns=ALL}) from 10.100.48.4:35231: error:
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException
   at
org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:832)
   at
org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:822)
   at
org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1755)
   at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
   at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
   at java.lang.reflect.Method.invoke(Method.java:597)
   at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
   at
org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)

Caused by: java.lang.ArrayIndexOutOfBoundsException
{code}

... and nothing else.

to read > --------------...
;;;","24/Jul/09 01:26;mnshah;I'm curious about why this is caused by a table only having delete operations applied to it.  In our application we are not performing any deletes.  The functionality is that we do a GET on a row, update the contents of the record, and then a PUT back into the same table.  Am i missing something? What are you referring to by ""table full of deletes"" ?;;;","24/Jul/09 05:18;stack;Chatting with Manish, his table schema is as follows:

{NAME => 'lookup_records', FAMILIES => [{NAME => 'piston', VERSIONS => true                                  
  '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536'                                       
 , IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}

And he is not deleting.

So, odd that he runs into this issue.;;;","24/Jul/09 21:17;stack;Two fixes:
1. Make it so if hfile has no content, you can do basic functions like
getFirstKey and getLastKey without array out of bounds
2. If a major compaction results in no output because all cells
deleted or expired, don't write an output file.

M  src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java
  (testMajorCompactingToNoOutput): Compaction test that will produce
  no output -- all contents were deleted.
M  src/test/org/apache/hadoop/hbase/regionserver/TestStore.java
  (testEmptyStoreFile): Test to make sure things basically work
  if an empty store hfile under the Store
M src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java
  (testEmptyHFile): Test to make sure hfile basically works if
  no content.
M  src/java/org/apache/hadoop/hbase/regionserver/StoreFileGetScan.java
  Formatting and in getStoreFile, allow that getFirstKey may come
  back null.
M src/java/org/apache/hadoop/hbase/regionserver/Store.java
  (compact): Javadoc cleanup. Some minor formatting cleanup (compact
  needs to be moved out of here and into its own classes but thats
  too big a change this close to RC).  Made it so can handle a
  compaction producing no product.  Make the writer instantiation
  lazy. Tidied logging a little.
M  src/java/org/apache/hadoop/hbase/KeyValue.java
  (keyToString): Allow that passed byte may be null. E.g. printing
  the first and last key of an empty hfile.
M src/java/org/apache/hadoop/hbase/io/hfile/HFile.java
  Test index is empty before getting first and last key.
  Javadoc.;;;","25/Jul/09 16:09;stack;Patch available... needs review.;;;","25/Jul/09 17:21;amansk;The patch is working for me. Not getting AIOOBE anymore.
+1;;;","27/Jul/09 12:38;streamy;Had same problem on our cluster.  Patch fixed AIOOB.  +1;;;","27/Jul/09 17:22;stack;Committed.  It worked to fix issue in two separate locations.  Committing also because it logs when scenario arises so we have some chance confirming indeedi ts because of major compaction that results in no output.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HBASE-1215 partial commit broke trunk, does not compile",HBASE-1661,12430498,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,15/Jul/09 14:19,13/Sep/09 22:24,01/Jul/25 07:49,15/Jul/09 14:28,0.20.0,,,,,0.20.0,,,,,,,0,"{noformat}
compile-core:
    [javac] Compiling 309 source files to /home/hbase20/src/hbase-0.20.0-trunk/build/classes
    [javac] /home/hbase20/src/hbase-0.20.0-trunk/src/java/org/apache/hadoop/hbase/util/FSUtils.java:300: missing return statement
    [javac]   }
    [javac]   ^
    [javac] /home/hbase20/src/hbase-0.20.0-trunk/src/java/org/apache/hadoop/hbase/util/Migrate.java:220: cannot find symbol
    [javac] symbol  : method allMajorCompacted()
    [javac] location: class org.apache.hadoop.hbase.util.Migrate
    [javac]     if (!allMajorCompacted()) {
    [javac]          ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/09 14:24;streamy;HBASE-1661-v1.patch;https://issues.apache.org/jira/secure/attachment/12413553/HBASE-1661-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25897,Reviewed,,,,Wed Jul 15 14:28:19 UTC 2009,,,,,,,,,,"0|i0hejz:",99620,,,,,,,,,,,,,,,,,,,,,"15/Jul/09 14:24;streamy;Compiles cleanly.  Still needs to be finished in migration.;;;","15/Jul/09 14:28;streamy;Committed after +1 from lars.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"client is broken, it requests ROOT region location from ZK too much",HBASE-1651,12430313,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,ryanobjc,ryanobjc,14/Jul/09 01:41,13/Sep/09 22:24,01/Jul/25 07:49,14/Jul/09 18:10,0.20.0,,,,,0.20.0,,,,,,,0,"something bad happened to the client, now it requests the ROOT region location literally a hundred times a second:

09/07/13 18:39:59 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/09 05:52;stack;hbase-1651.patch;https://issues.apache.org/jira/secure/attachment/12413386/hbase-1651.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25890,Reviewed,,,,Fri Jul 17 18:25:09 UTC 2009,,,,,,,,,,"0|i0hehr:",99610,,,,,,,,,,,,,,,,,,,,,"14/Jul/09 03:55;nitay;Was chatting with Stack about this. This brings up a good point, which is that we don't cache ZK results _anywhere_. The assumption we've had when writing all of this stuff was that we already did caching, so it should still work. For example, with the ROOT region, we used to get it from the master. I assumed the code previously would get it once, and only query the master for it again when it couldn't talk to ROOT anymore. So, by replacing it with ZK it should've still done the same caching. Clearly, either that logic wasn't right, or I broke it when we put the ZK stuff in. We should check all of our ZK queries, I have a feeling this is not the only place we'll find such a thing.

Good catch Ryan.;;;","14/Jul/09 03:57;stack;So, this log only shows if you enable DEBUG on zookeeper.

Looking in ZKW (and chatting w/ nitay), yeah, each log is a trip to ZK.  Would explain hbase-1614.

Taking a look...;;;","14/Jul/09 04:16;stack;Where you seeing this logging come out Ryan?  I don't see it in client or server.

I do see that we seem to clear cache too much.  On a NSRE, we relookup root and meta which seems unnecessary:

{code}
2009-07-14 04:10:10,282 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Removed .META.,,1 for tableName=.META. from cache because of TestTable,\x01\x00\x01\x06\x00\x03\x01\x02\x06\x06,99999999999999
2009-07-14 04:10:10,282 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: LOCATEROOTREGION
2009-07-14 04:10:10,305 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got XX.XX.XX.140:60020
2009-07-14 04:10:10,309 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Found ROOT at XX.XX.XX.140:60020
2009-07-14 04:10:10,318 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cached location address: XX.XX.XX.141:60020, regioninfo: REGION => {NAME => '.META.,,1', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192, TABLE => {{NAME => '.META.', IS_META => 'true', MEMSTORE_FLUSHSIZE => '16384', FAMILIES => [{NAME => 'historian', VERSIONS => '2147483647', COMPRESSION => 'NONE', TTL => '604800', BLOCKSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'}, {NAME => 'info', VERSIONS => '10', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}}
{code};;;","14/Jul/09 04:29;ryanobjc;this is on the client,the log4j is:

log4j.logger.org.apache.hadoop.hbase=DEBUG

This is new in the last few dozen commits, it didn't use to spew, the only difference is grabbing your new client code.;;;","14/Jul/09 04:29;ryanobjc;oh btw another critical aspect is I'm committing to a table with 2k regions.;;;","14/Jul/09 05:41;stack;Without below, we never cache root location -- we just get it every time:
{code}
Index: src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HConnectionManager.java	(revision 793429)
+++ src/java/org/apache/hadoop/hbase/client/HConnectionManager.java	(working copy)
@@ -512,7 +514,7 @@
           // second waits. The second thread will not do find.
           
           if (!useCache || rootRegionLocation == null) {
-            return locateRootRegion();
+            rootRegionLocation = locateRootRegion();
           }
           return rootRegionLocation;
         } 
{code}
Above seems safe enough.  Servers still get to set root location if they want.

The above 'fix' used to be in place.  Was removed here with this commit:

{code}
------------------------------------------------------------------------
r736371 | jimk | 2009-01-21 10:42:31 -0800 (Wed, 21 Jan 2009) | 1 line

HBASE-1121  Cluster confused about where -ROOT- is. This patch fixes ""re-finding"" the root region.
{code}

Here is patch that removed it:

{code}
svn diff -r736371:737213 src/java/org/apache/hadoop/hbase/client/HConnectionManager.java > /tmp/diff.txt
Index: src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HConnectionManager.java (revision 736371)
+++ src/java/org/apache/hadoop/hbase/client/HConnectionManager.java (revision 737213)
@@ -134,6 +134,7 @@
     private final Map<String, HRegionInterface> servers =
       new ConcurrentHashMap<String, HRegionInterface>();

+    // Used by master and region servers during safe mode only
     private volatile HRegionLocation rootRegionLocation;

     private final Map<Integer, SoftValueSortedMap<byte [], HRegionLocation>>
@@ -177,10 +178,12 @@
       return this.pause * HConstants.RETRY_BACKOFF[ntries];
     }

+    // Used by master and region servers during safe mode only
     public void unsetRootRegionLocation() {
       this.rootRegionLocation = null;
     }

+    // Used by master and region servers during safe mode only
     public void setRootRegionLocation(HRegionLocation rootRegion) {
       if (rootRegion == null) {
         throw new IllegalArgumentException(
@@ -445,7 +448,7 @@
           // second waits. The second thread will not do find.

           if (!useCache || rootRegionLocation == null) {
-            this.rootRegionLocation = locateRootRegion();
+            return locateRootRegion();
           }
           return rootRegionLocation;
         }
{code};;;","14/Jul/09 05:52;stack;Cache root in HConnectionManager.  This was undone a while ago.  As I read it, it should be fine putting it back.  Will test.;;;","14/Jul/09 06:01;ryanobjc;+1 on this.  I get less messages, and i took down the ROOT regionserver and we recovered ok.;;;","14/Jul/09 18:03;apurtell;+1 tested this also.;;;","14/Jul/09 18:10;stack;Tested too.  I killed root on loaded cluster and I then killed server with meta so clients would have to go back to find new root location.  I had a running shell with root cached.  Eventually it found the new root location.

Resolving.;;;","14/Jul/09 18:37;stack;I think we make too many requests of .META. too.  If we can't find an entry in the .META. for a particular region -- it just NSRE'd because it split or something -- then we purge the .META. from cache.  That seems wrong.  Its not like getting .META. again is going to give you a different response.;;;","17/Jul/09 18:25;stack;Backported to branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-1551 broke the ability to manage non-regionserver start-up/shut down. ie: you cant start/stop thrift on a cluster anymore,HBASE-1650,12430310,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,ryanobjc,ryanobjc,14/Jul/09 01:05,13/Sep/09 22:24,01/Jul/25 07:49,14/Jul/09 18:47,0.20.0,,,,,0.20.0,,,,,,,0,"I used to be able to do:

bin/hbase-daemons.sh stop thrift
bin/hbase-daemons.sh start thrift

I can't anymore. We can't manage anything but regionservers and zookeeper with this script now.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/09 03:02;stack;1650.patch;https://issues.apache.org/jira/secure/attachment/12413374/1650.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25889,Reviewed,,,,Tue Jul 14 18:47:40 UTC 2009,,,,,,,,,,"0|i0hehj:",99609,,,,,,,,,,,,,,,,,,,,,"14/Jul/09 03:01;stack;So, here is offending change:
{code}
diff --git a/bin/hbase-daemons.sh b/bin/hbase-daemons.sh
index 166af33..5af9408 100755
--- a/bin/hbase-daemons.sh
+++ b/bin/hbase-daemons.sh
@@ -37,6 +37,20 @@ bin=`cd ""$bin""; pwd`
 
 . $bin/hbase-config.sh
 
-exec ""$bin/regionservers.sh"" --config ""${HBASE_CONF_DIR}"" \
- cd ""${HBASE_HOME}"" \; \
- ""$bin/hbase-daemon.sh"" --config ""${HBASE_CONF_DIR}"" ""$@""
+remote_cmd=""cd ${HBASE_HOME}; $bin/hbase-daemon.sh --config ${HBASE_CONF_DIR} $@""
+args=""--config ${HBASE_CONF_DIR} $remote_cmd""
+
+command=$2
+case $command in
+  (regionserver)
+    exec ""$bin/regionservers.sh"" $args
+    ;;
+  (zookeeper)
+    exec ""$bin/zookeepers.sh"" $args
+    ;;
+  (*)
+    echo $usage
+    exit 1
+    ;;
+esac
+
{code}

Rather than print usage, running with regionservers should be default?;;;","14/Jul/09 03:02;stack;Suggested fix.;;;","14/Jul/09 18:05;apurtell;+1;;;","14/Jul/09 18:28;ryanobjc;+1 lets do it;;;","14/Jul/09 18:47;stack;Thanks for review Andrew.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ValueFilter may not reset its internal state,HBASE-1649,12430174,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,dogacan,dogacan,12/Jul/09 22:36,13/Sep/09 22:24,01/Jul/25 07:49,13/Jul/09 18:10,0.20.0,,,,,0.20.0,,,Filters,,,,0,"ValueFilter#reset is empty even though the class uses two internal variables. These values are reset in filterRow, however there are instances where filterRow may not be called. For example, if you chain two filters through FilterList (with PASS_ALL and the second filter being a ValueFilter) then during FilterList#filterRow if the first filter#filterRow returns true then ValueFilter#filterRow will not be called.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/09 22:37;dogacan;HBASE-1649.patch;https://issues.apache.org/jira/secure/attachment/12413245/HBASE-1649.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25888,Reviewed,,,,Mon Jul 13 18:10:29 UTC 2009,,,,,,,,,,"0|i0hehb:",99608,,,,,,,,,,,,,,,,,,,,,"12/Jul/09 22:37;dogacan;Simple patch that resets ValueFilter's internal state in reset().;;;","13/Jul/09 18:10;stack;Reviewed and made sure our TestVF passed.

Thanks for the patch Doğacan.  Good stuff.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Filter#filterRow is called too often, filters rows it shouldn't have",HBASE-1647,12430138,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,dogacan,dogacan,11/Jul/09 12:04,13/Sep/09 22:24,01/Jul/25 07:49,28/Jul/09 20:29,0.20.0,,,,,0.20.0,,,,,,,0,"Filter#filterRow is called from ScanQueryMatcher#filterEntireRow which is called from StoreScanner.next. However, if I understood the code correctly, StoreScanner processes KeyValue-s in a column-oriented order (i.e. after row1-col1 comes row2-col1, not row1-col2). Thus, when filterEntireRow is called, in reality, the filter only processed (via filterKeyValue) only one column of a row.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/09 22:46;dogacan;HBASE-1647-v2.patch;https://issues.apache.org/jira/secure/attachment/12413246/HBASE-1647-v2.patch","14/Jul/09 12:14;dogacan;HBASE-1647-v3.patch;https://issues.apache.org/jira/secure/attachment/12413418/HBASE-1647-v3.patch","15/Jul/09 13:08;dogacan;HBASE-1647-v4.patch;https://issues.apache.org/jira/secure/attachment/12413547/HBASE-1647-v4.patch","17/Jul/09 11:18;dogacan;HBASE-1647-v5.patch;https://issues.apache.org/jira/secure/attachment/12413801/HBASE-1647-v5.patch","17/Jul/09 11:54;dogacan;HBASE-1647-v6.patch;https://issues.apache.org/jira/secure/attachment/12413805/HBASE-1647-v6.patch","11/Jul/09 12:07;dogacan;ScanBug.java;https://issues.apache.org/jira/secure/attachment/12413196/ScanBug.java","11/Jul/09 12:07;dogacan;scanfilter.patch;https://issues.apache.org/jira/secure/attachment/12413197/scanfilter.patch",,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25886,Reviewed,,,,Tue Jul 28 20:29:34 UTC 2009,,,,,,,,,,"0|i0heh3:",99607,,,,,,,,,,,,,,,,,,,,,"11/Jul/09 12:07;dogacan;1) A simple class to demonstrate the problem. Not that col2:-s are all filtered even though the ValueFilter instance should only work on col1:-s. When running give -create as an argument to create the table.

2) Patch for the issue. Patch delete the ScanQueryMatcher#filterEntireRow method and instead calls Filter#filterRow in HRegion.RegionScanner#next. I am not sure if this patch is complete/correct but it fixes the problem for me. Not that you also need HBASE-1646 to test this patch (or the class above).;;;","12/Jul/09 22:46;dogacan;v2 of patch.

I think all reset()s should be moved into RegionScanner#next as well. Since we merge different columns of the same row in RegionScanner, calling reset anywhere else (especiall in ScanQueryMatcher, which is called from StoreScanner#next) seems like a bug.;;;","13/Jul/09 19:22;stack;Patch looks good.   +1.   Its a radical change in Filter processing though it looks right and all tests pass.  Can someone else look at this?  Ryan?  I'd like others input before commiting.

On StoreScanner running through in an column order rather than row-at-a-time, thats not how I understand it works but maybe thats how it appears in this context.;;;","13/Jul/09 19:22;stack;Bringing into 0.20.0.;;;","13/Jul/09 20:20;ryanobjc;I'm afraid this patch is not good, the assumptions of the reporter are not correct, we in fact see things in this order:

row1 / col1
row1 / col2
row2 / col1
row2 / col2

This patch removes a key piece of functionality I built into the new API that allows us to post-facto filter a row.

The API needs to be clearer perhaps.  The intent is:

- use filterRowKey() to have the earliest chance to filter an entire row based on the _KEY_ only.
- for more complex processing, implement filterKeyValue() and set internal state. 
-- At the ""End"" of the KeyValues for a row, filterRow() will be called, and you can choose, based on that state, to filter the row.  Eg: if a column was missing (which you can't know until you hit the end of the KeyValues for a row).  

If you don't use this feature, implement filterRow() { return false; }.  The JIT hotspot will take care of optimizing things.
;;;","13/Jul/09 20:31;dogacan;Hey Ryan,

I did not mean that with my description but I was not clear at all. From IRC:

dogacan: St^Ack: ""On StoreScanner running through in an column order rather than row-at-a-time, thats not how I understand it works but maybe thats how it appears in this context.""
[10:32pm] dogacan: you are right here. I meant (again if I understood code correctly) scanners go to next row to figure out they went too far [i mean we peek to the next row in StoreScanner then get DONE then call filterRow]
[10:32pm] dogacan: and when they do, they used to call filterRow

([.....] part was not on IRC)

Anyway, did you try the ScanBug class I attached? When you set a ValueFilter, it filters out all other columns (because in current way there is almost one filterRow call for every filterKeyValue call).;;;","14/Jul/09 12:14;dogacan;v3 of patch

1) It seems stopRow pass check is only done if results.isEmpty. This patch makes stopRow pass checks at every row change.

2) I moved filterRowKey into RegionScanner as well. Again, please review :), but I think calling filterRowKey in RegionScanner once should be safe and (very) slightly faster.
;;;","15/Jul/09 13:08;dogacan;v4 of patch.

* I have removed all test methods in TestStoreScanner as most of the filter methods are now called in RegionScanner. Should I also refactor the test methods to TestScanner?

* I have made a small change in TestScanner. RegionScanner#next's javadoc:

{code}

    /**
     * Get the next row of results from this region.
     * @param results list to append results to
     * @return true if there are more rows, false if scanner is done
     */

{code}

And in TestScanner#testStopRow:

{code}

      InternalScanner s = r.getScanner(scan);
      int count = 0;
      while (s.next(results)) {
        count++;
      }

{code}

In trunk count is 1. However, there is only one row to scan (""abc""). Since once we call next (and put KeyValue-s in results) there are no more rows so I think we must return false (thus count is 0). Please correct me if I am wrong here.

* There was a possibly serious bug in v3 in RegionScanner. It implicitly assumes that the caller cleared results list between calls to RegionScanner#next. If caller doesn't do that, we may delete results from older rows or even get stuck in an infinite loop. So I added a new field to RegionScanner. KeyValue-s are initially accumulated (or filtered) in this new field. Upon completion of next, they are added to the outResults. I am not sure if this is  necessary (no code in hbase reuses results).;;;","15/Jul/09 17:46;stack;.bq I have removed all test methods in TestStoreScanner as most of the filter methods are now called in RegionScanner. Should I also refactor the test methods to TestScanner?

Tests are kinda critical for this infrequently used but critical feature.

But this issue is more about how the new filter Interface works, fixing the context at which each of the filter methods are called.  Lets get that worked out first before we work on tests.

.bq On TestScanner#testStopRow.... 1 vs 0

That looks right.

On the javadoc change, I don't see it in the patch.

Otherwise, patch looks good to me.  Let me kick Ryan and get him to review it.




;;;","17/Jul/09 00:26;ryanobjc;There are some issues that need to be addressed before this can go in:

- results is now a field for no reason. This reduces GC efficiency and performance.
- RegionScanner#next is a mess now. Too many boolean flags, I don't detect a sense of clear minded purpose. Unbalanced and uncertain flags and filter.reset calls make me concerned about bugs.
- The last bug one is tests were deleted, instead of migrated. We lose test coverage with this patch.

I'm poking at it more, but the next and test issue are show stoppers.;;;","17/Jul/09 11:18;dogacan;>  # results is now a field for no reason. This reduces GC efficiency and performance.

I explained why in my previous comment. Not sure if mine is a valid reason for worrying though. It seems results is always cleared in internal hbase usage so my extra safeguard there may be pointless.

> RegionScanner#next is a mess now. Too many boolean flags, I don't detect a sense of clear minded purpose. 
> Unbalanced and uncertain flags and filter.reset calls make me concerned about bugs.

I see your point, yet in other ways, it is also clearer now. All the extra logic outside the while loop is moved into the loop, and stop row comparison code is now in one place.

I reduced boolean flags to one (filterCurrentRow). It is an optimization flag like stickyNextRow in underlying scanners.

I also refactored code a bit. Let me know if it is clearer now.

> # The last bug one is tests were deleted, instead of migrated. We lose test coverage with this patch.

I added tests to TestScanner.;;;","17/Jul/09 11:54;dogacan;oops. Sorry minor bug in last patch, updated.....;;;","17/Jul/09 22:03;clint.morgan;This patch fixes a few of my filter backed tests that were failing.;;;","20/Jul/09 08:10;ryanobjc;thanks for the updated patch. The tests need to be migrated not just deleted, but I'll poke at the implementation and its consequences compared to the previous implementation tomorrow.  I've posted it to github:

http://github.com/ryanobjc/hbase/commit/3b31d0f4b0c0df2ad519f421d35cbb216da054e1;;;","20/Jul/09 19:04;dogacan;No problem.

Btw, I updated the tests in latest patch (moved them into TestScanner).;;;","25/Jul/09 16:11;stack;Patch available, assigning Ryan for review.  Assign it to me Ryan if you want me to review it instead.;;;","28/Jul/09 08:52;ryanobjc;stack, put it in as is, and I'll make adjustments if necessary. i want to think about the code a bit more, but i've been busy. i'll do it tomorrow.;;;","28/Jul/09 12:11;stack;Ok Ryan.

Committed.  Thanks for patch Doğacan.;;;","28/Jul/09 12:52;stack;Reopening.  It may have broken scanning.  Not sure yet.  Looking.;;;","28/Jul/09 13:27;stack;Its not this patch that was prob.  Will reapply in a while.;;;","28/Jul/09 20:29;stack;Committed (again).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan-s can't set a Filter,HBASE-1646,12430137,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,dogacan,dogacan,11/Jul/09 11:26,06/Nov/09 08:40,01/Jul/25 07:49,12/Jul/09 23:22,0.20.0,,,,,0.20.0,,,,,,,0,"Scan#write:

{code}
      HbaseObjectWritable.writeObject(out, this.filter, Filter.class, null);
{code}

Because of the third argument (Filter.class), HbaseObjectWritable can not write or read the filter (as Filter is not instantiable).",,,,,,,,,,,,,,,HBASE-1957,,,,,,,,,,,,,,"12/Jul/09 17:48;dogacan;HBASE-1646-v2.patch;https://issues.apache.org/jira/secure/attachment/12413240/HBASE-1646-v2.patch","11/Jul/09 11:28;dogacan;scan.patch;https://issues.apache.org/jira/secure/attachment/12413194/scan.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25885,Reviewed,,,,Sun Jul 12 23:22:52 UTC 2009,,,,,,,,,,"0|i0hegv:",99606,,,,,,,,,,,,,,,,,,,,,"11/Jul/09 11:28;dogacan;You can just change third arg to this.filter.getClass() too. But using HbaseObjectWritable, you can not write your own classes as HOW throws exception if you give it a class that in not in its map. 

This patch instead just reads/writes the full class name.;;;","12/Jul/09 17:09;erikholstad@gmail.com;Hey Doĝacan.
I think we are trying to move away from the use of the Text class, maybe you can use ImmutableBytesWritable or something like that instead?

Regards Erik;;;","12/Jul/09 17:13;erikholstad@gmail.com;Do you think you can name the patches something like HBASE-1646-v1.patch in the future, just to keep it easy to track?

Regards Erik;;;","12/Jul/09 17:48;dogacan;Patch updated to use Bytes.{read,write}ByteArray instead of Text methods.;;;","12/Jul/09 23:22;stack;Committed.  Thanks for the patch Doğacan Güney.  I added to the Scan serialization a test for a Scan carrying a filter.  This patch is doubly good in that it removes the new HBaseConfiguration that was done inside in the HOW serializing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Result.row is cached in getRow; this breaks MapReduce",HBASE-1644,12430100,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,dogacan,dogacan,10/Jul/09 21:26,03/Apr/13 05:49,01/Jul/25 07:49,10/Jul/09 21:34,0.20.0,,,,,0.95.1,,,,,,,0,"In Result#getRow row field is computed (if row is null) and then is cached for further uses. But since MapReduce uses the same Result instance through different map()/reduce() calls, row field is not updated when Result instance changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/09 21:27;dogacan;rownull.patch;https://issues.apache.org/jira/secure/attachment/12413157/rownull.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25884,Reviewed,,,,Fri Jul 10 21:34:36 UTC 2009,,,,,,,,,,"0|i0hegf:",99604,,,,,,,,,,,,,,,,,,,,,"10/Jul/09 21:27;dogacan;Patch for the problem.

Also TableInputFormatBase#nextKeyValue reads Result into a temporary result variable then copies it into value field. AFAICS, there is no point in doing that. So this patch also removes the local result variable.;;;","10/Jul/09 21:34;stack;Committed.  Thanks for the patch Doğacan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PerformanceEvaluation should use scanner prefetching,HBASE-1635,12429963,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,streamy,streamy,09/Jul/09 17:48,13/Sep/09 22:24,01/Jul/25 07:49,09/Jul/09 17:56,0.20.0,,,,,0.20.0,,,,,,,0,"Right now default scanner prefetching is set to 1.  In PerformanceEvaluation, this leads to basically benchmarking RPC round-trip performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/09 17:54;jdcryans;1635.patch;https://issues.apache.org/jira/secure/attachment/12413037/1635.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25878,Reviewed,,,,Thu Jul 09 18:18:23 UTC 2009,,,,,,,,,,"0|i0heen:",99596,,,,,,,,,,,,,,,,,,,,,"09/Jul/09 17:54;jdcryans;Patch that adds precaching.;;;","09/Jul/09 17:55;streamy;Though extraordinarily complex, I trust that you have done the right thing.

+1 for commit;;;","09/Jul/09 17:56;jdcryans;Committed to trunk.;;;","09/Jul/09 18:05;stack;Won't this destabilize the whole code base?;;;","09/Jul/09 18:18;streamy;Anything for more performance!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRS unable to contact master,HBASE-1629,12429848,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,nitay,amansk,amansk,08/Jul/09 20:23,13/Sep/09 22:24,01/Jul/25 07:49,09/Jul/09 23:05,0.20.0,,,,,0.20.0,,,,,,,0,"HRS unable to contact master for initialization after expiration from ZK. Master thinks HRS is still up whereas HRS went down and now cannot restart. The RS logs have a flurry of the following warning messages:

2009-07-08 12:53:19,547 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: Unable to get master for initialization

More logs from the RS and the Master attached.",,,,,,,,,,,,,,,,,,,,,,,HBASE-1601,,,,,,"08/Jul/09 20:35;amansk;Master_log;https://issues.apache.org/jira/secure/attachment/12412906/Master_log","08/Jul/09 20:35;amansk;RS_Log;https://issues.apache.org/jira/secure/attachment/12412905/RS_Log","08/Jul/09 22:24;nitay;hbase-1629.patch;https://issues.apache.org/jira/secure/attachment/12412929/hbase-1629.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25874,,,,,Thu Jul 09 23:05:08 UTC 2009,,,,,,,,,,"0|i0hedj:",99591,,,,,,,,,,,,,,,,,,,,,"08/Jul/09 20:35;amansk;Attaching logs from the RS and the Master from the time the failure started;;;","08/Jul/09 22:24;nitay;Small patch for a convoluted problem. Amandeep, try this out, see if it fixes it for you.

Here's the problem:

{noformat}
[14:32]  <nitay> reportForDuty()
[14:32]  <nitay>     while (!getMaster()) {
[14:32]  <nitay>       sleeper.sleep();
[14:32]  <nitay>       LOG.warn(""Unable to get master for initialization"");
[14:32]  <nitay>     }
[14:33]  <nitay> getMaster()
[14:33]  <nitay>     HServerAddress masterAddress = null;
[14:33]  <nitay>     while (masterAddress == null) {
[14:33]  <nitay>       if (stopRequested.get()) {
[14:33]  <nitay>         return false;
[14:33]  <nitay>       }
{noformat}

This is an infinite loop which causes the messages at the end of the RS Log Amandeep posted.

The flow of logic that leads to this is the following:
# RS session with ZooKeeper expires.
# Master gets znode expiration, starts cleanup/shutdown of RS.
# RS gets its session expired, begins restart() logic, setting stopRequested.
# Meanwhile, RS run() thread is still talking to master.
# Master gets a message from RS, but doesn't know it because it's been removed. This is the ""received server report from unknown server..."" stuff. Tells the RS to reinitialize, sending MSG_CALL_SERVER_STARTUP.
# RS on getting MSG_CALL_SERVER_STARTUP calls reportForDuty() and is now in a loop. The restart() thread from ZooKeeper is waiting for the RS run() to finish, but it never will.

This simple patch makes reportyForDuty() fail fast when stopRequested is set.;;;","09/Jul/09 06:09;stack;+1

Would be great if AK could try it before commit but I'm fine w/ commit if not -- easy enough see if this fixes issue.;;;","09/Jul/09 23:05;nitay;Stack said to go ahead and commit this. Amandeep, if you see this again, feel free to reopen the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableInputFormatBase#nextKeyValue catches the wrong exception,HBASE-1627,12429801,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,dogacan,dogacan,dogacan,08/Jul/09 11:43,13/Sep/09 22:24,01/Jul/25 07:49,10/Jul/09 19:07,0.20.0,,,,,0.20.0,,,,,,,0,"TableInputFormatBase#nextKeyValue only catches UnknownScannerException from Scanner.next. However, scanner may throw other exceptions:

{code}
/* from HTable.ClientScanner#next */
          try {
            values = getConnection().getRegionServerWithRetries(callable);
          } catch (IOException e) {
            if (e instanceof UnknownScannerException &&
                lastNext + scannerTimeout < System.currentTimeMillis()) {
              ScannerTimeoutException ex = new ScannerTimeoutException();
              ex.initCause(e);
              throw ex;
            }
            throw e;
          }

{code}

Is there any reason why TIFB does not catch ScannerTimeoutException?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/09 11:44;dogacan;tableinput.patch;https://issues.apache.org/jira/secure/attachment/12412854/tableinput.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25873,Reviewed,,,,Fri Jul 10 19:07:54 UTC 2009,,,,,,,,,,"0|i0hed3:",99589,,,,,,,,,,,,,,,,,,,,,"08/Jul/09 11:44;dogacan;A simple patch to change catch clause to catch all IOException-s. Are there any downsides to this? If an IOException is thrown, reset your scanner, try again and if it fails again your job fails. Makes sense to me :D;;;","08/Jul/09 19:09;stack;IOE seems overly broad (almost everything is IOE at its base).

Above looks like a bug in TIFB.  It should have been changed to handle STE instead of USE.  If you are good with making this change, I'll commit your patch changing your overly general IOE to STE?;;;","08/Jul/09 21:49;dogacan;Yeah sure I can change it to STE but why is IOE being broad a problem? I mean, we catch the IOE and recreate the scanner, try next()ing again and fail if there is an exception again. Is there any scenario where this is harmful?;;;","10/Jul/09 18:08;stack;Because near everything is an IOE at its base in hbase, its hard to distingush the retryable from the IOE that is fatal.   My fear is that if we make the catch overly broad, it will retry for ever rather than fail.  If OK with you, I'll commit STE instead of IOE?  Thats a fix for a breakage.  We can keep an eye on this and if other IOE subclasses that can bear retry, lets add them?;;;","10/Jul/09 18:46;dogacan;Sorry for being thick but I still don't understand. AFAICS we only retry once. So if the IOE that we capture is fatal then when we try again we will get the fatal exception again and the job will fail. So IIRC we ONLY try once. 
  ;;;","10/Jul/09 19:03;stack;Thanks for persisting.  Its I who is think.  Yes, will retry once only since we redo the next inside the catch block.  Let me commit.  Good stuff.;;;","10/Jul/09 19:07;stack;Committed.  Thanks for the patch Doğacan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Adding check to Put.add(KeyValue kv), to see that it has the same row as when instantiated",HBASE-1625,12429769,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,erikholstad@gmail.com,erikholstad@gmail.com,erikholstad@gmail.com,07/Jul/09 23:46,13/Sep/09 22:24,01/Jul/25 07:49,08/Jul/09 04:04,0.20.0,,,,,0.20.0,,,Client,,,,0,When using the add(KeyValue kv) in Put there is no check to see if the kv has the same row as the row already in the put.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/09 23:49;erikholstad@gmail.com;HBASE-1625-v1.patch;https://issues.apache.org/jira/secure/attachment/12412803/HBASE-1625-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25871,Reviewed,,,,Wed Jul 08 04:04:37 UTC 2009,,,,,,,,,,"0|i0hecn:",99587,,,,,,,,,,,,,,,,,,,,,"07/Jul/09 23:49;erikholstad@gmail.com;Adding check in Put for same row and new test case.;;;","08/Jul/09 04:04;stack;Good one.  Thanks for the patch Erik.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Need to use special StoreScanner constructor for major compactions (passed sf, no caching, etc)",HBASE-1620,12429684,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,07/Jul/09 07:29,13/Sep/09 22:24,01/Jul/25 07:49,07/Jul/09 18:10,0.20.0,,,,,0.20.0,,,regionserver,,,,0,"Should not cache blocks during major compactions like with minor compactions.

Also, need to only work on passed StoreFiles.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/09 07:41;streamy;HBASE-1620-v1.patch;https://issues.apache.org/jira/secure/attachment/12412713/HBASE-1620-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25868,Reviewed,,,,Wed Jul 08 15:27:30 UTC 2009,,,,,,,,,,"0|i0hebr:",99583,,,,,,,,,,,,,,,,,,,,,"07/Jul/09 07:37;streamy;Broadening scope and making a blocker.  There is at least one nasty bug in major compactions.;;;","07/Jul/09 07:41;streamy;Adds new StoreScanner constructor that takes a list of StoreFiles.

Other changes include not adding to changed readers observer list and we automatically will not cache blocks now since we use the right thing.;;;","07/Jul/09 17:05;stack;Whats this patch adding?

You have yet to add the observer?;;;","07/Jul/09 18:01;stack;Jon explained whats going on here to me in IRC.

Adds constructor that takes the scanners setup doing major compaction rather than create new ones.  The new ones will not have the ""do not cache"" flag set.;;;","07/Jul/09 18:08;streamy;Also does not add to changed readers observer list.  We don't want to add newly flushed storefiles midway through compaction.  In the future, we could snapshot  the memcache and include it in the major compaction.  Would be a good way to help keep storefiles low.

Ready for commit.;;;","07/Jul/09 18:10;stack;Thanks for patch Jon.;;;","08/Jul/09 07:57;ryanobjc;it is possible this patch ups the perf of hbase... I now see smaller GC pauses, and I am achieving higher hbase perf than before.;;;","08/Jul/09 15:27;streamy;Definitely possible, though it's really from HBASE-1597 and then the reimplementation in HBASE-1615 not this issue (which is for major).

I'm unsure why this would not have impacted the old LRU... I would expect the new one to be faster not slower.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Redo MemStore heap sizing to be accurate, testable, and more like new LruBlockCache",HBASE-1607,12429425,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,02/Jul/09 20:54,13/Sep/09 22:24,01/Jul/25 07:49,03/Jul/09 19:44,0.20.0,,,,,0.20.0,,,regionserver,,,,0,"MemStore sizing is inaccurate and does not include all overhead.

I'm going to make it look like the LruBlockCache does.  Will provide a MemStore.heapSize() method that includes ALL overhead of the MemStore itself.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/09 22:29;streamy;HBASE-1607-v1.patch;https://issues.apache.org/jira/secure/attachment/12412428/HBASE-1607-v1.patch","03/Jul/09 19:20;streamy;HBASE-1607-v2.patch;https://issues.apache.org/jira/secure/attachment/12412518/HBASE-1607-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25859,Reviewed,,,,Fri Jul 03 19:44:33 UTC 2009,,,,,,,,,,"0|i0he93:",99571,,,,,,,,,,,,,,,,,,,,,"02/Jul/09 22:29;streamy;Patch does what's described in jira description.

Two things to note:

- MemStore, Store, and HRegion now all implement HeapSize.  This is not really being used yet but this is how LruBlockCache works and will allow for us to have intelligent load balancing, control of memory usage, etc.

- The Store itself is still tracking the size of the keys that have been added into the MemStore, rather than querying MemStore.heapSize().  I don't see another way, for now, because it decrements at the right time based on the snapshot and flush.  So MemStore.heapSize() is currently unused.

So what this patch has actually done is make the incremental sizing of each addition to MemStore more accurate.  And so that any changes to objects involved will throw errors in tests if overheads were not updated accordingly.;;;","02/Jul/09 22:30;streamy;Ready for review.  Patch kinda adds a lot but does not touch any major codepath outside of MemStore.heapSize -> MemStore.heapSizeChange and the constants in that.

All the heap sizing tests are passing for me on 32bit windows and 64bit linux.  Please verify io.TestHeapSize passes.;;;","02/Jul/09 22:32;streamy;MemStore is now public so that I can test it from TestHeapSize.  Otherwise could add a separate TestHeapSize inside regionserver package?;;;","03/Jul/09 00:23;stack;Remove rather than comment out stuff.

Is ClassSize utility to estimate a Class size or instance of a Class size?  If latter, should be renamed.

All these DEEP_OVERHEAD and FIXED_OVERHEAD defines make me nervous.  Would seem to be very brittle and hard to make changes.  Where are they used?  In test only?









;;;","03/Jul/09 00:49;streamy;Did I comment out stuff?  Didn't think I had.  This patch might remove previously commented out stuff.

ClassSize is a utility to help us size classes.  It generates the sizes for the different native java classes that we use in heap sizable classes.  It does nothing with an instantiated class.


I agree that FIXED_OVERHEAD and DEEP_OVERHEAD are not necessarily the best long term solution out there :)  Currently in trunk, our MemStore is sized with this beautiful line:

{noformat}
private final static int ESTIMATED_KV_HEAP_TAX = 60;
  long heapSize(final KeyValue kv, final boolean notpresent) {
    return notpresent?
      // Add overhead for value byte array and for Map.Entry -- 57 bytes
      // on x64 according to jprofiler.
      ESTIMATED_KV_HEAP_TAX + 57 + kv.getLength(): 0; // Guess no change in size.
  }
{noformat}

So this is a vast improvement.  It might look nasty but it's not a hard-coded number, rather it is calculated.  Interestingly, 57 bytes is not a valid size for something in memory because everything is aligned (4 byte on 32bit, 8 byte on 64bit).

Again, I set out with two goals.  Make sizing as accurate as possible, and make tests so that if any of our sized classes change the tests will fail.  This has been done with this patch and those before it.

So FIXED_OVERHEAD might look confusing, but it's principled and shows where the sizing is coming from.  When the unit test fails, ClassSize outputs it's sizing in debug mode so you can very easily see exactly where you missed.  Message is something like:  Expected <104> but got <96>.  And there is output of how many references, primitives, etc it found.  Quite simple to fix.  If you want to play with it, just modify one of the FIXED_OVERHEADs and run TestHeapSize.

This is not a permanent solution, perhaps, but works well and is far better than the hard coded values we have now.;;;","03/Jul/09 19:20;streamy;Removes any previous changes to HTable.

Updates ConcurrentSkipListMap entry size according to research by erik.;;;","03/Jul/09 19:44;stack;Reviewed and ran tests.  Thanks for patch Jon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseClient.getConnection() may return a broken connection without throwing an exception,HBASE-1604,12429383,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,jkff,jkff,02/Jul/09 12:02,13/Sep/09 22:24,01/Jul/25 07:49,02/Aug/09 23:30,0.19.2,,,,,0.20.0,,,Client,,,,0,"Consider the code of HBaseClient.getConnection():

{code}
    connection.setupIOstreams();
    return connection;
  }
{code}

Now consider the setupIOstreams() method:
{code}
      } catch (IOException e) {
        markClosed(e);
        close(); // Removes the connection from pool
      }
{code}

So, if something goes wrong inside of setupIOstreams, then after its invocation the connection will be broken (will have its .in and .out streams nulls, for example) and will not be in pool, but will still be returned from the getConnection method and cause further harm (for example, cause a NullPointerException in further calls such as sendCall, which use the in and out streams).

Suggested fix: make the setupIOstreams method rethrow the IOException inside that catch block.

Reproduction: Restart the hbase master and/or regionserver while a client program is running, and put a breakpoint into that catch block.

I actually observed a situation where the broken connection stayed in the pool, but I don't yet know how to reproduce it or what is the reason. I am investigating the issue, but for now at least the aforementioned bug should be fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/09 15:41;jkff;hbase-1604.patch;https://issues.apache.org/jira/secure/attachment/12413568/hbase-1604.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25858,Reviewed,,,,Sun Aug 02 23:30:41 UTC 2009,,,,,,,,,,"0|i0he8n:",99569,,,,,,,,,,,,,,,,,,,,,"15/Jul/09 12:38;jkff;2 weeks passed without reaction. Hey, developers!

I've implemented the suggested fix and it improved things somewhat; the described bug is fixed, and now the situation where a broken connection stays in pool occurs extremely rarely, but it still occurs. I still don't know why.;;;","15/Jul/09 15:21;streamy;Eugene, can you provide a patch?;;;","15/Jul/09 15:41;jkff;Attaching a patch against trunk.;;;","15/Jul/09 16:14;stack;Your patch seems to be reversed.  Is that possible Eugene?  You want to throw the exception, is that right?

Have you run into this issue or is this from reading code?

This code is actually from Hadoop.  Its the Hadoop RPC code with small amendments to suit hbase case.;;;","15/Jul/09 18:14;jkff;Oops. Sorry, the patch is indeed reversed; I want to throw the exception. Should I re-upload a correct version?

I have run into this issue and traced it in a debugger: indeed, broken connections were being returned after a regionserver restart, and this issue disappeared when I rethrew the exception.;;;","02/Aug/09 23:30;stack;Committed to 0.20 branch and TRUNK.  Thanks for the patch Eugene.  I just experienced something similar; there should be more info on busted connection.  Hopefully this helps (I reversed your reversed patch).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent unnecessary caching of blocks during compactions,HBASE-1597,12429253,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,01/Jul/09 05:01,10/Jun/11 00:37,01/Jul/25 07:49,03/Jul/09 18:41,0.20.0,,,,,0.20.0,,,,,,,0,"When running any kind of compaction, we read every block of every storefile being compacted into the block cache.

We would like to reuse any already cached blocks, if available, but otherwise we should not bog down the LRU with unnecessary blocks.

This is not as bad as it was with the old LRU because the latest LRU implementation (HBASE-1460) is scan-resistant.  This ensures that we are not causing massive eviction of the blocks that are being read multiple times or from in-memory tables.  However, this does add to the GC-woes of an import because each block gets further referenced, and for longer periods of time.  There is also overhead in running the LRU evictions.",,,,,,,,,,,,,,,,,,,,,,,HBASE-3976,,,,,,"03/Jul/09 00:38;streamy;HBASE-1597-v1.patch;https://issues.apache.org/jira/secure/attachment/12412442/HBASE-1597-v1.patch","03/Jul/09 17:47;streamy;HBASE-1597-v2.patch;https://issues.apache.org/jira/secure/attachment/12412513/HBASE-1597-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25852,Reviewed,,,,Fri Jul 03 18:41:12 UTC 2009,,,,,,,,,,"0|i0he7b:",99563,,,,,,,,,,,,,,,,,,,,,"01/Jul/09 05:04;streamy;Currently tied to 0.20.0 until we investigate further.  Ryan, what say you?;;;","03/Jul/09 00:38;streamy;Creates an additional HFile.Reader subclass called HFile.CompactionReader

Overrides a single method:  ByteBuffer readBlock(int)

Same code in that method except at the end it doesn't cache the block.

Adds a method StoreFile.getCompactionReader() which instantiates a new CompactionReader, taking the normal Reader as input.

Switched the necessary calls from getReader() to getCompactionReader().  There were about 6 of em.

The nasty part of this now is that it has to instantiate a new CompactionReader rather than just casting or setting a flag.  Lots of other ways to implement this, probably more cleanly and more efficient... Just wanted to put something up so we can have a basis for continued discussion


Compaction test passes.;;;","03/Jul/09 01:16;apurtell;+1

I think this is good enough to retire this issue and move on. Any chance that CompactionReader may acquire more function over time or be specialized? Having it as a separate reader class would not necessarily be bad in that case. ;;;","03/Jul/09 15:06;streamy;Good point, Andrew.  Somehow I forgot about an issue I actually filed, HBASE-1521.

Really what I didn't like in my patch was how I re-instantiate the Reader as a CompactionReader, since I can't cast up to it.  I did it in a way that just assigns all the internal variables so there's not much real overhead besides the instantiation.  Seems like there could be a prettier way but it works and performance impact should be negligible.

Thanks.;;;","03/Jul/09 15:41;streamy;I'm +1 on committing this if others are cool with the design.  Planning to look further into what we can do with this specialization for HBASE-1521;;;","03/Jul/09 17:16;stack;This seems odd:

{code}
+    public CompactionReader(Reader reader) {
+      super(reader.istream, reader.fileSize, reader.cache);
+      super.blockIndex = reader.blockIndex;
+      super.trailer = reader.trailer;
+      super.lastkey = reader.lastkey;
+      super.avgKeyLen = reader.avgKeyLen;
+      super.avgValueLen = reader.avgValueLen;
+      super.comparator = reader.comparator;
+      super.metaIndex = reader.metaIndex;
+      super.fileInfoLoaded = reader.fileInfoLoaded;
+      super.compressAlgo = reader.compressAlgo;
+    }
{code}

Why not make a constructor that takes above args?

Too much of readBlock is duplicated in CompactionReader.  Would suggest changing backing readBlock so it makes call to a new method that does the add-to-cache... then subclass this new method only.

Otherwise, nice patch.;;;","03/Jul/09 17:47;streamy;Adds a new method:  HFile.Reader.cacheBlock(String blockName, ByteBuffer buf)

HFile.CompactionReader now just overrides that method and does nothing inside of it.;;;","03/Jul/09 18:41;stack;Patch looks good. Tested it with upload. Logs look fine. Committing.... (Rolled up lrublockcache stats logging into one log message instead of 3 as part of commit). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-1554 broke org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.testResizeBlockCache,HBASE-1591,12429141,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,30/Jun/09 01:04,13/Sep/09 22:24,01/Jul/25 07:49,30/Jun/09 02:43,0.20.0,,,,,0.20.0,,,,,,,0,"Modification of LRU heap size calculations altered some of the test expectations in the LRU test.

{noformat}
Error Message
expected:<0> but was:<1>

Stacktrace
junit.framework.AssertionFailedError: expected:<0> but was:<1>
	at org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.testResizeBlockCache(TestLruBlockCache.java:435)
{noformat}

Patch coming...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/09 02:01;streamy;HBASE-1591-v1.patch;https://issues.apache.org/jira/secure/attachment/12412117/HBASE-1591-v1.patch","30/Jun/09 02:19;streamy;HBASE-1591-v2.patch;https://issues.apache.org/jira/secure/attachment/12412120/HBASE-1591-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25846,Reviewed,,,,Tue Jun 30 02:43:03 UTC 2009,,,,,,,,,,"0|i0he5z:",99557,,,,,,,,,,,,,,,,,,,,,"30/Jun/09 01:36;streamy;This is turning into a bit more of a fix on the LRU's sizing.  Patch soon.;;;","30/Jun/09 02:01;streamy;Cleans up TestLruBlockCache a great deal.  I was doing a lot of hard-coding of block sizes, cache sizes, etc... Was an imperfect science and not very portable.

I made LruBlockCache only have heapSize, not a separate cacheSize.  The unit tests are also fully testing the heap size tracking.

Any further work for sizing of this will be in HBASE-1590 though I'm not sure how necessary that is anymore.;;;","30/Jun/09 02:02;streamy;Please review and verify TestHeapSize and TestLruBlockCache pass.  Will be helpful to know your platform (OS/JVM and 32bit or 64bit);;;","30/Jun/09 02:09;streamy;One of the TestHeapSize tests is failing on a development box.;;;","30/Jun/09 02:19;streamy;Fixes a problem with alignment of String calculation and cleans the patch a little more.

Tested on 32bit windows and 64bit linux.  Would like to get verification from a few others that tests pass before committing.;;;","30/Jun/09 02:43;stack;Tested two tests passed locally.  Committed.  Let hudson tell us if its fixed hudson or not.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store scanner does not consult filter.filterRow at end of scan,HBASE-1580,12428806,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,clint.morgan,clint.morgan,24/Jun/09 19:59,13/Sep/09 22:24,01/Jul/25 07:49,26/Jun/09 05:26,0.20.0,,,,,0.20.0,,,regionserver,,,,0,"I have impelemented a columnValueFilter (with new interface) that should filter out the last of two rows in a table. However, I notice that filterRow is only being called on the first row, and the second row is returned.

This patch fixes it, but needs review. My first attempt at adding the call in the DONE_SCAN case did not fix it, but still seems right. The second addition at the end of the method fixed it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/09 20:00;clint.morgan;1580.patch;https://issues.apache.org/jira/secure/attachment/12411691/1580.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25842,Reviewed,,,,Fri Jun 26 05:26:49 UTC 2009,,,,,,,,,,"0|i0he3j:",99546,,,,,,,,,,,,,,,,,,,,,"25/Jun/09 20:33;ryanobjc;+1 looks good.

It would be nice to add a test so we don't get any regressions in the future.;;;","26/Jun/09 02:14;streamy;Agreed.  Patch looks good, but let's get a test in.;;;","26/Jun/09 05:23;stack;This is kinda hard to test with out a filter that exploits it.  Let me add a test over in hbase-1582.  Meantime I'll commit this cos two +1s.;;;","26/Jun/09 05:26;stack;Thanks for patch Clint.  I'll work on 1582 since you're busy for a while.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TIF needs to be able to set scanner caching size for smaller row tables & performance,HBASE-1576,12428706,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,apurtell,ryanobjc,ryanobjc,23/Jun/09 21:00,20/Sep/12 22:10,01/Jul/25 07:49,24/Jun/09 06:02,0.20.0,,,,,0.20.0,,,,,,,0,"TIF goes with the default scanner caching size (1).  When each row is processed very fast and is small, this limits the overall performance.  By setting a higher scanner caching level you can achieve 100x+ the performance with the exact same map-reduce and table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/09 00:23;apurtell;HBASE-1576.patch;https://issues.apache.org/jira/secure/attachment/12411592/HBASE-1576.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25841,,,,,Wed Jun 24 06:02:57 UTC 2009,,,,,,,,,,"0|i0he2n:",99542,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 22:01;apurtell;The HBaseConfiguration object is created from the JobConf (TableInputFormat.java, line ~58), so isn't this sufficient?

{code}
JobConf job = new JobConf();
// ... 
job.set(""hbase.client.scanner.caching"", ""100"");
// ... 
{code}

No problem to make a convenience method, though...
;;;","24/Jun/09 00:29;apurtell;Should the default also be upped to something like 30 as before, or even 100? I mean the default as set by TableMapReduceUtil, not in hbase-default.xml. ;;;","24/Jun/09 00:36;jdcryans;bq. Should the default also be upped to something like 30 as before, or even 100? I mean the default as set by TableMapReduceUtil, not in hbase-default.xml.

Every time I had to change it that was for a MR job where I was doing heavy processing on each row. So -1 on that.;;;","24/Jun/09 01:52;apurtell;Ok, then the patch stands as is. ;;;","24/Jun/09 03:55;stack;+1 on patch.  Ryan.... this work for you?;;;","24/Jun/09 06:02;stack;Committed after +1 from Ryan up on IRC;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client and server APIs to do batch deletes.,HBASE-1574,12428692,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,23/Jun/09 18:51,20/Nov/15 13:02,01/Jul/25 07:49,15/Sep/09 20:30,0.20.0,,,,,0.20.1,0.90.0,,,,,,2,in 880 there is no way to do a batch delete (anymore?).  We should add one back in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/09 00:55;stack;batchdelete-v2.patch;https://issues.apache.org/jira/secure/attachment/12419595/batchdelete-v2.patch","14/Sep/09 23:48;stack;batchdelete.patch;https://issues.apache.org/jira/secure/attachment/12419585/batchdelete.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25839,,,,,Fri Nov 20 13:02:09 UTC 2015,,,,,,,,,,"0|i0he27:",99540,,,,,,,,,,,,,,,,,,,,,"24/Jun/09 17:56;erikholstad@gmail.com;How are you thinking this Ryan, that they should be buffered too, or just batched and sent over?;;;","30/Jun/09 23:29;stack;Moving to 0.21;;;","14/Sep/09 01:04;stack;Pulling into 0.20.1.  We need it (I won't up the RPC version when I add the client/server methods since its a pure addition).;;;","14/Sep/09 23:48;stack;Any chance of a review J-D?

This is not a pretty addition.  In particular, the code in HConnectionManager is effectively duplicated.  I tried to make a class to hold common code but it just made stuff worse; state is held in too many variables across the full breadth of the method.  I factored out what I could.  It would help if Put and Delete had an ancestor or even if it was just an interface such as Row with a getRow method in it or ComparableRow.

;;;","14/Sep/09 23:48;stack;Hang on.. had an idea.... back in a minute.;;;","15/Sep/09 00:22;erikholstad@gmail.com;I'm not against having a common base class or interface, like update or so. The reason that we didn't do that in the first place was the different way that we were using them, for example with buffering and so on, but this might change now when we are doing this multi delete stuff. ;;;","15/Sep/09 00:55;stack;This is a bit better.  The ""common"" interface is Row.. things that have a row can implement Row.  Row has a getRow method.  Thats it.  I had RowComparable as the interface but thought it better to leave the two concepts distinct in case we want Row without Comparable.

In this patch, the duplication of code is gone.  We instantiate a Batch private utility class to run the batching.

J-D (or Holstad), review please.

I'm not upping the RPC version  number because I want this to go into 0.20.1 (upping RPC version will make 0.20.0 incompatible with 0.20.1).  I figure its ok because this is straight addtion of new method.;;;","15/Sep/09 15:05;jdcryans;In HRS, why do you copied the code from the delete method instead of calling it in a for()?

In HTable, I think there's something missing in the comment. ""If exception, list will have ""

I like the refactoring of HCM and that it handles more than Put, that's the way to go. I think processBatchOfRows should have a new name, my goal when I wrote it was that it be the single method to handle any batches but now it's different.;;;","15/Sep/09 15:21;stack;The batch delete is different from delete, no?  Would be hard to refactor so they shared commonage.

Yeah, will fix the HTable comment on commit?

Yeah, processBatchOfRows is not a good name but I don't want to change it for this commit to 0.20 branch.  I could open new issue for 0.21 where we look at these batch operations -- including batch get -- and figure out better method namings deprecating the old?

One other thing, I was going to change Row to be package private (Any reaction to this new interface?   Do you think Row a good name?  Should it be HasRow or something?).

Thanks J-D.

St.Ack;;;","15/Sep/09 15:29;jdcryans;I think Row is good, the rest is ok. +1 that you fix on commit.;;;","15/Sep/09 17:43;erikholstad@gmail.com;Patch looks good. 

I agree that we might have to rethink the structure of that code as it looks today if we want to support all these batch actions, to make it in a proper way.
I would be good to know how people are using the batch methods to see what code we share between the different calls. ;;;","15/Sep/09 20:30;stack;Commited branch and trunk (Thanks for review J-D and Holstad).  I made Row package private and fixed javadoc on commit.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Zookeeper log4j property set to ERROR on default, same output when cluster working and not working",HBASE-1572,12428687,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,23/Jun/09 17:52,13/Sep/09 22:24,01/Jul/25 07:49,23/Jun/09 22:42,0.20.0,,,,,0.20.0,,,,,,,0,"I was having some issues with getting zookeeper running, default log level being at ERROR meant that I didn't actually know whether the zk cluster was working or not.  INFO is fairly noisy but might be best?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 22:19;streamy;HBASE-1572-v1.patch;https://issues.apache.org/jira/secure/attachment/12411577/HBASE-1572-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25837,Reviewed,,,,Tue Jun 23 22:42:13 UTC 2009,,,,,,,,,,"0|i0he1r:",99538,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 18:24;nitay;Yeah I turned this down a while ago to prevent all of their cruft messages from filling up our logs. If it's preventing useful information from coming through that every user should see, then so be it, I'm with you. ;;;","23/Jun/09 19:07;stack;Lets run w/ INFO while we're zk noobs.;;;","23/Jun/09 22:19;streamy;Changes zookeeper log4j property to INFO from ERROR.;;;","23/Jun/09 22:42;stack;Committed.  Thanks for patch Jon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rare race condition can take down a regionserver. ,HBASE-1569,12428635,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,ryanobjc,ryanobjc,23/Jun/09 07:42,13/Sep/09 22:24,01/Jul/25 07:49,25/Jun/09 23:57,0.20.0,,,,,0.20.0,,,,,,,0,"this happened after > 24 hours of heavy import load on my cluster.  Luckily the shutdown seemed to be clean:

java.lang.IllegalAccessError: Call open first
        at org.apache.hadoop.hbase.regionserver.StoreFile.getReader(StoreFile.java:356)
        at org.apache.hadoop.hbase.regionserver.Store.getStorefilesIndexSize(Store.java:1378)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doMetrics(HRegionServer.java:1075)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:454)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/09 05:00;stack;1569-v2.patch;https://issues.apache.org/jira/secure/attachment/12411603/1569-v2.patch","24/Jun/09 00:46;stack;sf.patch;https://issues.apache.org/jira/secure/attachment/12411596/sf.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25834,,,,,Thu Jun 25 23:57:37 UTC 2009,,,,,,,,,,"0|i0he13:",99535,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 08:18;ryanobjc;this is a race condition, here is how it happens:

doMetrics() calls getStorefilesIndexSize() which gets a view of the storefiles ConcurrentSkipListMap at some point in time.  Working on this snapshot it calls each store file in turn asking for the index size.

In another thread, the compaction completion code finishes, first thing it does is:
- remove store files from the storefiles list.
- do some stuff
- close the aforementioned store files, which causes the this.reader to become null.

Back in thread #1, we run into the this.reader == null, and we throw the exception.

So we need to do either of:
- sync on this map, use a synced versin of the map
- allow the ability to check this metrics without causing a RS abort when we hit an exception.  Either catch it, or prevent it from happening.;;;","23/Jun/09 23:32;stack;At first I thought that use of ConcurrentSkipListSet the problem but thinking on it more, rather, we need to make code tolerate fact that a file has been moved or removed.  Alternative is syncing around file operations till they complete which is too much to ask.

A good while a go, an issue in metrics got HRS stuck in an infinite loop.

Let me try hack up a patch.;;;","24/Jun/09 00:46;stack;Unfinished first attempt.;;;","24/Jun/09 05:00;stack;Patch does following:

+ Wraps metrics in a try/catch that catches any exception, logs it and then moves on rather than let it out and kill HRS
+ Changed getReader so it doesn't do IllegalArgumentException with ""must open first"" but instead lets out the null Reader.
+ Then, changed whereever we get a Reader to check for null.  If null log it so we can see extent of phenomeon but in general just keep going.;;;","25/Jun/09 09:49;larsgeorge;I ran into the same issue, killed the HRS that hosted the ROOT partition. The ""Catalog Tables"" tbale in the UI was empty afterwards and the running MR job was failing fast. I synced to trunk and applied this patch and restarted the cluster. With the above patch two pieces are failing to update but they were the removed bloomfilter variables and it looks like it was removed in trunk already so no harm done.

Will report if I run into the new log outputs or if anything else happens.;;;","25/Jun/09 22:54;ryanobjc;+1 lgtm;;;","25/Jun/09 23:57;stack;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client doesnt consult old row filter interface in filterSaysStop() - could result in NPE or excessive scanning,HBASE-1568,12428624,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,ryanobjc,ryanobjc,23/Jun/09 02:24,13/Sep/09 22:24,01/Jul/25 07:49,23/Jun/09 16:19,0.20.0,,,,,0.20.0,,,Filters,,,,0,"The implementation of HTable.ClientScanner.filterSaysStop() doesnt refer to the old filter, which could result in an NPE if you use an old-style filter.

It also ignores the old style filter, so if you want to use old filters only, you dont get the effect you need.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 02:24;ryanobjc;HBASE-1568.patch;https://issues.apache.org/jira/secure/attachment/12411484/HBASE-1568.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25833,Reviewed,,,,Tue Jun 23 16:19:48 UTC 2009,,,,,,,,,,"0|i0he0v:",99534,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 02:27;ryanobjc;testing on my cluster this seems to fix the issues i was seeing.

we should have a test :-);;;","23/Jun/09 16:19;stack;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cant serialize new filters: ,HBASE-1567,12428622,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,ryanobjc,ryanobjc,23/Jun/09 02:18,13/Sep/09 22:24,01/Jul/25 07:49,24/Jun/09 05:34,0.20.0,,,,,0.20.0,,,Filters,,,,0,"09/06/22 19:14:28 ERROR io.HbaseObjectWritable: Unsupported type interface org.apache.hadoop.hbase.filter.Filter
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: writeClassCode
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: writeObject
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: write
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: writeObject
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: write
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: sendParam
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: call

ooooopsy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 02:19;ryanobjc;HBASE-1567.patch;https://issues.apache.org/jira/secure/attachment/12411482/HBASE-1567.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25832,Reviewed,,,,Wed Jun 24 05:34:47 UTC 2009,,,,,,,,,,"0|i0he0n:",99533,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 02:19;ryanobjc;not tested;;;","24/Jun/09 05:34;stack;Committed after adding a serialization patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"using Scan(startRow,stopRow) will cause you to iterate the entire table",HBASE-1566,12428621,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,ryanobjc,ryanobjc,23/Jun/09 02:14,13/Sep/09 22:24,01/Jul/25 07:49,25/Jun/09 23:56,0.20.0,,,,,0.20.0,,,,,,,0,"Right now the only way for the client scanner to know that we are at the 'end' of a scan is to client-side-wise use the filter to figure this out. 

This is not easy to fix because the server is unable to indicate the difference between 'done with this region', and 'you're at the end of your scan'.  In both cases we return 0 results, and the client can't figure out what it means.

Right now the best solution is to use filters, which is tricky since there is no StopRowFilter because that functionality is built in :-)

We might have to hack the 'stop row' functionality as a filter until we can improve the client-server API/RPC.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/09 21:13;ryanobjc;HBASE-1566.patch;https://issues.apache.org/jira/secure/attachment/12411861/HBASE-1566.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25831,,,,,Thu Jun 25 21:15:43 UTC 2009,,,,,,,,,,"0|i0he0f:",99532,,,,,,,,,,,,,,,,,,,,,"23/Jun/09 02:17;ryanobjc;actually it doesnt _iterate_ the entire table, but it does touch every RS after the one your rows are hosted on, even if not necessary!;;;","25/Jun/09 20:38;ryanobjc;changing Scan to add a filter to handle the stop row.  Hacky and annoying.;;;","25/Jun/09 21:13;ryanobjc;fixes this on my set-up;;;","25/Jun/09 21:15;stack;+1

That'll do for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incrementColumnValue does not write to WAL,HBASE-1563,12428592,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,22/Jun/09 17:26,13/Sep/09 22:24,01/Jul/25 07:49,25/Jun/09 22:12,0.20.0,,,,,0.20.0,,,,,,,0,"Incrementing never writes to the WAL.  Under failure scenarios, you will lose all increments since the last flush.

Do we want to expose the option to the client as to whether to write to WAL or not?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/09 17:27;streamy;HBASE-1563-v1.patch;https://issues.apache.org/jira/secure/attachment/12411428/HBASE-1563-v1.patch","22/Jun/09 18:16;streamy;HBASE-1563-v2.patch;https://issues.apache.org/jira/secure/attachment/12411437/HBASE-1563-v2.patch","23/Jun/09 22:44;streamy;HBASE-1563-v3.patch;https://issues.apache.org/jira/secure/attachment/12411581/HBASE-1563-v3.patch","24/Jun/09 22:16;streamy;HBASE-1563-v4.patch;https://issues.apache.org/jira/secure/attachment/12411720/HBASE-1563-v4.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25829,Reviewed,,,,Thu Jun 25 22:12:55 UTC 2009,,,,,,,,,,"0|i0hdzz:",99530,Adds new API with flag to incrementColumnValue for whether or not to write WAL,,,,,,,,,,,,,,,,,,,,"22/Jun/09 17:27;streamy;This patch adds a writeToWAL argument and appends to the edit log on every increment, however the boolean is not exposed to the client in this patch and is default true. Wanted to discuss with others before going further.

Thoughts?;;;","22/Jun/09 17:36;apurtell;IMHO, it's fine to expose the boolean to the client but there should be a client side API alternative which does not expose the boolean which internally sets writeToWAL as true. Everybody will be happy. Let's just do it. ;;;","22/Jun/09 17:40;streamy;+1 patch coming...;;;","22/Jun/09 18:16;streamy;Please review.;;;","22/Jun/09 18:38;stack;Why not have old incrementColumnValue call the new one with a flag set to 'true'?;;;","22/Jun/09 18:39;apurtell;Code is fine.

This portion of the javadoc:

bq. If the column value isn't long-like, this could throw an exception.

I understand what you are saying, but I think this may just confuse users. Can you consider an alternate wording? ;;;","22/Jun/09 20:18;ryanobjc;2 things:
- in the client code you build a 'get' that isnt used.
- we are writing to the WAL _after_ the increment, this may be the only way to do it effectively, but we should also think about this carefully.;;;","22/Jun/09 21:31;erikholstad@gmail.com;@Ryan I think the Get is just some old leftover when it was used to be sent over that way and should now be removed.;;;","23/Jun/09 22:44;streamy;- Removes unnecessary Get in client code
- Removed duplicate server calls in the client code, default just calls full w/ true
- Replaced increment javadoc with ""Atomically increments a column value. If the column value already exists and is not a big-endian long, this could throw an exception.""  Is that more clear?

@Ryan I got most the way through splitting up Store.incrementColumnValue logic so we figure out what we have to do first, then write to wal, then perform the insert.  The issue is when we increment in-place in the memcache.  If we truly don't want to write until the wal append, we'd have to make a copy of the KV, perform the increment on it, and then either increment the original memcache value or swap.  This more or less negates the in-place increment optimization.  Thoughts?;;;","23/Jun/09 23:02;stack;Patch looks good except for the identified weirdness where we write to WAL after the update.  To do this properly you'd need to row lock and make a copy.  Is that too onerous a price to pay?;;;","23/Jun/09 23:19;stack;Ryan notes that old code did row lock and was fast.

So, we should go ahead and pull the KV from memcache, copy it, increment it, add to WAL, then insert back into memcache?  It won't be too slow?  Maybe only do this if write to wal is enabled?;;;","23/Jun/09 23:21;stack;Why is there a race if a row lock is outstanding Ryan?;;;","24/Jun/09 00:49;streamy;We are already getting a row lock, there is no race condition that I can see.

I'd really like the increment-in-place over making a new KV each time we increment.  The downside is if the regionserver dies in the brief window after the update and before the WAL append, we lose a single increment.  If it dies on either side, then nothing would change.

The upside is we don't allocate a new KV each time.  That's what I'm after with increments, doing them in place and not doing a new insertion each time (and new allocation).  I have an internal ID assignment system that uses custom patches on 0.19.  Was hoping to drop our custom patches and use this out of the box, but if we copy the KV each time I'll likely patch it to not do that :)  I'd rather not lose any increments but the window is very small and you lose at most a single increment.  If a KV is 100 bytes, and I increment just 1M times in a day, i've unnecessarily allocated 100MB (and the worst kind, lots of small 100 byte allocations that stick around for a bit).

We still satisfy the property of ""Once the increment has returned successfully, it is safe"".  This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).;;;","24/Jun/09 01:51;apurtell;bq. We still satisfy the property of ""Once the increment has returned successfully, it is safe"". This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).

This makes sense to me. ;;;","24/Jun/09 02:04;ryanobjc;a row lock is only for writing.  The race happens when someone is reading the value you are incrementing.  They could copy a partially updated view into the RPC buffer, and the user could see something odd and unexpected (values going down for example).

If we used a ConcurrentSkipListSet, we can't put a duplicate, which requires us to temporarily remove the KeyValue, then put it back, opens a hole whereby we can look for a value and it not be there.  

So to support not-in-place modification we need to also move to a ConcurrentSkipListMap in memcache.;;;","24/Jun/09 03:15;stack;/me man, i love OSS

Someone please review 1577 -- it converts memcache to CSLM from CSLS.;;;","24/Jun/09 05:37;stack;So, interesting, need to operate on copy so readers don't see partially changed KV.  That makes sense.;;;","24/Jun/09 05:51;streamy;For now, let's just make the copy and make sure things are safe.  In addition to 1577.  Good stuff ryan.

Stack, you've brought your improper usage of /me onto the web now!;;;","24/Jun/09 05:52;streamy;Will post patch tomorrow;;;","24/Jun/09 22:16;streamy;New patch that copies the existing KV instead of incrementing in-place.

This is now broken.  The CSLMap has the same behavior as the CSLSet.  Since the keys are considered ""equal"" the values might be updated in the map (in this case meaningless) but they don't swap the key with the new one (since it's equal).  It does not have the behavior we wanted, an atomic put that replaces the Key in case of collisions.;;;","25/Jun/09 13:33;irfanmohammed;Can we please ensure that the ""atomic"" puts are honored? 

I migrated lot of my code to use the incrementColumnValue() because of the atomic nature of this call and it really reduces the data footprint and also the query time when aggregating the cell values.;;;","25/Jun/09 19:37;streamy;@Irfan First and foremost, we need HBASE-1577, but that is looking good.  Beyond that, we need to think more about how to make this behave atomically.  In thinking about it now, I do see some potential issues/race conditions that will lead to missing increments (with HBASE-1577 things will always _work_ but does not ensure atomicity of the increments).  Need to think more.;;;","25/Jun/09 19:46;streamy;@Irfan  Excuse me... Got confused for a second.  Once HBASE-1577 goes in, these absolutely *will* have atomic behavior.  Each is guaranteed to have worked and performed the increment once it returns, and all readers will always see the latest value.;;;","25/Jun/09 22:12;stack;Thanks for the patch Jon;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to handle the setting of 32 bit versus 64 bit machines,HBASE-1562,12428588,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,erikholstad@gmail.com,erikholstad@gmail.com,erikholstad@gmail.com,22/Jun/09 16:51,13/Sep/09 22:24,01/Jul/25 07:49,24/Jun/09 20:03,0.20.0,,,,,0.20.0,,,,,,,0,"After adding the tests to verify the correctness of the HeapSize calculations the question of where to set the type of machines that are in the cluster arose.
I would think that most people are using 64 bit machines but we still need to support the use of 32 bit. So the way I see it we can solve this problem in two ways,
we can either have a settable parameter the the user sets when starting up the cluster or we can try to figure it out ourselves. I think that the second solution would
be the best, to make it as easy as possible on the user. 
That means that we need to add extra sizes to HeapSize and maybe even to Bytes.

Thoughts, comments?",,,,,,,,,,,,,,,,,,,,,,,HBASE-1554,,,,,,"24/Jun/09 17:50;erikholstad@gmail.com;HBASE-1562-v1.patch;https://issues.apache.org/jira/secure/attachment/12411676/HBASE-1562-v1.patch","24/Jun/09 18:42;erikholstad@gmail.com;HBASE-1562-v2.patch;https://issues.apache.org/jira/secure/attachment/12411684/HBASE-1562-v2.patch","24/Jun/09 19:48;streamy;HBASE-1562-v3.patch;https://issues.apache.org/jira/secure/attachment/12411689/HBASE-1562-v3.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25828,Reviewed,,,,Wed Jun 24 20:03:33 UTC 2009,,,,,,,,,,"0|i0hdzr:",99529,,,,,,,,,,,,,,,,,,,,,"22/Jun/09 17:06;stack;Ask the JVM.  It knows (see the head of log where we print out JVM info.  Maybe way to ask it explicitly in a non-vendor-specific way).;;;","22/Jun/09 17:09;stack;Hudson looks like its 64bit solaris:

{code}
Last login: Fri Mar 20 11:52:10 2009 from 63.203.238.117
Sun Microsystems Inc.   SunOS 5.10      Generic January 2005
-bash-3.00$ isainfo -kv
64-bit amd64 kernel modules
{code};;;","22/Jun/09 17:33;apurtell;I am seeing HeapSize/ClassSize related failures on a 64 bit platform also, in TestHFile:

{noformat}
2009-06-22 17:31:59,860 DEBUG [main] util.ClassSize(147): Primitives 8, arrays 3
, references(inlcuding 16, for object overhead) 3, refSize 4, size 72
------------- ---------------- ---------------

Testcase: testTFileFeatures took 0.359 sec
Testcase: testMetaBlocks took 0.088 sec
Testcase: testNullMetaBlocks took 0.05 sec
Testcase: testCompressionOrdinance took 0.027 sec
Testcase: testComparator took 0.046 sec
Testcase: testHeapSizeForBlockIndex took 0.245 sec
        FAILED
expected:<72> but was:<128>
junit.framework.AssertionFailedError: expected:<72> but was:<128>
        at org.apache.hadoop.hbase.io.hfile.TestHFile.testHeapSizeForBlockIndex(TestHFile.java:273)
{noformat}
;;;","22/Jun/09 17:51;erikholstad@gmail.com;@Stack will play with the System.getProperties(); to see what I can dig out, looks ok, but need to test on 32 bit machine.

@Purtell looking at the refSize value it looks like this is a 32 bit machine, so might be need to add some extra checks to the way the arc is figured out. ;;;","22/Jun/09 17:55;apurtell;@Holstad: Then your refsizes are wrong:

{noformat}
% uname -a
Linux BOA01.trendmicro.com 2.6.18-128.1.6.el5.centos.plusxen #1 SMP Thu Apr 2 13
:32:02 EDT 2009 x86_64 x86_64 x86_64 GNU/Linux

% java -version
java version ""1.6.0_13""
Java(TM) SE Runtime Environment (build 1.6.0_13-b03)
Java HotSpot(TM) 64-Bit Server VM (build 11.3-b02, mixed mode)
{noformat};;;","22/Jun/09 18:01;stack;Related, @Holstad, from a pointer posted over in hbase-1554 by Nitay, ""However, if you'd like to write code which is platform specific (shame on you),  the system property sun.arch.data.model has the value ""32"", ""64"", or ""unknown"".  ""  Also, do you want to relate that issue to this one?;;;","22/Jun/09 18:08;erikholstad@gmail.com;@Purtell
Can you put this in some testcode and see what you get?

Properties sysProps = System.getProperties();
System.out.println(""arc "" + sysProps.getProperty(""sun.arch.data.model""));

@Stack, haha yeah that is a great pointer. The reason that we are seeing some of these problems are the code taken from Derby used to decide the arc size, but
we can make this much better if we don't need to run gc and try to figure it out that way, just want to see what result Andrew gets.

;;;","22/Jun/09 18:33;apurtell;@Holstad: {{arc 64}};;;","22/Jun/09 18:39;erikholstad@gmail.com;@Purtell thanks, will use use that way to determine the arc structure instead of the old one from Derby.

Since we are going to not only have one set of Sizes to deal with but two, we cannot longer keep them in HeapSize, so I propose that they are moved into ClassSize and are
calculated after first finding out the arc size.;;;","24/Jun/09 17:50;erikholstad@gmail.com;Tested this in 32 and 64 bit JVM and it passes in both. Might have to sit down with Jon to make better test for the blockcache after hi is ready and with Ryan and look at Blockindex, since it is kinda hard to test as it is right now.;;;","24/Jun/09 18:42;erikholstad@gmail.com;Readded empty line.;;;","24/Jun/09 19:28;stack;Erik:

As written, does this all work?

I see that ClassSize only sets its DEFINEs for things like REFERENCE and INTEGER if you call reload.  Otherwise they are 0.  Reload won't be called on static loading of ClassSize so I'd expect that when you do somethign like this in HLogKey -- ClassSize.ARRAY -- that its going to get the 0 value, not the calculated value suited from 32bit or 64bit jvm.   Would suggest you move the bulk of the reload method into a static at head of ClassSize as in:

{code}
static {
  +    Properties sysProps = System.getProperties();
+    String arcModel = sysProps.getProperty(""sun.arch.data.model"");
+    
+    //Default value is set to 8, covering the case when arcModel is unknown
+    REFERENCE = 8;
+    if (arcModel.equals(THIRTY_TWO)) {
+      REFERENCE = 4;
     }
+    
+    ARRAY = 3 * REFERENCE;
 
-    refSize = ( 4 > sz) ? 4 : sz;
-    minObjectSize = 4*refSize;
+    ARRAYLIST = align(OBJECT + REFERENCE + Bytes.SIZEOF_INT + 
+        align(Bytes.SIZEOF_INT));
+    
+    BYTE_BUFFER = align(OBJECT + REFERENCE + Bytes.SIZEOF_INT + 
+        3 * Bytes.SIZEOF_BOOLEAN + 4 * Bytes.SIZEOF_INT + Bytes.SIZEOF_LONG); 
+    
+    INTEGER = align(OBJECT + Bytes.SIZEOF_INT);
+    
+    MAP_ENTRY = align(OBJECT + 5 * REFERENCE + Bytes.SIZEOF_BOOLEAN);
+    
+    OBJECT = 2 * REFERENCE;
+    
+    TREEMAP = align(OBJECT + 2 * Bytes.SIZEOF_INT + (5+2) * REFERENCE + 
+        ClassSize.align(OBJECT + Bytes.SIZEOF_INT));
+    
+    STRING = align(OBJECT + REFERENCE + 3 * Bytes.SIZEOF_INT);
}
{code};;;","24/Jun/09 19:48;streamy;Erik is out of the office for the afternoon, so I'm filling in to get this patch in (holding up block cache development)....

As stack suggested, patch changes from requiring instantiation or a call to reload() to static {}

Otherwise, I reviewed erik's patch and looks good.  Tests pass.;;;","24/Jun/09 20:03;stack;Looks good.  Committed.  Thanks Erik (and Jon).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable Mismatch between javadoc and what it actually does,HBASE-1561,12428538,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,22/Jun/09 07:50,13/Sep/09 22:24,01/Jul/25 07:49,22/Jun/09 17:23,0.20.0,,,,,0.20.0,,,,,,,0,"The code is:
  /** 
   * Delete all cells that match the passed row and column and whose
   * timestamp is equal-to or older than the passed timestamp, using an
   * existing row lock.
   * @param row Row to update
   * @param column name of column whose value is to be deleted
   * @param ts Delete all cells of the same timestamp or older.
   * @param rl Existing row lock
   * @throws IOException 
   * @deprecated As of hbase 0.20.0, replaced by {@link #delete(Delete)}
   */
  public void deleteAll(final byte [] row, final byte [] column, final long ts,
      final RowLock rl)
  throws IOException {
    Delete d = new Delete(row, ts, rl);
    d.deleteColumn(column);
    delete(d);
  }


The code should call deleteColumns() instead.
  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/09 16:46;streamy;HBASE-1561-v2.patch;https://issues.apache.org/jira/secure/attachment/12411425/HBASE-1561-v2.patch","22/Jun/09 07:50;ryanobjc;HBASE-1561.patch;https://issues.apache.org/jira/secure/attachment/12411375/HBASE-1561.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25827,Reviewed,,,,Mon Jun 22 17:23:31 UTC 2009,,,,,,,,,,"0|i0hdzj:",99528,,,,,,,,,,,,,,,,,,,,,"22/Jun/09 16:27;streamy;Reviewing this patch, I came across some more issues in HTable related to deletes.  Patch soon.;;;","22/Jun/09 16:46;streamy;Adds more explicit documentation in Delete javadoc.  Timestamp set in constructor is only for a delete row, you have to specify individually if you do explicit families/columns.

Also updates old api in HTable to use new api properly.  Adds javadoc to Delete.deleteColumns and adds timestamp to it.

I also copied over a private method stripColon() from HCD so that the old api is indifferent as to whether it is passed ""family:"" or ""family"".  As it is before this patch, old code would be sending family: but our new API expects it without that.;;;","22/Jun/09 17:23;stack;Committed.  Fixed a Get and a filter packge-info warning message at same time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deletes use 'HConstants.LATEST_TIMESTAMP' but no one translates that into 'now',HBASE-1558,12428499,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,ryanobjc,ryanobjc,21/Jun/09 03:42,13/Sep/09 22:24,01/Jul/25 07:49,22/Jun/09 18:10,0.20.0,,,,,0.20.0,,,Client,regionserver,,,0,Deletes don't update MAX_TIMESTAMP -> now like puts do.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/09 07:22;ryanobjc;HBASE-1558-v2.patch;https://issues.apache.org/jira/secure/attachment/12411335/HBASE-1558-v2.patch","21/Jun/09 04:20;ryanobjc;HBASE-1558.patch;https://issues.apache.org/jira/secure/attachment/12411332/HBASE-1558.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25824,Reviewed,,,,Mon Jun 22 18:10:11 UTC 2009,,,,,,,,,,"0|i0hdyv:",99525,,,,,,,,,,,,,,,,,,,,,"21/Jun/09 04:03;ryanobjc;it needs to be done in HRegion so we can put the proper TS into the WAL.;;;","21/Jun/09 04:20;ryanobjc;here's a prototype fix, but we need tests.;;;","21/Jun/09 07:22;ryanobjc;Here is a version with tests;;;","21/Jun/09 17:19;apurtell;+1, patch v2. ;;;","22/Jun/09 18:10;stack;Thanks Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassSize missing in trunk,HBASE-1553,12428424,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,erikholstad@gmail.com,streamy,streamy,19/Jun/09 22:22,13/Sep/09 22:24,01/Jul/25 07:49,19/Jun/09 22:42,0.20.0,,,,,0.20.0,,,util,,,,0,Patch for HBASE-1387 went in without ClassSize.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 22:27;erikholstad@gmail.com;HBASE-1553-v1.patch;https://issues.apache.org/jira/secure/attachment/12411279/HBASE-1553-v1.patch","19/Jun/09 22:38;erikholstad@gmail.com;HBASE-1553-v2.patch;https://issues.apache.org/jira/secure/attachment/12411280/HBASE-1553-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25820,Reviewed,,,,Mon Jun 22 17:58:28 UTC 2009,,,,,,,,,,"0|i0hdxj:",99519,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 22:27;erikholstad@gmail.com;Patch for missing ClassSize.;;;","19/Jun/09 22:38;erikholstad@gmail.com;Added the missing testfile;;;","19/Jun/09 22:39;streamy;Applies cleanly, patch looks good, tests pass.

+1 for commit;;;","19/Jun/09 22:42;apurtell;+1 committed. New testcase passes.;;;","22/Jun/09 17:58;stack;Sorry for trouble caused.  Thanks for taking care of it lads.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
atomicIncrement doesnt increase hregion.memcacheSize,HBASE-1547,12428333,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,19/Jun/09 08:26,13/Sep/09 22:24,01/Jul/25 07:49,19/Jun/09 22:15,0.20.0,,,,,0.20.0,,,,,,,0,this prevents flushing!  ,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1557,,,,"19/Jun/09 08:56;ryanobjc;HBASE-1547.patch;https://issues.apache.org/jira/secure/attachment/12411202/HBASE-1547.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25818,Reviewed,,,,Mon Jun 22 19:54:53 UTC 2009,,,,,,,,,,"0|i0hdw7:",99513,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 15:39;apurtell;Minor point, instead of new ValueAndSize type, why not just return and use the KeyValue?

+1
;;;","19/Jun/09 17:12;erikholstad@gmail.com;@Andrew, so you would return the KeyValue and check at the region if kv.getValue() is the same as amount?;;;","19/Jun/09 19:17;stack;atomic increment doesn't increase size of memcache so this issue is a little odd

if you want the value persisted, perhaps flush every n requests rather than via an artificial manipulation of size?

ValueAndSize seems odd too.  When would size ever be other than a long?  Can't it be a define for atomicIncrement?;;;","19/Jun/09 22:15;apurtell;Committed. ;;;","20/Jun/09 01:29;ryanobjc;stack, that is not entirely correct:

- when incrementing a value that never existed, we add to memcache
- when incrementing a value that was in store files only, we add to memcache

the code doesnt increase the size of memcache when we do the in-place modifications.

I think there are some race conditions in the existing implementation of incrementValue, we need to talk more.;;;","22/Jun/09 19:54;stack;@ryan OK.  I looked at patch again and see how I misread.  Do we need to talk more about race conditions?  Should we open new issue to cover race conditions?

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
atomicIncrements creating new values with Long.MAX_VALUE,HBASE-1545,12428329,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,19/Jun/09 07:50,13/Sep/09 22:24,01/Jul/25 07:49,19/Jun/09 22:02,0.20.0,,,,,0.20.0,,,,,,,0,"Atomic increment is creating new key values with timestamp of Long.MAX_VALUE.  This is not good, makes it hard to do range queries (as most of Thrift queries are).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 07:51;ryanobjc;HBASE-1545.patch;https://issues.apache.org/jira/secure/attachment/12411194/HBASE-1545.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25816,Reviewed,,,,Fri Jun 19 22:02:23 UTC 2009,,,,,,,,,,"0|i0hdvr:",99511,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 15:34;apurtell;+1;;;","19/Jun/09 19:18;stack;+1;;;","19/Jun/09 22:02;stack;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan returns rows beyond the endRow when the column is specified,HBASE-1542,12428291,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,irfanmohammed,irfanmohammed,18/Jun/09 20:11,13/Sep/09 22:24,01/Jul/25 07:49,19/Jun/09 05:13,0.20.0,,,,,0.20.0,,,Client,,,,0,"We ran into an issue where the scan resulted in rows beyond the endRow. Are we doing something incorrectly here? The test case is given below. When the scan.addColumn(...) is commented, the rows has { ""row333"" } but having the scan.addColumn(...) in the scan gives rows { ""row555"" }.
","hbase-0.20.0-dev
ubuntu 9.04
jdk 6.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/09 20:14;irfanmohammed;HBaseScanBugTest.java;https://issues.apache.org/jira/secure/attachment/12411131/HBaseScanBugTest.java","18/Jun/09 20:14;irfanmohammed;hbase_jira_output.log;https://issues.apache.org/jira/secure/attachment/12411132/hbase_jira_output.log",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25814,,,,,Fri Jun 19 05:13:36 UTC 2009,,,,,,,,,,"0|i0hdv3:",99508,,,,,,,,,,,,,,,,,,,,,"18/Jun/09 20:14;irfanmohammed;the junit test case for documenting the bug. ouput is also attached.;;;","18/Jun/09 23:39;ryanobjc;Hi, thanks for the detailed report.  The unit test seems to rely on some package local variables not sent along.  In the mean time I wrote what I think is an equivalent test base inside the HRegion test, which does not spin up a cluster (so maybe it's not accurate?).  The test is:

{noformat}

+  public void testScanner_StopRow1542() throws IOException {
+    byte [] tableName = Bytes.toBytes(""test_table"");
+    byte [] family = Bytes.toBytes(""testFamily"");
+    initHRegion(tableName, getName(), family);
+
+    byte [] row1 = Bytes.toBytes(""row111"");
+    byte [] row2 = Bytes.toBytes(""row222"");
+    byte [] row3 = Bytes.toBytes(""row333"");
+    byte [] row4 = Bytes.toBytes(""row444"");
+    byte [] row5 = Bytes.toBytes(""row555"");
+
+    byte [] col1 = Bytes.toBytes(""Pub111"");
+    byte [] col2 = Bytes.toBytes(""Pub222"");
+
+
+
+    Put put = new Put(row1);
+    put.add(family, col1, Bytes.toBytes(10L));
+    region.put(put);
+
+    put = new Put(row2);
+    put.add(family, col1, Bytes.toBytes(15L));
+    region.put(put);
+
+    put = new Put(row3);
+    put.add(family, col2, Bytes.toBytes(20L));
+    region.put(put);
+
+    put = new Put(row4);
+    put.add(family, col2, Bytes.toBytes(30L));
+    region.put(put);
+
+    put = new Put(row5);
+    put.add(family, col1, Bytes.toBytes(40L));
+    region.put(put);
+
+    Scan scan = new Scan(row3, row4);
+    scan.setMaxVersions();
+    scan.addColumn(family, col1);
+    InternalScanner s = region.getScanner(scan);
+
+    List<KeyValue> results = new ArrayList<KeyValue>();
+    assertEquals(true, s.next(results));
+    assertEquals(0, results.size());
+  }
{noformat}

Post HBASE-1541, this test case passes.  

Could you update to latest trunk (at least r786259) and try your test again?  Hopefully it passes now!

Thanks again!;;;","19/Jun/09 05:12;irfanmohammed;Took the latest update and the test passes. Thanks for the quick turn around.;;;","19/Jun/09 05:13;irfanmohammed;Tested with the latest code and the rows scan work correctly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scanning multiple column families in the presence of deleted families results in bad scans,HBASE-1541,12428207,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,18/Jun/09 06:10,13/Sep/09 22:24,01/Jul/25 07:49,18/Jun/09 23:17,0.20.0,,,,,0.20.0,,,,,,,0,"This unit test fails:

{noformat}
  public void testScanner_DeleteOneFamilyNotAnother() throws IOException {
    byte [] tableName = Bytes.toBytes(""test_table"");
    byte [] fam1 = Bytes.toBytes(""columnA"");
    byte [] fam2 = Bytes.toBytes(""columnB"");
    initHRegion(tableName, getName(), fam1, fam2);

    byte [] rowA = Bytes.toBytes(""rowA"");
    byte [] rowB = Bytes.toBytes(""rowB"");

    byte [] value = Bytes.toBytes(""value"");

    Delete delete = new Delete(rowA);
    delete.deleteFamily(fam1);

    region.delete(delete, null, true);

    // now create data.
    Put put = new Put(rowA);
    put.add(fam2, null, value);
    region.put(put);

    put = new Put(rowB);
    put.add(fam1, null, value);
    put.add(fam2, null, value);
    region.put(put);

    Scan scan = new Scan();
    scan.addFamily(fam1).addFamily(fam2);
    InternalScanner s = region.getScanner(scan);
    List<KeyValue> results = new ArrayList<KeyValue>();
    s.next(results);
    assertTrue(Bytes.equals(rowA, results.get(0).getRow()));

    results.clear();
    s.next(results);
    assertTrue(Bytes.equals(rowB, results.get(0).getRow()));

  }
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/09 07:32;ryanobjc;HBASE-1541-1488-v1.patch;https://issues.apache.org/jira/secure/attachment/12411043/HBASE-1541-1488-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25813,,,,,Thu Jun 18 23:17:10 UTC 2009,,,,,,,,,,"0|i0hduv:",99507,,,,,,,,,,,,,,,,,,,,,"18/Jun/09 07:32;ryanobjc;Fixes Thrift server tests (1488) as well as this issue.;;;","18/Jun/09 20:43;stack;All tests passed for me (except for a zk failure which seem to get on my local host).;;;","18/Jun/09 23:17;ryanobjc;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Controlled crash of regionserver not hosting meta/root leaves master in spinning state, regions not reassigned",HBASE-1536,12428173,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,streamy,streamy,17/Jun/09 20:56,13/Sep/09 22:24,01/Jul/25 07:49,17/Jun/09 22:28,0.20.0,,,,,0.20.0,,,master,,,,0,Testing for HBASE-867 uncovered some nastiness introduced from HBASE-1304 when a regionserver goes down.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/09 20:58;ryanobjc;HBASE-1536.patch;https://issues.apache.org/jira/secure/attachment/12410976/HBASE-1536.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25812,,,,,Wed Jun 17 22:28:07 UTC 2009,,,,,,,,,,"0|i0hdtr:",99502,,,,,,,,,,,,,,,,,,,,,"17/Jun/09 22:07;streamy;Tested.  +1 on commit;;;","17/Jun/09 22:07;streamy;Thanks Ryan!;;;","17/Jun/09 22:09;apurtell;+1;;;","17/Jun/09 22:28;streamy;Closing, already committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable.incrementColumnValue hangs() ,HBASE-1525,12427931,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,irfanmohammed,irfanmohammed,15/Jun/09 19:20,13/Sep/09 22:24,01/Jul/25 07:49,15/Jun/09 20:18,0.20.0,,,,,0.20.0,,,regionserver,,,,0,"In the following code, 

   @Test
    public void usingIncrement() throws Exception 
    {
        long siteId = 1234;
        long publisherId = 5678;
        Date eventTime = DATE_INPUT_FORMAT.parse(""2009-06-15 13:08:54"");
        
        long[] metrics = new long[] { 10, 22, 32 };
        
        byte[] rowKey = Bytes.toBytes(siteId + ""_"" + ROW_KEY_FORMAT.format(eventTime));
        byte[] family = Bytes.toBytes(FAMILY_PUBLISHER);
        byte[] qualifier = Bytes.toBytes(publisherId);
        
        HTable table = getTable();
        
        for (int i1 = 0, n1 = metrics.length; n1 > 0; i1++, n1--) {
            LOGGER.info(""processing [{0}] ..."", i1);
            table.incrementColumnValue(rowKey, family, qualifier, metrics[i1]);
            LOGGER.info(""processing [{0}] completed"", i1);
        }
        
        table.close();

        queryMetrics(table, siteId, publisherId, eventTime);
    }

The call table.incrementColumnValue hangs. Have to kill the hbase client and the master processes to get around the problem.","ubuntu 9.04
jdk 1.6.0.13
hadoop 0.20.0
hbase 0.20.0-dev [trunk]",,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/09 19:24;irfanmohammed;1525.patch;https://issues.apache.org/jira/secure/attachment/12410701/1525.patch","15/Jun/09 20:06;ryanobjc;HBASE-1525-v2.patch;https://issues.apache.org/jira/secure/attachment/12410704/HBASE-1525-v2.patch","15/Jun/09 19:21;irfanmohammed;client_dump.out;https://issues.apache.org/jira/secure/attachment/12410700/client_dump.out","15/Jun/09 19:21;irfanmohammed;hbase-irfan-master-damascus.out;https://issues.apache.org/jira/secure/attachment/12410699/hbase-irfan-master-damascus.out",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25804,,,,,Mon Jun 15 20:18:38 UTC 2009,,,,,,,,,,"0|i0hdrj:",99492,,,,,,,,,,,,,,,,,,,,,"15/Jun/09 19:21;irfanmohammed;Attached the client and the master thread dumps.;;;","15/Jun/09 19:22;irfanmohammed;Check for the non-null condition in the finally block when releasing the rowLock.;;;","15/Jun/09 19:24;irfanmohammed;check for the non-null in the finally block before releasing the row lock.;;;","15/Jun/09 19:40;apparition;ah.. yeah... good find!;;;","15/Jun/09 20:06;ryanobjc;Looks like the row lock is being acquired and never released.  Refactoring lossage probably.

Here is a slightly revised patch (use -p1) to always release the row lock.  There is some confusion because most of the other functions are like so:

public void doSomething(..., Integer lockid) {

// stuff

} finally {
  if (lockid == null)
     releaseLock(lid);
}
}

this looks weird, but what is really happening is we are unlocking the rowlock if we are _not_ passed a row lock (lockid == null), otherwise we leave the row locked and trust our caller to unlock it.;;;","15/Jun/09 20:08;stack;+1 on patch;;;","15/Jun/09 20:18;ryanobjc;thanks for the bug report!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in BaseScanner,HBASE-1523,12427854,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,15/Jun/09 06:14,13/Sep/09 22:24,01/Jul/25 07:49,15/Jun/09 21:23,0.20.0,,,,,0.20.0,,,,,,,0,"ever since HBASE-1522 I get this in master:

2009-06-14 23:09:33,043 ERROR org.apache.hadoop.hbase.master.BaseScanner: Unexpected exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:74)
        at org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:118)
        at org.apache.hadoop.hbase.master.BaseScanner.hasReferences(BaseScanner.java:300)
        at org.apache.hadoop.hbase.master.BaseScanner.cleanupSplits(BaseScanner.java:267)
        at org.apache.hadoop.hbase.master.BaseScanner.scanRegion(BaseScanner.java:229)
        at org.apache.hadoop.hbase.master.MetaScanner.scanOneMetaRegion(MetaScanner.java:73)
        at org.apache.hadoop.hbase.master.MetaScanner.maintenanceScan(MetaScanner.java:129)
        at org.apache.hadoop.hbase.master.BaseScanner.chore(BaseScanner.java:136)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:68)


preventing ROOT/etc from getting assigned. ouch!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/09 06:43;ryanobjc;HBASE-1523.patch;https://issues.apache.org/jira/secure/attachment/12410625/HBASE-1523.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25802,Reviewed,,,,Mon Jun 15 21:23:30 UTC 2009,,,,,,,,,,"0|i0hdr3:",99490,,,,,,,,,,,,,,,,,,,,,"15/Jun/09 21:23;stack;Ryan committed this a while ago:

{code}
------------------------------------------------------------------------
r784670 | rawson | 2009-06-14 23:44:08 -0700 (Sun, 14 Jun 2009) | 3 lines

HBASE-1523 - NPE in BaseScanner


durruti:cleantrunk stack$ svn diff -r782314:784670 src/java/org/apache/hadoop/hbase/master/BaseScanner.java 
Index: src/java/org/apache/hadoop/hbase/master/BaseScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/BaseScanner.java    (revision 782314)
+++ src/java/org/apache/hadoop/hbase/master/BaseScanner.java    (revision 784670)
@@ -298,7 +298,7 @@
   throws IOException {
     boolean result = false;
     HRegionInfo split =
-      Writables.getHRegionInfo(rowContent.getValue(splitFamily, splitQualifier));
+      Writables.getHRegionInfoOrNull(rowContent.getValue(splitFamily, splitQualifier));
     if (split == null) {
       return result;
     }
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StoreFileScanner catches and ignore IOExceptions from HFile,HBASE-1520,12427832,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,14/Jun/09 10:45,13/Sep/09 22:24,01/Jul/25 07:49,15/Jun/09 05:46,0.20.0,,,,,0.20.0,,,,,,,0,"In 'next' we are catching and ignoring IOExceptions - this is masking when we are having HDFS issues.  We should throw the exception.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/09 10:47;ryanobjc;HBASE-1520.patch;https://issues.apache.org/jira/secure/attachment/12410568/HBASE-1520.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25800,,,,,Mon Jun 15 02:24:59 UTC 2009,,,,,,,,,,"0|i0hdqf:",99487,,,,,,,,,,,,,,,,,,,,,"14/Jun/09 10:47;ryanobjc;this helps. ;;;","15/Jun/09 02:24;stack;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Delete Trackers using compareRow, should just use raw binary comparator",HBASE-1518,12427824,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,13/Jun/09 23:51,13/Sep/09 22:24,01/Jul/25 07:49,14/Jun/09 03:17,0.20.0,,,,,0.20.0,,,,,,,0,"Doesn't matter when using a normal table, but this is using a special comparator on _columns_ for the catalog tables.  Replace comparator.compareRows with Bytes.compareTo",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/09 23:54;streamy;HBASE-1518-v1.patch;https://issues.apache.org/jira/secure/attachment/12410557/HBASE-1518-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25798,,,,,Sun Jun 14 03:17:51 UTC 2009,,,,,,,,,,"0|i0hdpz:",99485,,,,,,,,,,,,,,,,,,,,,"13/Jun/09 23:54;streamy;As described in issue.;;;","14/Jun/09 02:59;stack;+1 on patch (one test needs to be brought into alignment with new constructor API).  I'll commit soon.  Doesn't fix the weird thing I'm seeing over in hbase-1495 though.;;;","14/Jun/09 03:17;stack;Committed.  Thanks for patch jgray.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate if StoreScanner will not return the next row if earlied-out of previous row,HBASE-1516,12427821,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,13/Jun/09 23:01,13/Sep/09 22:24,01/Jul/25 07:49,15/Jun/09 05:46,0.20.0,,,,,0.20.0,,,regionserver,,,,0,"StoreScanner line 122 states:

    // this wont get us the next row if the previous round hasn't iterated
    // past all the cols from the previous row. Potential bug!

Investigate whether this is a bug or not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/09 23:32;streamy;HBASE-1516-v1.patch;https://issues.apache.org/jira/secure/attachment/12410556/HBASE-1516-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25797,,,,,Mon Jun 15 02:23:33 UTC 2009,,,,,,,,,,"0|i0hdpj:",99483,,,,,,,,,,,,,,,,,,,,,"13/Jun/09 23:14;streamy;Looks like this is not a bug because the StoreScanner doesn't use the DONE keyword unless you've reached the end.  Rather, it uses SEEK_NEXT_ROW to ensure that it is always seeked down to the next row.

This is correct behavior, but also potentially inefficient.  Need to implement shortcuts in HFile to allow skipping of blocks when available.  Opened HBASE-1517;;;","13/Jun/09 23:32;streamy;Removes the comment.;;;","15/Jun/09 02:19;ryanobjc;+1 I'll commit ;;;","15/Jun/09 02:23;stack;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compactions too slow,HBASE-1513,12427779,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,viper799,viper799,12/Jun/09 19:42,13/Sep/09 22:24,01/Jul/25 07:49,15/Jun/09 05:46,0.20.0,,,,,0.20.0,,,regionserver,,,,0," know there was some changes to compaction a few days back I just got back 
to where I could run imports with the new client api
and noticing that compactions are vary slow I meand 100x+ slower then they 
used to be I thought one of the servers compaction thread had died
but after retesting and ruleing that out. 
I tested with and without compression and with and with out 
blockcache and all are running about the same

I got one small compaction to complete it took 1977 secs to compact 48.56MB
thats about 25kb sec
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/09 22:48;ryanobjc;HBASE-1513-v2.patch;https://issues.apache.org/jira/secure/attachment/12410553/HBASE-1513-v2.patch","14/Jun/09 21:52;ryanobjc;HBASE-1513-v3.patch;https://issues.apache.org/jira/secure/attachment/12410605/HBASE-1513-v3.patch","14/Jun/09 22:18;ryanobjc;HBASE-1513-v4.patch;https://issues.apache.org/jira/secure/attachment/12410607/HBASE-1513-v4.patch","13/Jun/09 08:55;ryanobjc;HBASE-1513.patch;https://issues.apache.org/jira/secure/attachment/12410536/HBASE-1513.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25796,,,,,Wed Jun 17 06:52:19 UTC 2009,,,,,,,,,,"0|i0hdov:",99480,,,,,,,,,,,,,,,,,,,,,"12/Jun/09 19:55;stack;Yes... something is going on.  Thanks for filing issue Billy.;;;","13/Jun/09 08:55;ryanobjc;looks like the logic to do the minor compaction is too expensive during minor compactions.

So I changed the minor compaction to offer an ability to just quickly iterate through a set of files.  It's REALLY fast now!;;;","13/Jun/09 08:57;ryanobjc;minor compactor previously was too complex and did too much logic.  new one kicks ass.

major compactions will be slow however (of course).;;;","13/Jun/09 14:18;viper799;testing minor compaction 
is major compaction going to be at the speed I seen above?
if so we still going to have major problems when you start talking about 256mb regions
256*1024/25kb/sec = 2.9 hours per region
that means running only major compactions we will only be able to major compact 8 regions a day not good for someone with 100 or so regions per server.

Billy
;;;","13/Jun/09 14:22;viper799;+1 on minor compaction
1mins, 17sec for 103.3MB compaction = 1.34mb/s

What has changed to slow down the old way of compaction all the logic test should be the same or similar? 
;;;","13/Jun/09 16:35;erikholstad@gmail.com;@Ryan
What does the new minor compaction do and what does it not do? I.e what did you remove?;;;","13/Jun/09 22:48;ryanobjc;the previous patch removed delete tracking, which was bad, and this one adds it back in.

On my cluster I can compact 70mb in 7s.

There may be a follow up bug 'major compaction too slow'.;;;","13/Jun/09 23:34;viper799;second patch will not apply to trunk

{code}
patching file src/java/org/apache/hadoop/hbase/regionserver/Store.java
Hunk #1 FAILED at 829.
1 out of 1 hunk FAILED -- saving rejects to file src/java/org/apache/hadoop/hbase/regionserver/Store.java.rej
{code}

My compaction was with compression so might be much faster without.
;;;","14/Jun/09 15:42;streamy;Billy, the codepath is very new for compactions in 0.20, not the same at all.  However, it should be able to be faster with the new stuff rather than slower we just need to spend more time optimizing.

Opened HBASE-1521 for further optimizations (marked for 0.20.1 for now), this issue really about ""fixing"" slowness introduced.;;;","14/Jun/09 21:52;ryanobjc;this one should apply correctly;;;","14/Jun/09 22:18;ryanobjc;this one has the try/finally;;;","15/Jun/09 02:22;stack;+1 on v4 (after some back and forth with Ryan on IRC);;;","17/Jun/09 06:52;ryanobjc;im seeing major compact speeds at about 10+ minutes per region.  the situation is not as dire as indicated, but we could probably be faster.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Shell ""close_region"" reveals a Master<>HRS problem, regions are not reassigned",HBASE-1508,12427530,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,larsgeorge,larsgeorge,10/Jun/09 08:26,13/Sep/09 22:24,01/Jul/25 07:49,22/Jun/09 23:39,0.20.0,,,,,0.20.0,,,master,,,,0,"When issuing a ""close_region"" on the shell the Master logs these entries:

{code}
...
2009-06-09 22:11:31,141 DEBUG org.apache.hadoop.hbase.master.RegionManager: Applying operation in tasklists to region
2009-06-09 22:11:33,557 DEBUG org.apache.hadoop.hbase.master.HMaster: Attempting to close region: TestTable,0000291328,1244572849139
2009-06-09 22:11:33,560 INFO org.apache.hadoop.hbase.master.HMaster: Marking TestTable,0000291328,1244572849139 as closed on 192.168.2.103:63745; cleaning SERVER + STARTCODE; master will tell regionserver to close region on next heartbeat
2009-06-09 22:11:34,156 DEBUG org.apache.hadoop.hbase.master.RegionManager: Applying operation in tasklists to region
...
{code}

But that is it, no further processing is done. The regions stays closed, and even across a restart it stays closed. 

According to what I got told the region should be automatically reassigned to a new server. Please confirm that this is what is expected. If not and the above seems right, then please disregard and close issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/09 23:01;stack;1508.patch;https://issues.apache.org/jira/secure/attachment/12411472/1508.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25794,Reviewed,,,,Mon Jun 22 23:39:26 UTC 2009,,,,,,,,,,"0|i0hdnz:",99476,,,,,,,,,,,,,,,,,,,,,"10/Jun/09 08:29;larsgeorge;More importantly, the heartbeat and the HRS closing the region is not logged anywhere. Btw, this is using the local standalone HBase and the TestTable created by the PerformanceTest class as per HBASE-1363.

So what happens is that the table does not have the closed regions anymore in its region list, but when looking at the regions hosted by the one HRS running they are listed. This seems wrong?;;;","20/Jun/09 17:46;apurtell;A workaround from the shell is 'enable <table>' after issuing the close_region command. This causes the master to discover the offline region and attempt to reassign. ;;;","20/Jun/09 17:48;apurtell;Even a hint of region reassignment trouble should raise priority to Blocker. ;;;","21/Jun/09 07:24;ryanobjc;the problem might be that the region is being marked as closed in meta and thus never eligible for reassignment?;;;","21/Jun/09 07:37;ryanobjc;HBASE-1558 will cause troubles in this area.  It might be the root cause in fact...

Since the Master relies on the HRS to report it, if it never does we'll never really close it... but assuming that the region got closed then opened again, 1558 will cause problems with scanners seeing the new region assignment since the delete will be masking the put.

Code inspecting doesnt seem to suggest there is anything else going on, so can you try this again with latest trunk?;;;","22/Jun/09 22:59;stack;I broke close_region with this commit:

{code}
------------------------------------------------------------------------
r773168 | stack | 2009-05-09 06:08:58 +0000 (Sat, 09 May 2009) | 1 line

HBASE-1344  WARN IllegalStateException: Cannot set a region as open if it has not been pending -- part2
{code}

Its the addition of this bit of code that is problem (around #355):

{code}
      if (info.isOffline() || this.master.regionManager.regionIsInTransition(info.getRegionNameAsString()) ||
          (serverName != null && this.master.serverManager.isDead(serverName))) {
        return;
      }
{code}

We added above checks so catalog scanners would not fight the shutdown scanner work.  The above is overly constraining.

Changing it to this makes close_region for both binary and non-binary keys work again:

{code}
      if (info.isOffline() ||
        (serverName != null && this.master.regionManager.regionIsInTransition(info.getRegionNameAsString())) ||
          (serverName != null && this.master.serverManager.isDead(serverName))) {
        return;
      }
{code}

Here, if serverName is null, then we've not assigned OR its been closed by somethign like 'close_region' (which clears the start code and server from .META. for the particular region).;;;","22/Jun/09 23:01;stack;Please review.;;;","22/Jun/09 23:08;apurtell;+1, but please add a comment above that code block;;;","22/Jun/09 23:09;ryanobjc;+1;;;","22/Jun/09 23:39;stack;Thanks for review lads (I added comment as you suggested on commit Andrew).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyValue$KeyComparator array overrun,HBASE-1500,12427414,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,apurtell,apurtell,08/Jun/09 23:00,13/Sep/09 22:24,01/Jul/25 07:49,15/Jun/09 05:48,0.20.0,,,,,0.20.0,,,,,,,0,"{code}
09/06/08 22:58:47 INFO zookeeper.ZooKeeper: Initiating client connection, host=B
OA03:2181,BOA02:2181,BOA01:2181,BOA04:2181 sessionTimeout=10000 watcher=org.apac
he.hadoop.hbase.zookeeper.WatcherWrapper@518bf072
09/06/08 22:58:47 INFO zookeeper.ClientCnxn: zookeeper.disableAutoWatchReset is
false
09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Attempting connection to server BOA
04/172.20.3.231:2181
09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Priming connection to java.nio.chan
nels.SocketChannel[connected local=/172.20.3.232:40296 remote=BOA04/172.20.3.231
:2181]
09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Server connection successful
09/06/08 22:58:47 WARN mapred.JobClient: Use GenericOptionsParser for parsing th
e arguments. Applications should implement Tool for the same.
09/06/08 22:58:47 WARN mapred.JobClient: No job jar file set.  User classes may
not be found. See JobConf(Class) or JobConf#setJar(String).
09/06/08 22:58:47 INFO zookeeper.ZooKeeper: Initiating client connection, host=B
OA03:2181,BOA02:2181,BOA01:2181,BOA04:2181 sessionTimeout=10000 watcher=org.apac
he.hadoop.hbase.zookeeper.WatcherWrapper@362f0d54
09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Attempting connection to server BOA
03/172.20.3.230:2181
09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Priming connection to java.nio.chan
nels.SocketChannel[connected local=/172.20.3.232:42792 remote=BOA03/172.20.3.230
:2181]
09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Server connection successful
09/06/08 22:58:48 INFO mapred.TableInputFormatBase: split: 0->BOA04.trendmicro.c
om:,01e33c601a7a9dd0ddb5c8427438f2f1
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 32
        at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:798)
        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compareRows(KeyValue.j
ava:1760)
        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:
1696)
        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:
1755)
        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:
1687)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getCac
hedLocation(HConnectionManager.java:697)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locate
RegionInMeta(HConnectionManager.java:541)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locate
Region(HConnectionManager.java:525)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locate
Region(HConnectionManager.java:488)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getReg
ionLocation(HConnectionManager.java:342)
        at org.apache.hadoop.hbase.client.HTable.getRegionLocation(HTable.java:1
91)
        at org.apache.hadoop.hbase.mapred.TableInputFormatBase.getSplits(TableIn
putFormatBase.java:296)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)
        at net.iridiant.crawler.mapred.DocumentParser.main(Unknown Source)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/09 10:04;ryanobjc;HBASE-1500.patch;https://issues.apache.org/jira/secure/attachment/12410566/HBASE-1500.patch","11/Jun/09 06:10;apurtell;hbase.log.gz;https://issues.apache.org/jira/secure/attachment/12410361/hbase.log.gz","11/Jun/09 21:26;apurtell;test.log.gz;https://issues.apache.org/jira/secure/attachment/12410431/test.log.gz","11/Jun/09 21:26;apurtell;test.rb;https://issues.apache.org/jira/secure/attachment/12410430/test.rb",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25791,,,,,Mon Jun 15 02:29:42 UTC 2009,,,,,,,,,,"0|i0hdmf:",99469,,,,,,,,,,,,,,,,,,,,,"10/Jun/09 20:16;larsgeorge;It looks like it happens here from the above

{code}
    public int compare(byte[] left, int loffset, int llength, byte[] right,
        int roffset, int rlength) {
      // Compare row
      short lrowlength = Bytes.toShort(left, loffset);
      short rrowlength = Bytes.toShort(right, roffset);
      int compare = compareRows(left, loffset + Bytes.SIZEOF_SHORT,
          lrowlength,
          right, roffset + Bytes.SIZEOF_SHORT, rrowlength);
      if (compare != 0) {
        return compare;
      }
{code}

It reads the lengths from the key byte array assuming they are correct and then applies those lengths to the compareRow() call, which then fails.

This indicates a borked length from a key array? Could you instrument the code above and log the lrowlength, rrowlength and the the real left.length and right.length?

If that mismatches, the question is why?;;;","11/Jun/09 05:41;apurtell;This is pretty basic breakage. I inserted a few 1000 rows of data keyed with MD5 sums, then tried to run a count from the shell:

{code}
count 'content'
09/06/10 22:36:15 DEBUG client.HConnectionManager$TableServers: Cache hit for ro
w <> in tableName content: location server 192.168.56.1:39961, location region n
ame content,,1244694091453
09/06/10 22:36:15 DEBUG client.HConnectionManager$TableServers: Cache hit for ro
w <> in tableName .META.: location server 192.168.56.1:39961, location region na
me .META.,,1
09/06/10 22:36:15 DEBUG client.HTable$ClientScanner: Creating scanner over conte
nt starting at key ''
09/06/10 22:36:15 DEBUG client.HTable$ClientScanner: Advancing internal scanner
to startKey at ''
09/06/10 22:36:15 DEBUG client.HConnectionManager$TableServers: Cache hit for ro
w <> in tableName content: location server 192.168.56.1:39961, location region n
ame content,,1244694091453
09/06/10 22:36:17 DEBUG client.HConnectionManager$TableServers: address: 192.168
.56.1:39961, regioninfo: REGION => {NAME => 'content,,1244698495875', STARTKEY =
> '', ENDKEY => '7f47a51d4f7a8c882288d86a4c5cdc3d', ENCODED => 1221125875, TABLE
 => {{NAME => 'content', FAMILIES => [{NAME => 'content', VERSIONS => '1', COMPR
ESSION => 'GZ', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false',
 BLOCKCACHE => 'false'}, {NAME => 'info', VERSIONS => '1', COMPRESSION => 'NONE'
, TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE =>
 'false'}, {NAME => 'url', VERSIONS => '1', COMPRESSION => 'NONE', TTL => '21474
83647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}}
Current count: 1000, row: 0a917eadf27f4917f1a823f085d1e4bd

Current count: 2000, row: 156d46ad656a85f47e4e8d50c23167e9

Current count: 3000, row: 1fef03d4744e5252dee5f44c36f74a00

Current count: 4000, row: 2a8376e2718993eab5a7aa3cfc769e42

Current count: 5000, row: 34e566f2cd7eb35ea2d7ce6bc2312638

Current count: 6000, row: 3f68045f0aae18f432e0de665909f202

Current count: 7000, row: 49c514099448cfd9609445b37f5b161f

Current count: 8000, row: 5476db74043ccf5e40cb7974e7c07c83

Current count: 9000, row: 5f3998658a477b7cbf806f4082643f2a

Current count: 10000, row: 69fd32fcf76f64116602def18248938b

Current count: 11000, row: 74be3000056b58b8b487c5243aadf02b

Current count: 12000, row: 7ef67ca4c45a28d90847288300f05d4a

Current count: 13000, row: 89783606fa73bcbf10a7b850bfa57877

Current count: 14000, row: 945c830a5eecd63e948e736deada42f3

Current count: 15000, row: 9f41e1454905fd7416f89aa4380a65e1

Current count: 16000, row: aa54517d0450127c90e4dd4631eaa010

Current count: 17000, row: b57b55373bf1302135dcefb044a58528

Current count: 18000, row: c06d66ca04ab273cc96f050ded843160

Current count: 19000, row: cb36f29c8d12098e25f007d7f658f495

Current count: 20000, row: d59d9e495fd2f983b302e67d94aa5962

Current count: 21000, row: e10430e1fda5ad363c72bb5ab7e7f8dd

Current count: 22000, row: ec1d2d9cc89cd8ec67ffa1db1f3a7474

Current count: 23000, row: f73d7cb9759f556b94aad178e6a6e0d5

09/06/10 22:37:24 DEBUG client.HTable$ClientScanner: Advancing forward from regi
on REGION => {NAME => 'content,,1244698495875', STARTKEY => '', ENDKEY => '7f47a
51d4f7a8c882288d86a4c5cdc3d', ENCODED => 1221125875, TABLE => {{NAME => 'content
', FAMILIES => [{NAME => 'content', VERSIONS => '1', COMPRESSION => 'GZ', TTL =>
 '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'
}, {NAME => 'info', VERSIONS => '1', COMPRESSION => 'NONE', TTL => '2147483647',
 BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'}, {NAME => 'u
rl', VERSIONS => '1', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '
65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}}
09/06/10 22:37:24 DEBUG client.HTable$ClientScanner: Advancing internal scanner
to startKey at '7f47a51d4f7a8c882288d86a4c5cdc3d'
NativeException: java.lang.RuntimeException: org.apache.hadoop.hbase.client.Retr
iesExhaustedException: Trying to contact region server null for region , row '7f
47a51d4f7a8c882288d86a4c5cdc3d', but failed after 5 attempts.
Exceptions:
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
        from org/apache/hadoop/hbase/client/HTable.java:2002:in `hasNext'
        from sun.reflect.GeneratedMethodAccessor2:-1:in `invoke'
        from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
        from java/lang/reflect/Method.java:597:in `invoke'
        from org/jruby/javasupport/JavaMethod.java:298:in `invokeWithExceptionHandling'
        from org/jruby/javasupport/JavaMethod.java:259:in `invoke'
        from org/jruby/java/invokers/InstanceMethodInvoker.java:36:in `call'
        from org/jruby/runtime/callsite/CachingCallSite.java:70:in `call'
        from org/jruby/ast/CallNoArgNode.java:61:in `interpret'
        from org/jruby/ast/WhileNode.java:127:in `interpret'
        from org/jruby/ast/NewlineNode.java:104:in `interpret'
        from org/jruby/ast/BlockNode.java:71:in `interpret'
        from org/jruby/internal/runtime/methods/InterpretedMethod.java:163:in `call'
        from org/jruby/internal/runtime/methods/DefaultMethod.java:144:in `call'
        from org/jruby/runtime/callsite/CachingCallSite.java:110:in `call'
        from C_3a_/cygwin/opt/HBASE_minus_7e_1_dot_3/bin/hirb.rb:369:in `method_
_25$RUBY$count'
... 106 levels...
        from C_3a_/cygwin/opt/HBASE_minus_7e_1_dot_3/bin/hirb#start:-1:in `call'
        from org/jruby/internal/runtime/methods/DynamicMethod.java:226:in `call'
        from org/jruby/internal/runtime/methods/CompiledMethod.java:211:in `call'
        from org/jruby/internal/runtime/methods/CompiledMethod.java:71:in `call'
        from org/jruby/runtime/callsite/CachingCallSite.java:253:in `cacheAndCall'
        from org/jruby/runtime/callsite/CachingCallSite.java:72:in `call'
        from C_3a_/cygwin/opt/HBASE_minus_7e_1_dot_3/bin/hirb.rb:450:in `__file__'
        from C_3a_/cygwin/opt/HBASE_minus_7e_1_dot_3/bin/hirb.rb:-1:in `load'
        from org/jruby/Ruby.java:577:in `runScript'
        from org/jruby/Ruby.java:480:in `runNormally'
        from org/jruby/Ruby.java:354:in `runFromMain'
        from org/jruby/Main.java:229:in `run'
        from org/jruby/Main.java:110:in `run'
        from org/jruby/Main.java:94:in `main'
        from C:\cygwin\opt\HBASE-~1.3\/bin/hirb.rb:369:in `count'
        from (hbase):4
{code};;;","11/Jun/09 06:10;apurtell;Complete log from initialization through first split through failed count attached. Test environment runs master and regionserver in single process, uses local filesystem, is fed by a Heritrix instance. ;;;","11/Jun/09 21:26;apurtell;To reproduce, run the attached script using the latest trunk.

{{./bin/hbase shell test.rb}}

Wait for the split to finish. Then, try a count:

{{./bin/hbase shell}}
{{hbase> count 'test'}}

{code}
count 'test'

09/06/11 14:23:32 DEBUG client.HConnectionManager$TableServers: address: 192.168
.56.1:22720, regioninfo: REGION => {NAME => 'test,,1244755386281', STARTKEY => '
', ENDKEY => '7fd99653bf71ea6c747d34db7846c708', ENCODED => 1840196805, TABLE =>
 {{NAME => 'test', FAMILIES => [{NAME => 'content', VERSIONS => '3', COMPRESSION
 => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLO
CKCACHE => 'false'}, {NAME => 'info', VERSIONS => '3', COMPRESSION => 'NONE', TT
L => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'fa
lse'}]}}
09/06/11 14:23:32 DEBUG client.HConnectionManager$TableServers: Cache hit for ro
w <> in tableName .META.: location server 192.168.56.1:22720, location region na
me .META.,,1
09/06/11 14:23:32 DEBUG client.HTable$ClientScanner: Creating scanner over test
starting at key ''
09/06/11 14:23:32 DEBUG client.HTable$ClientScanner: Advancing internal scanner
to startKey at ''
09/06/11 14:23:32 DEBUG client.HConnectionManager$TableServers: Cache hit for ro
w <> in tableName test: location server 192.168.56.1:22720, location region name
 test,,1244755386281
Current count: 1000, row: ffe83c9f18ab2f057cd6435f04228377

09/06/11 14:23:33 DEBUG client.HTable$ClientScanner: Advancing forward from regi
on REGION => {NAME => 'test,,1244755386281', STARTKEY => '', ENDKEY => '7fd99653
bf71ea6c747d34db7846c708', ENCODED => 1840196805, TABLE => {{NAME => 'test', FAM
ILIES => [{NAME => 'content', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '21
47483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'}, {
NAME => 'info', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLO
CKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}}
09/06/11 14:23:33 DEBUG client.HTable$ClientScanner: Advancing internal scanner
to startKey at '7fd99653bf71ea6c747d34db7846c708'
NativeException: java.lang.RuntimeException: org.apache.hadoop.hbase.client.Retr
iesExhaustedException: Trying to contact region server null for region , row '7f
d99653bf71ea6c747d34db7846c708', but failed after 5 attempts.
Exceptions:
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
        from org/apache/hadoop/hbase/client/HTable.java:2002:in `hasNext'
        from sun.reflect.GeneratedMethodAccessor2:-1:in `invoke'
{code}

Probably the script does a bit more than is strictly necessary to reproduce. 

Attached also is the master+regionserver log at DEBUG level from the test. ;;;","12/Jun/09 07:11;larsgeorge;Just asking Andrew, since you lost me here a bit, the initial issue with the comparator and the first log dump, what has that do with the later client based issues? When I check you first set of logs there were hardly any issues (in terms of exceptions) in it. How does this all fit together? Sorry for being thick.;;;","12/Jun/09 19:22;apurtell;Some detail I added up on IRC:

<apurtell>	i was wondering if it was just me...
<apurtell>	yeah would be great if someone else can confirm the repro works for them...
<apurtell>	what's odd is the shell's count of all the rows comes back before it even tries to advance the scanner to the next split. some kind of META sort problem, parent region coming back before daughter? i'd like to dig in but this code is all new to me
<apurtell>	in the repro case, the split is triggered and confirmed, so there is more than one region...
<apurtell>	this is the same basic problem as with TableSplit, iterating over META is going off the rails, giving bad results, triggering the out of bounds exception
<apurtell>	at least this is what it looks like to me
<apurtell>	started immediately after commit of 1304
<apurtell>	noticed it first while testing stargate scanners, scanners hang/crash/fail on tables with more than one region

;;;","13/Jun/09 18:58;apurtell;Testing with patch for HBASE-1495 changes the behavior of the test case some. The iteration of regions seems in order now, but the AIOBE remains.

{code}
Version: 0.20.0-dev, r784310, Sat Jun 13 18:51:23 UTC 2009
hbase(main):001:0> count 'test'
09/06/13 18:54:14 INFO zookeeper.ZooKeeperWrapper: Quorum servers: localhost:2181
NativeException: java.lang.RuntimeException: org.apache.hadoop.hbase.client.
RetriesExhaustedException: Trying to contact region server null for region , row
'86d6531b7fd76860445f8ffd1a0461fc', but failed after 5 attempts.
Exceptions:
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
java.lang.ArrayIndexOutOfBoundsException: 32
        from org/apache/hadoop/hbase/client/HTable.java:2000:in `hasNext'
        from sun.reflect.GeneratedMethodAccessor2:-1:in `invoke'
        from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
{code}
;;;","14/Jun/09 07:44;ryanobjc;this might be related to HBASE-1503 - the hfile might be going bad during a compaction and or split.

I saw this on a concurrent insert/count on my own cluster.

Either the data is bad (hmm) or the hfile is having a problem?;;;","14/Jun/09 07:48;ryanobjc;the regionserver logs say something like so:

2009-06-14 00:44:43,751 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: java.lang.NullPointerException
2009-06-14 00:44:43,752 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 2 on 60021, call next(-1247002001233657007, 30) from 10.10.20.13:36528: error: java.io.IOExc
eption: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:832)        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:822)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1889)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:643)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.read(BoundedRangeFileInputStream.java:97)
        at org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.read(BoundedRangeFileInputStream.java:85)
        at org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.read(BoundedRangeFileInputStream.java:78)
        at org.apache.hadoop.io.compress.BlockDecompressorStream.rawReadInt(BlockDecompressorStream.java:120)
        at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:66)
        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:74)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:100)        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.decompress(HFile.java:951)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.readBlock(HFile.java:907)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.next(HFile.java:1082)        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.next(StoreFileScanner.java:56)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:79)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:140)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:106)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1716)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1878)
        ... 5 more
2009-06-14 00:44:45,757 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: java.lang.IllegalArgumentException
2009-06-14 00:44:45,758 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 5 on 60021, call next(-1247002001233657007, 30) from 10.10.20.13:36528: error: java.io.IOExc
eption: java.lang.IllegalArgumentException
java.io.IOException: java.lang.IllegalArgumentException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:832)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:822)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1889)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:643)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.IllegalArgumentException
        at java.nio.Buffer.position(Buffer.java:218)        at org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.next(HFile.java:1073)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.next(StoreFileScanner.java:56)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:79)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:140)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:106)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1716)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1878)
        ... 5 more
2009-06-14 00:44:47,762 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: java.lang.NegativeArraySizeException
2009-06-14 00:44:47,763 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 4 on 60021, call next(-1247002001233657007, 30) from 10.10.20.13:36528: error: java.io.IOException: java.lang.NegativeArraySizeException
java.io.IOException: java.lang.NegativeArraySizeException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:832)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:822)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1889)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:643)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.NegativeArraySizeException
        at org.apache.hadoop.hbase.KeyValue.getRow(KeyValue.java:890)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1708)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1878)
        ... 5 more

then we get the negative array size exceptions from then on out.  very strange.

this seems very related to 1503.  I wonder how we can fix up hfile to be less errorful.;;;","14/Jun/09 08:47;ryanobjc;i see this on my laptop.  looks like the scanner is returning a null item in an array at some point which needless to say isnt handy.;;;","14/Jun/09 10:04;ryanobjc;here's a fix to the problem.  I verified it fixes it.

Thanks for the great bug report apurtell, complete with single-machine repro test case!

Enjoy the fix.;;;","14/Jun/09 16:30;apurtell;+1 confirmed fix. ;;;","15/Jun/09 02:29;stack;+1

{code}
02:27 < St^Ack> why 1500 change getComparator?
02:27 < dj_ryan> heh
02:27 < dj_ryan> it fixes 1500
02:27 < dj_ryan> here's why
02:27 < dj_ryan> the old code
02:27 < dj_ryan> was treating the rowkey
02:27 < dj_ryan> as a _full_ KeyValue key
02:27 < dj_ryan> and was decoding ints
02:27 < dj_ryan> to figure out the length of thw row part
02:27 < dj_ryan> but we just wanted to compare the row portion
02:28 < St^Ack> oh
02:28 < St^Ack> bad
02:29 < dj_ryan> yes
02:29 < dj_ryan> very very bad

{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong or indeterminate behavior when there are duplicate versions of a column,HBASE-1485,12427237,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,pranavkhaitan,streamy,streamy,05/Jun/09 18:08,20/Nov/15 13:01,01/Jul/25 07:49,08/Sep/10 17:23,0.20.0,,,,,0.90.0,,,regionserver,,,,1,"As of now, both gets and scanners will end up returning all duplicate versions of a column.  The ordering of them is indeterminate.

We need to decide what the desired/expected behavior should be and make it happen.

Note:  It's nearly impossible for this to work with Gets as they are now implemented in 1304 so this is really a Scanner issue.  To implement this correctly with Gets, we would have to undo basically all the optimizations that Gets do and making them far slower than a Scanner.",,,,,,,,,,HBASE-2265,HBASE-2406,,,,,,,,,,,,,,HBASE-997,,,,"08/Sep/10 16:55;streamy;HBASE-1485-v9.patch;https://issues.apache.org/jira/secure/attachment/12454126/HBASE-1485-v9.patch","07/Sep/10 15:44;evertot;TestCellUpdates.java;https://issues.apache.org/jira/secure/attachment/12454026/TestCellUpdates.java",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25781,Reviewed,,,,Fri Nov 20 13:01:15 UTC 2015,,,,,,,,,,"0|i0hdj3:",99454,,,,,,,,,,,,,,,,,,,,,"05/Jun/09 18:33;stack;Just to say Gets and Scanners should work the same, whatever we decide.;;;","09/Jul/09 18:40;streamy;I've had at least three people with a use case for this.

Might create a couple sub-tasks here so we can at least head in the right direction.

First, we need to make scanners ignore duplicate versions of the same column.  The trickiest part is, how do we determine which to keep?  We want to always come from the latest storefile, but I believe their IDs are still random and not timestamps?  We might need to make that change to fix this.  Would also then require a modification to the KVHeap to take this into account, all other things considered equal.

Once we have scanners working, that will mean the proper thing is enforced on major (and if we want, minor) compactions.

Gets will only work once we re-implement Gets as an optimized scan (taking advantage of bloom filters, mostly).


I remember why I punted this to 0.20.1, the tricky part at the beginning is pretty tough and touches a good bit of core read-path code.

Revisiting now, we'll see.  Anyone else interested in this / want to work on it?;;;","10/Jul/09 05:29;stack;Yeah, as ryan suggested, we should exploit sequenceid -- maybe name file for sequenceid?;;;","12/Aug/09 22:50;streamy;Bumped to 0.21;;;","21/Aug/09 22:42;stack;Related to hbase-997;;;","27/Jan/10 00:48;stack;From the list:

{code}
On Tue, Jan 26, 2010 at 9:36 AM, Rod Cope <rod.cope at openlogic dot com> wrote:
> Hi,
>
> I¹m seeing behavior on 0.20.2 and 0.20.3 that doesn¹t seem quite right and
> would like to know if this is by design, a bug, or something I¹m doing
> wrong.
>
> Background:
>
> When I do a put that includes a timestamp like this (conceptually ­ I know
> this is not the actual API), it works just fine.
>  put ³table², ³family², ³column², ³bbb², 12345
>
> Then, if I do another put in the same client code using the same timestamp
> like this...
>  put ³table², ³family², ³column², ³aaa², 12345
>
> ...and I create a scanner, grab a Result, and iterate over all values using
> list(), I get this...
>  ³table², ³family², ³column², ³aaa², 12345
>
> So far, so good.  Now, if I truncate the table from the shell and run a new
> program that does a flush() on the table between the two put¹s, but does it
> in the same client program back-to-back, I also get the same results from
> list().
>
> -----
>
> Problem:
>
> Here¹s where the trouble starts.  I truncate the table and run a new program
> that puts ³bbb², flushes the table, and quits.  Here¹s what I get from
> list():
>  ³table², ³family², ³column², ³bbb², 12345
>
> Then I run another program that puts ³aaa², flushes, and quits.  Here¹s what
> I get from list():
>  ³table², ³family², ³column², ³aaa², 12345
>  ³table², ³family², ³column², ³bbb², 12345
>
> And if I then run a third program that puts ³ccc², flushes, and quits, I get
> this from list():
>  ³table², ³family², ³column², ³ccc², 12345
>  ³table², ³family², ³column², ³bbb², 12345
>  ³table², ³family², ³column², ³aaa², 12345
>
> I¹m getting three different values for identical
> table/family/qualifier/timestamp tuples.  Does this seem right?  There also
> doesn¹t seem to be a defined sort order, probably because the timestamps are
> identical.
>
> Also, if instead of using list(), I use getMap(), then I always only get a
> single result.  The single result is always the last item in the lists above
> (i.e., ³bbb² then ³bbb² then ³aaa²).  I get identical results from using
> getNoVersionMap().
>
> I suspect that this same behavior could occur when HBase decides to flush on
> its own, but I could be wrong.  As you can imagine, this can cause problems
> because clients can¹t know from the results of calling list() which value is
> ³right² or ³newest².  They also can¹t rely on getMap() or getNoVersionMap()
> because the single result that gets returned is not necessarily ³right² or
> ³newest².
>
> I¹ve reproduced everything above in a stand-alone installation and also with
> a 7 regionserver cluster with the final 0.20.3.  I started down this
> debugging path originally because I ran into this problem on the 7
> regionserver cluster with one table of 100+ regions.  I was flushing
> programmatically at the end of some large imports because I'm doing
> setWriteToWAL(false) for load performance.
>
> Am I doing something wrong?  Did I miss an HBase assumption about flushing
> and/or identical timestamps?
>
> Any help would be much appreciated.
{code};;;","10/Jun/10 21:54;ryanobjc;Right now Get = 1 row Scans, thus making this issue at least twice as easy :-);;;","03/Sep/10 02:29;stack;Hurray!;;;","06/Sep/10 22:55;pranavkhaitan;Patch for this issue posted at https://review.cloudera.org/r/780/;;;","07/Sep/10 15:44;evertot;We've tried the patch posted at https://review.cloudera.org/r/780/ here at Outerthought.
The attached file is a unit test showing that the patch works at first sight.
However, performing updates on existing timestamps, in combination with triggering major compactions things don't work as expected.

I've used negative-assertions in order to make the tests succeed, and added a comment where we would expect the result to be otherwise.

I've also added a test with the example where a row is deleted and then an update on an older timestamp afterwards remains hidden by the delete.;;;","07/Sep/10 15:53;streamy;Great stuff Evert.  This is really helpful.

In the delete case, this is still the expected behavior.  This patch doesn't change anything with regards to insertions past deletions.;;;","07/Sep/10 15:57;evertot;The attached test TestCellUpdates.java also intializes a new HBaseTestingUtility for each test method.
Setting this up only once for the whole test class causes issues when triggering the major compaction which I haven't been able to pinpoint yet.;;;","07/Sep/10 15:58;pranavkhaitan;Thanks Evert, for point this out. As Jonathan mentioned, the behavior remains the same for the delete case. I will look more into the flushing case and keep you updated.;;;","07/Sep/10 16:15;evertot;I understand that the delete case isn't related to this jira. It would have been better to put it in a separate unit test.;;;","07/Sep/10 19:30;pranavkhaitan;This is resolved. The fixed patch and explanation for this has been posted at https://review.cloudera.org/r/780/. Let me know if there are further questions. 

Thanks again Evert, for making us aware of this case!;;;","08/Sep/10 16:55;streamy;Final patch posted by Pranav on reviewboard.  Has been reviewed and approved for commit by Ryan and myself.  Tests passing besides what already fails on trunk.;;;","08/Sep/10 17:23;streamy;Committed to trunk.  Thanks Pranav!  Great work!;;;","08/Sep/10 19:13;stevenn;Thanks all!;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"During cluster shutdown, deleting zookeeper regionserver nodes causes exceptions",HBASE-1471,12426846,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,streamy,streamy,01/Jun/09 19:08,13/Sep/09 22:24,01/Jul/25 07:49,01/Jun/09 23:05,0.20.0,,,,,0.20.0,,,,,,,0,"Exception happens during some unit tests:

{noformat}
2009-06-01 11:05:18,075 INFO  [RegionServer:0] regionserver.HRegionServer(665): stopping server at: 192.168.249.1:41463
2009-06-01 11:05:18,076 INFO  [RegionServer:0] regionserver.HRegionServer(679): RegionServer:0 exiting
2009-06-01 11:05:18,077 DEBUG [HMaster] zookeeper.ZooKeeperWrapper(531): Deleting node: 1243883037682
Exception in thread ""HMaster"" java.lang.IllegalArgumentException: Path must start with / character
	at org.apache.zookeeper.ZooKeeper.validatePath(ZooKeeper.java:537)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:642)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.clearRSDirectory(ZooKeeperWrapper.java:532)
	at org.apache.hadoop.hbase.master.RegionManager.stop(RegionManager.java:620)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:425)
2009-06-01 11:05:19,077 INFO  [main] hbase.LocalHBaseCluster(294): Shutdown HMaster 1 region server(s)
2009-06-01 11:05:19,081 INFO  [main] hbase.HBaseTestCase(592): Shutting down FileSystem
2009-06-01 11:05:19,081 INFO  [main] hbase.HBaseTestCase(599): Shutting down Mini DFS 
Shutting down the Mini HDFS Cluster
Shutting down DataNode 1
2009-06-01 11:05:19,178 DEBUG [HMaster-EventThread] client.HConnectionManager$TableServers(203): Got ZooKeeper event, state: Disconnected, type: None, path: null
2009-06-01 11:05:19,178 DEBUG [RegionManager.rootScanner-EventThread] client.HConnectionManager$TableServers(203): Got ZooKeeper event, state: Disconnected, type: None, path: null
2009-06-01 11:05:19,179 DEBUG [main-EventThread] client.HConnectionManager$TableServers(203): Got ZooKeeper event, state: Disconnected, type: None, path: null
2009-06-01 11:05:19,179 INFO  [main-EventThread] regionserver.HRegionServer(367): Got ZooKeeper event, state: Disconnected, type: None, path: null
2009-06-01 11:05:19,179 DEBUG [main-EventThread] regionserver.HRegionServer(372): Ignoring ZooKeeper event while shutting down
2009-06-01 11:05:19,185 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@d522de2] datanode.DataXceiverServer(137): DatanodeRegistration(127.0.0.1:36486, storageID=DS-350082559-192.168.249.1-36486-1243883037153, infoPort=58600, ipcPort=57545):DataXceiveServer: java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:170)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:102)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:130)
	at java.lang.Thread.run(Thread.java:636)

Shutting down DataNode 0
2009-06-01 11:05:20,293 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@42238016] datanode.DataXceiverServer(137): DatanodeRegistration(127.0.0.1:37436, storageID=DS-1878503705-192.168.249.1-37436-1243883036827, infoPort=41289, ipcPort=36123):DataXceiveServer: java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:170)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:102)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:130)
	at java.lang.Thread.run(Thread.java:636)

2009-06-01 11:05:21,395 WARN  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$ReplicationMonitor@286e4365] namenode.FSNamesystem$ReplicationMonitor(2306): ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
2009-06-01 11:05:21,399 INFO  [Thread-121] regionserver.HRegionServer$ShutdownThread(951): Starting shutdown thread.
2009-06-01 11:05:21,400 INFO  [Thread-121] regionserver.HRegionServer$ShutdownThread(959): Shutdown thread complete
{noformat}

This is from the test org.apache.hadoop.hbase.client.TestHTable.testHTable()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/09 23:04;stack;1471.patch;https://issues.apache.org/jira/secure/attachment/12409610/1471.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25773,,,,,Tue Jun 02 01:05:53 UTC 2009,,,,,,,,,,"0|i0hdg7:",99441,,,,,,,,,,,,,,,,,,,,,"01/Jun/09 23:04;stack;Patch made w/ nitay/jd guidance.;;;","02/Jun/09 01:05;streamy;Stack, you pushed this to 1304 Git repo, correct?

I'm still seeing it with latest 1304:

{noformat}
2009-06-01 17:03:18,040 DEBUG [HMaster] hbase.RegionHistorian(333): Offlined
2009-06-01 17:03:18,043 DEBUG [HMaster] zookeeper.ZooKeeperWrapper(531): Deleting node: 1243904574848
Exception in thread ""HMaster"" java.lang.IllegalArgumentException: Path must start with / character
	at org.apache.zookeeper.ZooKeeper.validatePath(ZooKeeper.java:537)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:642)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.clearRSDirectory(ZooKeeperWrapper.java:532)
	at org.apache.hadoop.hbase.master.RegionManager.stop(RegionManager.java:620)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:428)
{noformat}

Same test as before, TestHTable.testHTable();;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Binary keys are not first class citizens,HBASE-1466,12426814,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,01/Jun/09 08:26,13/Sep/09 22:24,01/Jul/25 07:49,08/Jun/09 02:56,0.20.0,,,,,0.20.0,,,,,,,0,"If you use binary keys, you don't get full features as if you were not using binary keys.  Some things that are broken:

- grep/less cant search in logfiles with binary data
- displays are unreadable due to weird utf8/other issues
- can't use the region historian
- etc
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/09 21:27;ryanobjc;HBASE-1466-v2.patch;https://issues.apache.org/jira/secure/attachment/12410073/HBASE-1466-v2.patch","06/Jun/09 21:16;ryanobjc;HBASE-1466.patch;https://issues.apache.org/jira/secure/attachment/12410070/HBASE-1466.patch","01/Jun/09 08:29;ryanobjc;ryan-uber.patch;https://issues.apache.org/jira/secure/attachment/12409546/ryan-uber.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25768,,,,,Mon Jun 08 02:56:51 UTC 2009,,,,,,,,,,"0|i0hdfb:",99437,,,,,,,,,,,,,,,,,,,,,"01/Jun/09 08:29;ryanobjc;this also fixes HBASE-1467, HBASE-1468;;;","01/Jun/09 15:32;stack;Committed with following differences:

+ Moved HFileStats#main to HFile.   Thats where I think folks will go to look for it.
+ Did not commit the bloom filter implementations.  Seemed odd that this patch was removing bloom filters from core then adding implementations to util.  I pasted the implemetnations to a patch up in hbase-1200
+ I commented out bits of the new murmurhash test because they are failing.... maybe you can check it out: HBASE-

This patch seems to add a lot of logging but we can turn that down later.

Ryan, does this patch resolve HBASE-1131?  HBASE-1363?

Thanks for the sweet patch Ryan.;;;","01/Jun/09 15:34;stack;HBASE-1469 is the murmur hash issue.;;;","06/Jun/09 20:56;ryanobjc;this never got applied to trunk...;;;","06/Jun/09 21:16;ryanobjc;a revised patch that does 2 things:
- removes bloom filter flags from HFile
- enables binary key pretty printing.;;;","06/Jun/09 21:20;ryanobjc;supplemental patch to fix build problem. layer on after the previous patch.;;;","06/Jun/09 21:26;ryanobjc;updated patch that builds against trunk.;;;","06/Jun/09 21:27;ryanobjc;v2 of patch that builds against trunk.;;;","06/Jun/09 22:00;streamy;Applied v2 patch to trunk.  All tests I ran passed.  +1 on commit.;;;","08/Jun/09 01:57;stack;Sorry for bungling this one.  Thought I'd applied it.;;;","08/Jun/09 02:56;stack;Looks like Ryan committed this -- resolving.

{code}
Author: rawson
Date: Sat Jun  6 22:07:20 2009
New Revision: 782314

URL: http://svn.apache.org/viewvc?rev=782314&view=rev
Log:
HBASE-1466, binary keys now are pretty-printed, removed HFile bloom filter flag
...
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hclient still seems to depend on master,HBASE-1462,12426762,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,ryanobjc,ryanobjc,31/May/09 09:07,13/Sep/09 22:24,01/Jul/25 07:49,03/Jun/09 16:48,0.20.0,,,,,0.20.0,,,,,,,0,"during a master down, but cluster up event, my clients seem to not work.

clients shouldnt need to talk to the master anymore in 0.20.  We should double check this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/09 19:50;jdcryans;hbase-1462-v1.patch;https://issues.apache.org/jira/secure/attachment/12409685/hbase-1462-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25765,Reviewed,,,,Wed Jun 03 16:48:03 UTC 2009,,,,,,,,,,"0|i0hdef:",99433,,,,,,,,,,,,,,,,,,,,,"02/Jun/09 19:50;jdcryans;Patch that removes a bunch of getMaster() in HCM (keeping the important ones). I tested this patch on a 4 nodes cluster with PE sequentialWrite 5, I killed the master once the table was created and waited 1 min before bringing it back with no problem. It also passes the tests.;;;","02/Jun/09 21:41;stack;Patch looks good to me (I didn't try it).  Would suggest committing and lets see how it goes.  Can open new issues if removal of getMasters is a prob (My guess a vestige from pre-zk).;;;","03/Jun/09 16:48;jdcryans;Committed. I think it's there because we didn't remove them before.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Taking down ROOT/META regionserver can result in cluster becoming in-operational,HBASE-1457,12426646,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,29/May/09 05:46,11/Jun/22 20:46,01/Jul/25 07:49,31/May/09 16:29,0.20.0,,,,,,,,,,,,0,"Take down a regionserver via controlled or uncontrolled shutdown, the master doesn't properly reassign the root/meta regions. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/09 16:03;stack;1457-0.19.patch;https://issues.apache.org/jira/secure/attachment/12409504/1457-0.19.patch","29/May/09 17:53;stack;HBASE-1457-v2.patch;https://issues.apache.org/jira/secure/attachment/12409407/HBASE-1457-v2.patch","30/May/09 04:54;ryanobjc;HBASE-1457-v3.patch;https://issues.apache.org/jira/secure/attachment/12409451/HBASE-1457-v3.patch","31/May/09 08:40;ryanobjc;HBASE-1457-v4.patch;https://issues.apache.org/jira/secure/attachment/12409492/HBASE-1457-v4.patch","29/May/09 05:47;ryanobjc;HBASE-1457.patch;https://issues.apache.org/jira/secure/attachment/12409340/HBASE-1457.patch",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25762,,,,,Sun May 31 16:29:44 UTC 2009,,,,,,,,,,"0|i0hddb:",99428,,,,,,,,,,,,,,,,,,,,,"29/May/09 05:47;ryanobjc;may not patch against trunk cleanly :-(;;;","29/May/09 16:54;apurtell;Patch looks good. I'll try it out.;;;","29/May/09 17:34;stack;Patch changes MetaRegion so it takes a HRegionInfo rather than region name and startkey.

It ensures -ROOT- and .META. assignment happens first -- previous -ROOT- didn't get special treatment.  Also, doesn't depend on getting close of catalog region.  Instead, exiting, checks if server was carrying catalog regions and if it was, schedules them for immediate assignment (no log splitting when server exits, as opposes to crashes).

It takes the updating of region historian out of the main code path processing alls-well messages putting it instead on the todo queue to be processed by worker thread IF meta and root are on line.


;;;","29/May/09 17:53;stack;

I reviewed Ryan's patch and its all good to me.  I was going to suggest adding toString to the new anonymous TODO queue addition but see it already done.  The attached patch applies cleanly to TRUNK.;;;","30/May/09 04:54;ryanobjc;fixes a case where ROOT isnt recovered after a regionserver hard kill -9 type crash.  Beefed up handling of ROOT/META in ProcessServerShutdown.;;;","30/May/09 13:35;stack;This last patch seems to work great but the only odd thing is that it always reassigns -ROOT- and .META.   I flush -ROOT- region then I kill -9 the -ROOT- server.  I see the logs being split and then -ROOT- assigned.  I see the regionserver opening -ROOT- AND applying edits but then when -ROOT- scanner runs, it says server and startcode are empty and things .META. assignment invalid.

I'm looking into the above some.;;;","30/May/09 18:35;apurtell;+1 for committing -v3 patch now to trunk and 0.19 branch and work on stack's reported nit in another issue.;;;","31/May/09 08:40;ryanobjc;the latest fix, including:
- make region historian writes into todo queue
- make todo queue a priority queue, putting higher priority items to the top
- ensure double assignment of ROOT/META can't happen
- prevent assignment bugs when the cluster is mis-loaded, and ensure ROOT/META get assigned as fast as possible to the first server (rather than the best server as was previously)
-- assignment could get stuck when the 'best' server was unable to contact the master because the ROOT/META is offline. Very ugly bug.
- reduce how much we retry in pending operations, this can delay recovery because if the META/ROOT goes down while processing a TODO, the recovery of the META/ROOT has to wait until the currently running pending operation times out. This could take over 5 minutes previously (!!).  1 second time outs * 10 * 2-3 per commit() * 2 attempts takes a long time.
- improve a bug where if ROOT was unavailable some pending operations might fail and not get requeued.
- Handle bugs where a server would go offline and 'forget' to mention that ROOT or META went offline, thus delaying reassignment.  Now we force META/ROOT offline ASAP and get them reassigned as fast as possible on clean shutdown.
- Improved unclean shutdown handling of META - instead of waiting for the ROOT scanner to detect a bad assignment and fix it, be more proactive and put the META to be assigned once log split is finished.  This can improve META recovery time by 5-10 seconds.
- Fixed a rare but deadly NPE in ProcessRegionOpen, improved the handling of failed todo operations - instead of putting them back into the todo queue, put them into the delayed queue (since the NPE is a side effect of not having ROOT online yet).

Yes, All these bugs are incorporated in this relatively small patch. (933 lines of diff)  
;;;","31/May/09 15:54;stack;I tested it.  Works great.  There is an issue where if -ROOT- goes down, after successful redeploy, I see that the .META. also will be redeployed (says assignment is invalid though it is not).  Will make separate issue for this.

Working on the backport.  Its a little sticky.;;;","31/May/09 16:03;stack;0.19 patch based on v4.  Testing now.;;;","31/May/09 16:29;stack;Tested 0.19 patch.  The recovery is not as sleek as it is in 0.20. because no zk back in the branch -- but it works.  I left in the 'alls well' message in trunk but removed it in branch.  Its a little obnoxious but we can turn it off just before release.  Meantime will help debugging.  Thanks for great patch Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update DemoClient.py for thrift 1.0,HBASE-1455,12426623,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,tim_s,tim_s,28/May/09 20:31,13/Sep/09 22:24,01/Jul/25 07:49,30/May/09 21:44,0.20.0,,,,,0.20.0,,,Thrift,,,,0,"There were a number of little things that have changed, eg: mutations are constructed differently (HBASE-1343) and gets return lists now.

",,,,,,,,,,,,,,,,,,,,,,,HBASE-1343,HBASE-1153,,,,,"28/May/09 20:33;tim_s;HBASE-1455.patch;https://issues.apache.org/jira/secure/attachment/12409298/HBASE-1455.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25761,,,,,Sat May 30 21:44:44 UTC 2009,,,,,,,,,,"0|i0hdcv:",99426,,,,,,,,,,,,,,,,,,,,,"28/May/09 20:33;tim_s;A number of little fixes that take into account all the changes to the thrift api. This will work now and when we upgrade to the coming proper thrift release. It looks a bit more pythonic now that we don't use NotFound exceptions.;;;","30/May/09 21:44;stack;Thanks for the patch Tim.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRegionServer is using wrong info bind address from hbase-site.xml,HBASE-1435,12425753,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,larsgeorge,larsgeorge,larsgeorge,18/May/09 18:59,13/Sep/09 22:34,01/Jul/25 07:49,19/May/09 04:57,0.19.2,,,,,0.19.3,,,regionserver,,,,0,"From HRegionServer.java, lines 1149+ (on current trunk):

{code}
    if (port >= 0) {
      String addr = this.conf.get(""hbase.master.info.bindAddress"", ""0.0.0.0"");
      // check if auto port bind enabled
      boolean auto = this.conf.getBoolean(""hbase.regionserver.info.port.auto"",
          false);
{code}

The above line needs to be changed to

{code}
      String addr = this.conf.get(""hbase.regionserver.info.bindAddress"", ""0.0.0.0"");
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/09 19:04;larsgeorge;1435.patch;https://issues.apache.org/jira/secure/attachment/12408407/1435.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25751,,,,,Tue May 19 04:57:05 UTC 2009,,,,,,,,,,"0|i0hd8v:",99408,,,,,,,,,,,,,,,,,,,,,"18/May/09 19:04;larsgeorge;Patch fixes the property key.;;;","19/May/09 04:57;stack;Commited to trunk and branch.  Thanks for the patch Lars.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in HTable.checkAndSave when row doesn't exist,HBASE-1431,12425663,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,guiga,guiga,guiga,16/May/09 23:13,13/Sep/09 22:24,01/Jul/25 07:49,18/May/09 05:19,0.19.2,,,,,0.19.3,0.20.0,,regionserver,,,,0,"To reproduce, just invoke htable.checkAndSave(batchUpdate, expectedValues, lock) using a batchUpdate of a row that doesn't exist. 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/09 23:54;guiga;HBASE-1431.patch;https://issues.apache.org/jira/secure/attachment/12408327/HBASE-1431.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25749,,,,,Mon May 18 05:19:45 UTC 2009,,,,,,,,,,"0|i0hd7z:",99404,,,,,,,,,,,,,,,,,,,,,"16/May/09 23:15;guiga;Just wrote tests and the fix (I hope :-). I'll be submitting the patch as soon as all tests finish. ;;;","16/May/09 23:54;guiga;please, could you review this code? ;;;","17/May/09 00:29;apurtell;Looks good. +1;;;","18/May/09 05:19;apurtell;Committed to trunk and 0.19 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
close HLog (and open new one) if there hasnt been edits in N minutes/hours,HBASE-1401,12425051,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,ryanobjc,ryanobjc,10/May/09 22:32,13/Sep/09 22:24,01/Jul/25 07:49,18/May/09 17:44,0.20.0,,,,,0.19.3,0.20.0,,,,,,0,this will help narrow the write hole on clusters with periods of light load.  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/09 17:53;stack;logroller-v3.patch;https://issues.apache.org/jira/secure/attachment/12408397/logroller-v3.patch","17/May/09 21:48;stack;rollwriteronperiod-v2.log;https://issues.apache.org/jira/secure/attachment/12408346/rollwriteronperiod-v2.log","17/May/09 21:46;stack;rollwriteronperiod.log;https://issues.apache.org/jira/secure/attachment/12408345/rollwriteronperiod.log",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25740,,,,,Mon May 18 17:53:18 UTC 2009,,,,,,,,,,"0|i0hd13:",99373,,,,,,,,,,,,,,,,,,,,,"11/May/09 01:26;apurtell;This would also be a necessary thing for HBASE-1295. Under that scenario it's not unreasonable to consider rolling HLog every 1 minute, or 5 minutes, assuming there have been edits of course.;;;","11/May/09 17:24;stack;We could add this feature to the LogFlusher thread, the thread that runs an optional sync on a period.;;;","14/May/09 17:50;stack;Made this a blocker.  Pset needs it.  If we set default to be every hour, then thats 100 close/opens per hour.  Thats a little load on hdfs, but one that it probably should carry fine (though I remember in old days when we used to run compactions on a periiod, that  there'd be a big spike in hdfs activity.. but thats a little different ... heavy reading and writing per region rather than per regionserver).

Need to backport this too for 0.19.3.;;;","17/May/09 21:46;stack;Patch that adds to the rollwriter thread a check on each iteration whether N milliseconds have elapsed.  If they have, call roll log.  roll log will roll log if there has been at least one edit since last roll.  Default period is one hour.  Means I/O on cluster every hour, one open/close per regionserver.

Tested on little cluster.  When I take off load, it seems to do right thing (I had it configured to run every 3 minutes and had some debug in place not included in this patch):

{code}
2009-05-17 21:38:43,297 [regionserver/0:0:0:0:0:0:0:0:60021.logRoller] INFO org.apache.hadoop.hbase.regionserver.LogRoller: LOGROLL BECAUSE PERIOD
2009-05-17 21:38:43,347 [regionserver/0:0:0:0:0:0:0:0:60021.logRoller] INFO org.apache.hadoop.hbase.regionserver.HLog: Roll /hbasetrunk2/.logs/aa0-000-15.u.powerset.com_1242595530778_60021/hlog.dat.1242596143117, entries=2, calcsize=0, filesize=378. New hlog /hbasetrunk2/.logs/aa0-000-15.u.powerset.com_1242595530778_60021/hlog.dat.1242596323344
2009-05-17 21:38:43,347 [regionserver/0:0:0:0:0:0:0:0:60021.logRoller] DEBUG org.apache.hadoop.hbase.regionserver.HLog: Found 0 hlogs to remove  out of total 8; oldest outstanding seqnum is 2758980 from region TestTable,0399337743,1242595974815
2009-05-17 21:41:43,517 [regionserver/0:0:0:0:0:0:0:0:60021.logRoller] INFO org.apache.hadoop.hbase.regionserver.LogRoller: LOGROLL BECAUSE PERIOD
2009-05-17 21:44:43,697 [regionserver/0:0:0:0:0:0:0:0:60021.logRoller] INFO org.apache.hadoop.hbase.regionserver.LogRoller: LOGROLL BECAUSE PERIOD
{code}

So, under load, the periodic rolling doesn't get a chance to run because we're rolling at the size limit.  After we take off the load we see behavior like the above where 3 minutes have elapsed since last roll and 2 edits in the file so we roll.  The next two periods elapse but we don't roll because no edits.;;;","17/May/09 21:48;stack;I attached wrong patch.   Here's right one.;;;","17/May/09 21:53;stack;Applied branch and trunk.  Closing.;;;","18/May/09 06:12;apurtell;With the change set for this issue applied, regionservers shut down right away just after coming up. If I patch out the changes, all tests pass. Reverted.;;;","18/May/09 17:44;stack;Committing again.  Last minute change broke my patch.;;;","18/May/09 17:53;stack;Here is what I applied to trunk and branch to reclose this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableOperation doesnt format keys for meta scan properly,HBASE-1398,12424995,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,09/May/09 05:58,13/Sep/09 22:24,01/Jul/25 07:49,09/May/09 06:12,0.20.0,,,,,0.20.0,,,,,,,0,"to scan the meta table, the start row must be in the format 'table_name,,' - the commas are not optional.

I found another place in TableOperation which was missing this, causing hbase to close too many regions from unrelated tables (ouch!)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/09 06:01;ryanobjc;HBASE-1398.patch;https://issues.apache.org/jira/secure/attachment/12407686/HBASE-1398.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25738,,,,,Sat May 09 06:12:29 UTC 2009,,,,,,,,,,"0|i0hd0f:",99370,,,,,,,,,,,,,,,,,,,,,"09/May/09 06:12;stack;Thanks for the patch Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
change how we build/configure lzocodec,HBASE-1392,12424890,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,08/May/09 05:59,13/Sep/09 22:24,01/Jul/25 07:49,08/May/09 15:40,0.20.0,,,,,0.20.0,,,,,,,0,"i got a reply to my proposed patch for lzocodec:

Instead of deriving from DefaultCodec, you probably want to do the followng:

CompressionCodec lzoCodec = (CompressionCodec)
ReflectionUtils.newInstance(Class.forName(""com.hadoop.compression.lzo.LzoCodec""), conf);

setConf is automatically handled by RefletionUtils.newInstance.


We should do that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/09 06:00;ryanobjc;HBASE-1392.patch;https://issues.apache.org/jira/secure/attachment/12407601/HBASE-1392.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25733,,,,,Fri May 08 15:40:25 UTC 2009,,,,,,,,,,"0|i0hcz3:",99364,,,,,,,,,,,,,,,,,,,,,"08/May/09 15:40;stack;Thanks for the patch Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestGetRowVersions doesn't pass on 0.19 branch,HBASE-1372,12424617,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,05/May/09 16:52,13/Sep/09 22:31,01/Jul/25 07:49,05/May/09 23:25,0.19.2,,,,,0.19.2,,,,,,,0,Currently TestGetRowVersions doesn't pass because the region server is too slow to setup root and meta and gets a lease expired.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/09 20:08;jdcryans;hbase-1372.patch;https://issues.apache.org/jira/secure/attachment/12407275/hbase-1372.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25720,Reviewed,,,,Tue May 05 23:25:15 UTC 2009,,,,,,,,,,"0|i0hcun:",99344,,,,,,,,,,,,,,,,,,,,,"05/May/09 16:54;jdcryans;This is what happens:

{code}
    [junit] 2009-05-05 12:53:43,790 INFO  [IPC Server handler 3 on 60000] master.RegionManager(401): assigning region test,,1241542405273 to the only server 127.0.0.1:57693
    [junit] 2009-05-05 12:53:48,716 DEBUG [main] client.HConnectionManager$TableServers(570): locateRegionInMeta attempt 0 of 10 failed; retrying after sleep of 5000
    [junit] java.net.ConnectException: Call to /127.0.0.1:46252 failed on connection exception: java.net.ConnectException: Connection refused
    [junit] 	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:728)
    [junit] 	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:704)
...
    [junit] 2009-05-05 12:53:48,717 DEBUG [main] client.HConnectionManager$TableServers(691): Removed .META.,,1 for tableName=.META. from cache because of test,row,99999999999999
    [junit] 2009-05-05 12:53:49,789 INFO  [ServerManager.leaseChecker] master.ServerManager$ServerExpirer(761): 127.0.0.1:57693 lease expired
    [junit] 2009-05-05 12:53:49,791 DEBUG [HMaster] master.HMaster(438): Processing todo: ProcessServerShutdown of 127.0.0.1:57693
    [junit] 2009-05-05 12:53:49,791 INFO  [HMaster] master.ProcessServerShutdown(235): process shutdown of server 127.0.0.1:57693: logSplit: false, rootRescanned: false, numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
    [junit] 2009-05-05 12:53:49,795 INFO  [HMaster] regionserver.HLog(714): Splitting 1 log(s) in hdfs://localhost.localdomain:56376/user/hadoop/log_127.0.0.1_1241542413670_57693
    [junit] 2009-05-05 12:53:49,795 DEBUG [HMaster] regionserver.HLog(743): Splitting 1 of 1: hdfs://localhost.localdomain:56376/user/hadoop/log_127.0.0.1_1241542413670_57693/hlog.dat.1241542414746
    [junit] 2009-05-05 12:53:49,806 INFO  [HMaster] regionserver.HLog(725): log file splitting completed for hdfs://localhost.localdomain:56376/user/hadoop/log_127.0.0.1_1241542413670_57693
{code};;;","05/May/09 19:50;jdcryans;Digging in the code I saw that the region server was waiting after the master when reporting. What happened is that when the master assigns a region, it writes an entry in the region historian. Problem is, the first time HCM (under the historian) sees a table there is a huge wait.... which was too long for the 6 seconds lease in the tests. I think for 0.19.2 we should just move that waiting time right up when the master starts with something like this in ProcessRegionOpen:

{code}
          server.batchUpdate(metaRegionName, b, -1L);
          if (!this.historian.isOnline()) {
            // This is safest place to do the onlining of the historian in
            // the master. When we get to here, we know there is a .META.
            // for the historian to go against.
            this.historian.online(this.master.getConfiguration());
          }
          this.historian.addRegionOpen(regionInfo, serverAddress);
          this.historian.getRegionHistory(""dummy""); <<<<
{code}

For trunk we could work on improving HCM.;;;","05/May/09 20:08;jdcryans;Patch that passes all tests.;;;","05/May/09 21:26;apurtell;+1 I think a workaround like this is fine for the branch. ;;;","05/May/09 23:25;stack;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
re-enable LZO using hadoop-gpl-compression library,HBASE-1370,12424581,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,05/May/09 07:04,13/Sep/09 22:24,01/Jul/25 07:49,06/May/09 21:26,0.20.0,,,,,0.20.0,,,,,,,0,"now that hadoop-gpl-compression has been released, we can add an optional run time dependency to allow LZO compression again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/09 08:36;ryanobjc;HBASE-1370-v2.patch;https://issues.apache.org/jira/secure/attachment/12407224/HBASE-1370-v2.patch","05/May/09 07:07;ryanobjc;HBASE-1370.patch;https://issues.apache.org/jira/secure/attachment/12407218/HBASE-1370.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25718,,,,,Wed May 06 21:26:32 UTC 2009,,,,,,,,,,"0|i0hcu7:",99342,,,,,,,,,,,,,,,,,,,,,"05/May/09 07:07;ryanobjc;this adds a run-time dependency only.  It also depends on bug #6 of the hadoop-gpl-compression bug.;;;","05/May/09 08:36;ryanobjc;need to enable in the HColumnDescriptor;;;","05/May/09 16:19;stack;Patch looks good. 

Have you tried enabling from shell/disabling from shell?

If I wanted to enable hbase lzo, where are instructions on how?  Can we include in javadoc somewhere?

;;;","06/May/09 17:13;stack;Up on IRC we discussed adding a few notes to wiki on how you'd enable.  Waiting on these before applying.;;;","06/May/09 21:26;stack;Committed.  Added pointer to lzo wiki page howto into javadoc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in TableInputFormatBase.setInputColums,HBASE-1365,12424396,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,01/May/09 15:47,20/Sep/12 22:10,01/Jul/25 07:49,02/May/09 23:56,0.19.1,0.20.0,,,,0.19.2,0.20.0,,,,,,0,Typo in method name.  TableInputFormatBase.getInputColums should be .getInputColumns.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/09 16:08;streamy;hbase-1365-v1.patch;https://issues.apache.org/jira/secure/attachment/12407014/hbase-1365-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25715,,,,,Sat May 02 23:56:40 UTC 2009,,,,,,,,,,"0|i0hct3:",99337,,,,,,,,,,,,,,,,,,,,,"01/May/09 16:08;streamy;Applies (with offsets) to both 0.19 branch and 0.20 trunk.

Don't know how this has slipped by for so long :);;;","02/May/09 23:56;stack;Applied branch and trunk.  Thanks for the patch Jon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
If one sets the hbase.master to 0.0.0.0 non local regionservers can't find the master,HBASE-1357,12424199,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,posix4e,posix4e,29/Apr/09 18:38,02/May/13 02:28,01/Jul/25 07:49,02/Jun/09 17:54,0.20.0,0.20.1,0.90.0,,,0.20.0,,,master,regionserver,,,0,"(2:11:20 PM) posix4e: so i want to run a back master on each node
(2:11:29 PM) posix4e: and i have my hbase.master set to 0.0.0.0
(2:14:59 PM) posix4e: each master only gets the local regionserver connecting
(2:15:08 PM) posix4e: as it must be using that variable to know what to connect to
(2:15:32 PM) nitay: the RS don't use hbase.master* anymore
(2:15:36 PM) nitay: ohhh i think i know th eproblem
(2:15:44 PM) nitay: so the RS use ZK to get the master address
(2:15:49 PM) nitay: but the masters are writing 0.0.0.0 to it
(2:15:58 PM) nitay: b/c they write whatever was in their conf
(2:16:20 PM) posix4e: yea
(2:16:42 PM) nitay: can u do a zookeeper dump of that node to verify my thinking?
(2:16:55 PM) posix4e: yea
(2:17:12 PM) nitay: it should be /hbase/master, unless u've changed the defaults
(2:17:59 PM) nitay: hmm s o ye this is a problem, we solved this in RS (allowing 0.0.0.0) by having master actually write RS's address to ZK when it gets contacted
(2:18:21 PM) nitay: so now we need to find a way to find out the _actual_ address the master has bound to
(2:19:47 PM) posix4e: is their a way to do that?
(2:20:16 PM) nitay: i dont know, good question
(2:20:18 PM) posix4e: or does it require code changes i.e. regionserver checking zk
(2:20:27 PM) nitay: did u verify the master address?
(2:20:48 PM) posix4e: one sec
(2:21:03 PM) nitay: its almost like we want ZK to be able to tell us what address we're using to talk to it
(2:21:20 PM) nitay: that assumes u dont have different NICs to talk to ZK vs. HBase
(2:21:59 PM) nitay: posix4e, u can't really use the RS as far as i can tell b/c the RS knows nothing about the master until the master address appears in ZK
(2:22:25 PM) posix4e: 0:0:0:0:0:0:0:0:60000
(2:22:40 PM) nitay: yep that's the magic
(2:22:45 PM) nitay: k thx for verifying
(2:22:54 PM) nitay: u want to open up a JIRA?
(2:22:57 PM) posix4e: but if i could tell hbase.site to just use my hostname:port it would work ok
(2:22:58 PM) posix4e: yea
(2:23:09 PM) posix4e: can i quote this conversation?
(2:23:18 PM) nitay: yes please do
(2:23:45 PM) nitay: also, to fix this here and now for u, u'd essentially need to actually set hbase.master* to the ip/host u're using
(2:23:55 PM) nitay: and change it on each backup master to that guy's host/ip
(2:24:02 PM) nitay: i know, its a royal PITA
(2:24:59 PM) posix4e: yea
(2:25:03 PM) posix4e: no problem
(2:25:20 PM) nitay: but that should work till we find a better solution
(2:25:21 PM) posix4e: I am trying to think how a patch would work
(2:25:25 PM) posix4e: have a masters file?
(2:25:44 PM) nitay: yeah if u have any ideas please offer them
(2:25:46 PM) nitay: hmm interesting idea
(2:26:16 PM) nitay: and then do some local gethostbyname() type thing checking against masters file?
(2:26:26 PM) posix4e: yea
(2:28:23 PM) nitay: one thing to note is we've talked about eventually getting to a place where any RS can be master
(2:28:30 PM) nitay: but i like your idea
(2:28:37 PM) nitay: post it on the JIRA
(2:30:24 PM) nitay: i gotta run, thanks for the info posix4e - very helpful, its great to hear from people actually using this stuff
(2:32:56 PM) posix4e: yep

I also solved this by manually setting the hbase.master  on each host to point to the local hostname, which sucks.",All,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1445,,,HBASE-1448,"28/May/09 15:45;jdcryans;hbase-1357-v1.patch;https://issues.apache.org/jira/secure/attachment/12409276/hbase-1357-v1.patch","01/Jun/09 17:23;jdcryans;hbase-1357-v2.patch;https://issues.apache.org/jira/secure/attachment/12409573/hbase-1357-v2.patch","01/Jun/09 20:01;jdcryans;hbase-1357-v3.patch;https://issues.apache.org/jira/secure/attachment/12409590/hbase-1357-v3.patch","02/Jun/09 15:29;jdcryans;hbase-1357-v4.patch;https://issues.apache.org/jira/secure/attachment/12409674/hbase-1357-v4.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25712,Incompatible change,Reviewed,,,Tue Jun 02 17:54:25 UTC 2009,,,,,,,,,,"0|i0hcrr:",99331,"hbase.master and hbase.master.hostname are now obsolete. hbase.cluster.distributed must be set at ""true"" to have a fully-distributed setup along with at least one configured ZK server which is not pointing at localhost.
Also, zoo.cfg must be in the classpath of every client.",,,,,,,,,,,,,,,,,,,,"30/Apr/09 23:41;nitay;Moving out of 0.20 for now. In meeting we discussed:

- Getting rid of hbase.master.hostname property completely. Detect master address using an inet address detection scheme similar to what Hadoop does.
- The above breaks the default ZooKeeper case of a single server running on the master node. RegionServers currently find this ZooKeeper using the master address property. This default case should be replaced by having the zoo.cfg turn into a template file which can be created and rsync'ed to RegionServer nodes by e.g. start-hbase.sh script.
- Add a masters file, similar to regionservers, with pool of servers for master election.;;;","21/May/09 22:04;stack;Boys chatting on #hbase today figure we should do this -- just remove master address and do lookup of host and write that to ZK.  Bringing it back into 0.20.0.  Nitay says assign it to him.;;;","21/May/09 22:27;posix4e;We use the same script which restarts all of the zookeeper , thrift ,
master , region and rsync nodes to rewrite the hbase master address.
It's hacky but it works. The problem is the zookeeper node. I think
the way you do that in zookeeper is with a sequence and a watch (I
think). I would code this up but I am in a car on my handy. If no one
has it fixed by Tuesday I will talk to Nitay about it.


-- 
Sent from my mobile device
;;;","21/May/09 23:22;stack;Chatting w/ Nitay, he recalled that reason this issue was punted to 0.21 was because that in distributed hbase -- not standalone nor pseudo-distributed -- then clients and regionservers need to know where the zookeeper quorum is.  This means edit of zoo.cfg WHEN YOU WANT TO RUN IN DISTRIBUTED MODE.  Chatting w/ Nitay, I thought we could continue hiding ZK from noobs by doing something like adding a new property in hbase-site.xml named zookeeper.quorum and in it we'd list all quorum members and then in background we'd write the zoo.cfg but Nitay just raised his eyebrow when i suggested this.

So, I'm with him now.  Lets not beat around the bush.  When doing distributed mode, then you need to edit the ZK config.  Will reinforce that ZK is cluster mediator.  I think its fine.  The two basic modes out of the box will just work w/o zoo.cfg edits.  What you think J-D?;;;","22/May/09 14:34;jdcryans;Ok let's do it like that. One thing that we should do tho is to have a more detailed ""How to run distributed mode"" documentation.;;;","26/May/09 18:44;jdcryans;Nitay agreed to let me work on the problem since it's related to HBASE-1445.;;;","26/May/09 19:10;jdcryans;Chatting with Nitay, we figured it would be best to just have a ""cluster mode"" configuration that tells HBase whether it's local or distributed instead of the master address since it won't be needed anymore.;;;","28/May/09 15:45;jdcryans;First rough cut. There is no documentation tho the Getting Started will have to change a bit.

I removed hbase.master.hostname and added a cluster mode. ZK complains and exits if mode is distributed but it's still managed by HBase like this:

{quote}
2009-05-28 11:34:58,288 FATAL org.apache.hadoop.hbase.zookeeper.HQuorumPeer: Zookeeper should be managed only in a local cluster mode. Please edit conf/zoo.cfg and remove hbase.cluster.mode to set your ZK quorum addresses
{quote}

It passes test tho, and I tried PE in standalone, pseudo and fully distributed. All works.;;;","30/May/09 22:01;stack;The choice of configuration name and setting is important since it'll be one of the first things presented a user.

hbase.cluster.mode with 'local' | 'distributed' values seems a little awkward to me (and error-prone).

What about hbase.cluster.distributed with a yes/no or true/false answer (I think Configuration will do right thing whatever you provide).

If 'false', then hbase cluster is not distributed across a cluster of machines -- its standalone or pseudo-distributed.

The description in hbase-default needs to be a little clearer.  Also, I'm not too clear on what happens.  If distributed, it means user needs to edit zoo.cfg to point at quorum?  Where is the switch for whether or not hbase starts up the quorum?

Otherwise, patch looks great.  I like --master option to PE.  I'd also remove the commented out code in HConstants.



;;;","01/Jun/09 17:23;jdcryans;In this patch I changed hbase.cluster.mode to hbase.cluster.distributed with options false|true. I also added new documentation in overview.html to explain the new steps for the fully-distributed setup.;;;","01/Jun/09 20:01;jdcryans;Patch adds some clarification in the doc, adds hbase.master.dns.nameserver and hbase.master.dns.interface to specify which interface the master should use to communicate with which DNS (just like in hadoop).;;;","01/Jun/09 21:39;stack;I still get this:

{code}
2009-06-01 21:29:19,071 ERROR org.apache.hadoop.hbase.master.HMaster: Can not start master
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
    at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1090)
    at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1127)
Caused by: java.io.IOException: Could not read quorum servers from zoo.cfg
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.<init>(ZooKeeperWrapper.java:90)
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.<init>(ZooKeeperWrapper.java:78)
    at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:245)
    ... 6 more
{code}

Its because I still have this in my zoo.cfg:

{code}
server.0=${hbase.cluster.distributed}:2888:3888
{code}

Its odd having a variable whose setttings are true/false in this position.

It should be 'hbase.hostname', something we calculate for you when in local/pseudo mode putting in place hostname unless its overridden.

Exception needs to be better.

We can't put localhost here?   If hbase.cluster.distributed is true and this zoo.cfg is localhost then throw exception?  Would that work?;;;","02/Jun/09 15:29;jdcryans;This latest version of the patch clears up hbase.cluster.distributed from zoo.cfg like Stack described. This will be thrown when the hbase.cluster.distributed is true and the value in zoo.cfg is localhost:

{quote}
localhost: starting zookeeper, logging to /home/jdcryans/svn/hbase/trunk/bin/../logs/hbase-jdcryans-zookeeper-jdcryans.mtl.out
localhost: java.io.IOException: The server in zoo.cfg cannot be set to localhost in a fully-distributed setup because it won't be reachable. See ""Getting Started"" for more information.
localhost: 	at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.parseConfig(HQuorumPeer.java:141)
localhost: 	at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.parseZooKeeperConfig(HQuorumPeer.java:82)
localhost: 	at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.main(HQuorumPeer.java:58)
{quote}

I tested master failover on 3 nodes doing hbase-daemons.sh start master then regionserver (which is kinda fun) and killed the first, then the second master. What I first saw is all regions getting reassigned (which was supposed to be fix) but this was because of the alls well messages:
{quote}
2009-06-02 11:02:19,941 DEBUG org.apache.hadoop.hbase.master.HMaster: Started service threads
2009-06-02 11:02:19,942 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 9 on 62000: starting
2009-06-02 11:02:19,956 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.1.88:62020, regionname: -ROOT-,,0, startKey: <>} complete
2009-06-02 11:02:20,144 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 54 row(s) of meta region {server: 192.168.1.87:62020, regionname: .META.,,1, startKey: <>} complete
2009-06-02 11:02:20,939 DEBUG org.apache.hadoop.hbase.master.ServerManager: Process all wells: address: 192.168.1.88:62020, startcode: 1243954293421, load: (requests=4, regions=19, usedHeap=27, maxHeap=963) openingCount: 0, nobalancingCount: 4
2009-06-02 11:02:20,945 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Wrote out of safe mode
2009-06-02 11:02:20,945 INFO org.apache.hadoop.hbase.master.RegionManager: exiting safe mode
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server is overloaded. Server load: 19 avg: 6.333333333333333, slop: 0.1
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 12 regions. mostLoadedRegions has 10 regions in it.
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0003047546,1242765850701
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0011562998,1241636821854
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0006799914,1242765888384
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0006349363,1242765888384
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0001399792,1242840206758
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0000072704,1242764140942
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0008901015,1242765794486
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0002902745,1242765697441
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0000283600,1242764792449
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0000612479,1242394901854
2009-06-02 11:02:20,955 INFO org.apache.hadoop.hbase.master.RegionManager: Skipped 0 region(s) that are in transition states
2009-06-02 11:02:21,164 DEBUG org.apache.hadoop.hbase.master.ServerManager: Process all wells: address: 192.168.1.87:62020, startcode: 1243954293575, load: (requests=57, regions=19, usedHeap=26, maxHeap=963) openingCount: 0, nobalancingCount: 4
2009-06-02 11:02:21,165 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server is overloaded. Server load: 19 avg: 12.666666666666666, slop: 0.1
2009-06-02 11:02:21,165 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 6 regions. mostLoadedRegions has 10 regions in it.
{quote}

That's because the load is empty when adding a new region server so I added a check to instead use the load provided by the RS during the failover inspection. So it's fixed.;;;","02/Jun/09 16:55;stack;+1.  Commit it.

I tested it.  Makes sense.  Doc. will need rework but lets get all the other bits in there first before we do that.;;;","02/Jun/09 17:54;jdcryans;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incrementColumnValue/Memcache bug,HBASE-1352,12423948,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,atppp,atppp,27/Apr/09 22:36,13/Sep/09 22:31,01/Jul/25 07:49,03/May/09 21:27,0.19.1,,,,,0.19.2,,,regionserver,,,,0,"When I do a large amount of incrementColumnValue(), I got NullPointerException thrown from Memcache.java:

2009-04-27 14:59:03,620 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 1 on 59710, call incrementColumnValue([B@1317bfb, [B@1c2a074, [B@b427c1, 1) from 127.0.0.1:52296: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.Memcache.get(Memcache.java:609)
        at org.apache.hadoop.hbase.regionserver.Memcache.get(Memcache.java:244)
        at org.apache.hadoop.hbase.regionserver.Memcache.get(Memcache.java:226)
        at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:2636)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2299)
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:912)
(called from thrift interface)

I consistently hit this exception after restarting Hbase (exactly the same location in my code)

Quick fix is (with HBASE-1319.patch applied) at Memcache.java:609, say if (deletes != null) deletes.add(itKey);

but probably better to change Memcache.java:226. Also the trunk already changed the StoreKey format and doesn't have this bug.",Ubuntu 8.10,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/09 17:07;atppp;HBASE-1352.patch;https://issues.apache.org/jira/secure/attachment/12406664/HBASE-1352.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25709,,,,,Sun May 03 21:27:42 UTC 2009,,,,,,,,,,"0|i0hcqv:",99327,,,,,,,,,,,,,,,,,,,,,"28/Apr/09 08:02;stack;Attach a patch and I'll apply it.  Thanks.;;;","29/Apr/09 00:32;stack;How about this instead of creating new Map:

{code}
Index: src/java/org/apache/hadoop/hbase/regionserver/Memcache.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/Memcache.java (revision 768432)
+++ src/java/org/apache/hadoop/hbase/regionserver/Memcache.java (working copy)
@@ -606,7 +606,7 @@
           }
         } else {
           // Cell holds a delete value.
-          deletes.add(itKey);
+          if (deletes != null) deletes.add(itKey);
         }
       } else {
         // By L.N. HBASE-684, map is sorted, so we can't find match any more.
{code};;;","29/Apr/09 00:41;atppp;That fixes the problem. In fact this was what I proposed initially too and tested.

But I was worried that extra checking slows down other calls, also the trunk code creates new Map too when deletes is not passed. That's why I changed line 226 instead in the uploaded patch.

Either way is fine.... I prefer stack's fix because it feels safer...;;;","03/May/09 21:27;stack;Committed above test for null.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Splitting up the compare of family+column into 2 different compares,HBASE-1336,12423508,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,erikholstad@gmail.com,erikholstad@gmail.com,22/Apr/09 20:31,13/Sep/09 22:24,01/Jul/25 07:49,06/May/09 00:37,0.20.0,,,,,0.20.0,,,,,,,0,"When comparing family+column you can end up in  a situation like column1 is ""abcd:efg"" and column2 is ""abc:defg"" which in the current implementation of
KeyValue.KeyComparator.compare will result in a faulty result.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/09 00:23;stack;1336-v2.patch;https://issues.apache.org/jira/secure/attachment/12407301/1336-v2.patch","01/May/09 23:23;stack;1336.patch;https://issues.apache.org/jira/secure/attachment/12407048/1336.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25701,,,,,Wed May 06 00:23:08 UTC 2009,,,,,,,,,,"0|i0hcnr:",99313,,,,,,,,,,,,,,,,,,,,,"22/Apr/09 20:36;erikholstad@gmail.com;The way I see it we can do this in two different ways, first one is to compare the family length first and then the whole (family+columns) byte[] or we can compare the 
family first and then the column. I have used the second approach for all the places that I'm comparing in the new server implementation. The first approach might be better
if you can actually do memcompare on byte[]. I our case I don't think it will matter, since we still need to compare byte by byte. ;;;","01/May/09 23:23;stack;Patch that does column compare in two parts -- family then qualifier.   Includes unit test.

Not applying yet.  Not all tests passing.;;;","06/May/09 00:23;stack;Fix a couple of bugs in first patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.META. region running into hfile errors,HBASE-1334,12423407,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,21/Apr/09 20:45,13/Sep/09 22:24,01/Jul/25 07:49,21/Apr/09 23:53,0.20.0,,,,,0.20.0,,,,,,,0,"my .META. region refuses to do anything, I get this snippet of a error in the log file:

Caused by: java.lang.IllegalArgumentException at 
                java.nio.Buffer.position(Buffer.java:236) at 
org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.blockSeek(HFile.java:1121)

Looks like the seek code is breaking somehow - seeking before the beginning of the block maybe?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/09 20:46;ryanobjc;HBASE-1334.patch;https://issues.apache.org/jira/secure/attachment/12406055/HBASE-1334.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25699,,,,,Tue Apr 21 23:53:56 UTC 2009,,,,,,,,,,"0|i0hcnb:",99311,,,,,,,,,,,,,,,,,,,,,"21/Apr/09 20:46;ryanobjc;when we changed the comparators to take k,offset,len, we forgot to update this one!;;;","21/Apr/09 23:53;stack;Committed.  Thanks Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"regionserver carrying .META. starts sucking all cpu, drives load up - infinite loop?",HBASE-1332,12423347,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,21/Apr/09 07:37,13/Sep/09 22:24,01/Jul/25 07:49,21/Apr/09 08:14,0.20.0,,,,,0.20.0,,,,,,,0,"the symptom is the cluster comes to a dead halt.  Lookups on meta don't seem to work, and the regionserver carrying .META. goes hot - using 800% CPU or more, driving system LA up really really high (I've seen it as high as 26).  Thread dumps seem to indicate every IPC handler is stuck in Bytes.binarySearch().

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/09 07:39;ryanobjc;HBASE-1332.patch;https://issues.apache.org/jira/secure/attachment/12406007/HBASE-1332.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25697,,,,,Tue Apr 21 08:14:28 UTC 2009,,,,,,,,,,"0|i0hcmv:",99309,,,,,,,,,,,,,,,,,,,,,"21/Apr/09 07:39;ryanobjc;It turns out that if the .META. has a hfile that doesnt start at ',,' (aka: first row in the table), we end up with an infinite loop.  As you can see, in this function, we 'continue' and never finish this function.;;;","21/Apr/09 08:14;stack;Committed.  Thanks for the patch Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
binary keys broken on trunk,HBASE-1330,12423238,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,20/Apr/09 06:09,13/Sep/09 22:24,01/Jul/25 07:49,20/Apr/09 07:11,0.20.0,,,,,0.20.0,,,,,,,0,"The symptom is commits fail with 'table not found' exception - even though the table does in fact exist!

Digging in a little with debug logs indicate that getClosestRowBefore() is returning NULL, which for a table that exists should never be!  A key always falls into a region - either the first or the last one at the very least.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/09 06:11;ryanobjc;HBASE-1330.patch;https://issues.apache.org/jira/secure/attachment/12405898/HBASE-1330.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25696,,,,,Mon Apr 20 07:11:17 UTC 2009,,,,,,,,,,"0|i0hcmf:",99307,,,,,,,,,,,,,,,,,,,,,"20/Apr/09 06:11;ryanobjc;turns out the clone of a comparator loses the 'meta-ness' or 'root-ness'.  ;;;","20/Apr/09 06:13;ryanobjc;this is a typical META bug - using the normal Bytes comparator to compare the 'row' section causes problems when the first byte is '\0'.  So if you have keys that start with a value < ',', most common in the binary key case (when the first byte is '\0'), you end up with incorrect storing saying things like:

table,,1234  > table,\0,1234

which is not correct.  since [] < '\0'.
;;;","20/Apr/09 07:07;stack;+1 on commit after chatting with Ryan on the posted patch.;;;","20/Apr/09 07:11;stack;Thanks for fixing ugly issue Ryan.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in atomicIncrement 0.19.1 only,HBASE-1319,12422538,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,10/Apr/09 00:47,13/Sep/09 22:31,01/Jul/25 07:49,23/Apr/09 04:37,0.19.1,,,,,0.19.2,,,,,,,0,"this patch was appended to 803, but not applied into 0.19.1 before it's release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/09 00:48;ryanobjc;HBASE-1319.patch;https://issues.apache.org/jira/secure/attachment/12405127/HBASE-1319.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25687,,,,,Thu Apr 23 04:37:13 UTC 2009,,,,,,,,,,"0|i0hckf:",99298,,,,,,,,,,,,,,,,,,,,,"23/Apr/09 04:37;stack;Committed to 0.19 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift server doesnt know about atomicIncrement,HBASE-1318,12422537,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tim_s,ryanobjc,ryanobjc,10/Apr/09 00:42,13/Sep/09 22:24,01/Jul/25 07:49,03/May/09 17:00,0.19.1,0.20.0,,,,0.19.2,0.20.0,,,,,,0,the thrift server needs the atomicIncrement API implemented,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/09 23:31;ryanobjc;HBASE-1318-191.patch;https://issues.apache.org/jira/secure/attachment/12407053/HBASE-1318-191.patch","01/May/09 23:30;ryanobjc;HBASE-1318-trunk.patch;https://issues.apache.org/jira/secure/attachment/12407052/HBASE-1318-trunk.patch","10/Apr/09 00:46;ryanobjc;HBASE-1318.patch;https://issues.apache.org/jira/secure/attachment/12405126/HBASE-1318.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25686,,,,,Sun May 03 17:00:41 UTC 2009,,,,,,,,,,"0|i0hck7:",99297,,,,,,,,,,,,,,,,,,,,,"23/Apr/09 04:09;stack;Ryan, what version of thrift did you use generating code?;;;","23/Apr/09 04:16;stack;What it this: 20080411p1?;;;","23/Apr/09 04:40;ryanobjc;Yes exactly.


;;;","23/Apr/09 04:47;stack;04:41 < St^Ack> dj_ryan: on 1318, was it the posted version of thrift?
04:42 < dj_ryan> yes
;;;","23/Apr/09 04:49;stack;I applied patch to trunk and got this:
{code}
compile:
    [javac] Compiling 284 source files to /home/stack/checkouts/hbase/trunk/build/classes
    [javac] /home/stack/checkouts/hbase/trunk/src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java:415: cannot find symbol
    [javac] symbol  : method incrementColumnValue(byte[],byte[],long)
    [javac] location: class org.apache.hadoop.hbase.client.HTable
    [javac]         return table.incrementColumnValue(row, column, amount);
    [javac]                     ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error
{code};;;","30/Apr/09 23:02;ryanobjc;Tim, do you mind giving this a shot?

Thanks!;;;","01/May/09 09:03;tim_s;Sure thing;;;","01/May/09 23:29;ryanobjc;this builds against 0.19.1 (a SVN checkout I have).  I'm running tests, but I dont know why it doesn't work.;;;","01/May/09 23:30;ryanobjc;this one compiles on a trunk version.;;;","01/May/09 23:31;ryanobjc;this version compiles against 0.19.1 in my own SVN checkout.;;;","03/May/09 17:00;stack;Committed to branch and trunk (I presume the 'right' version of thrift was used generating classes).  Thanks for the patch Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HFile rejects key in Memcache with empty value,HBASE-1309,12422060,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,apurtell,apurtell,05/Apr/09 02:18,13/Sep/09 22:24,01/Jul/25 07:49,07/Apr/09 18:23,0.20.0,,,,,0.20.0,,,,,,,0,"2009-04-05 02:12:56,497 FATAL org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: content,,1238896745127
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:878)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:771)
	at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:229)
	at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.run(MemcacheFlusher.java:139)
Caused by: java.io.IOException: Value cannot be null or empty
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkValue(HFile.java:485)
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:447)
	at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:501)
	at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:463)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:863)
	... 3 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/09 19:35;apurtell;HBASE-1309.patch;https://issues.apache.org/jira/secure/attachment/12404677/HBASE-1309.patch","05/Apr/09 02:19;apurtell;hbase-hadoop-regionserver-nashua.zip;https://issues.apache.org/jira/secure/attachment/12404659/hbase-hadoop-regionserver-nashua.zip",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25678,,,,,Tue Apr 07 18:22:49 UTC 2009,,,,,,,,,,"0|i0hcif:",99289,,,,,,,,,,,,,,,,,,,,,"05/Apr/09 03:30;apurtell;Changed issue title after some initial digging in the code. 

Null values is one thing. Should empty values just be ignored rather than cause exceptions? Actually empty values are valid also: Apps may only want to test for presence of a key.;;;","05/Apr/09 19:25;apurtell;From Ryan on hbase-dev@:

> I think it might be legit to store an empty value,
> empty key = not gonna happen.
>
> but if the key has enough data in it, eg: a delete tombstone, the value
> might be not interesting, so why store a placeholder?
>
> what particular circumstance is this happening under?  I dont want to 'fix'
> hfile by allowing empty/null values only to mask a different error that you
> are getting...;;;","05/Apr/09 19:29;apurtell;In my opinion HBase should not dictate to users what they can or cannot store into the system.

I did not say anything about empty keys, which of course makes no sense. 

The particular circumstance in this place is a crawling application that stores content indexed by SHA-1 hash but also includes a family 'url:' which stores, for each unique object according to hash, the list of URLs where it was encountered. The URLs are stored as column qualifiers to produce a list of unique values with no need for scanning for duplicate elimination etc. There is no need to store data also because all the data is already in the key. ;;;","05/Apr/09 19:31;apurtell;A more basic case is booleans as existence tests on keys. ;;;","06/Apr/09 21:10;apurtell;If there are no objections I'm going to apply the patch attached to this issue.;;;","07/Apr/09 18:22;apurtell;Committed to trunk. Passes all local tests. Tested under concurrent write/scan load.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index configuration prevents HBase from starting,HBASE-1303,12421732,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,kweiner,kweiner,01/Apr/09 01:39,13/Sep/09 22:24,01/Jul/25 07:49,02/Apr/09 07:33,0.19.1,,,,,0.19.2,0.20.0,,regionserver,,,,0,"HBase does not start up when configured to use the IndexedRegionServer with the following properties in hbase-site.xml

{code:xml}
 <property>
     <name>hbase.regionserver.class</name>
     <value>org.apache.hadoop.hbase.ipc.IndexedRegionInterface</value>
     <description>Indexing is enabled for this hbase server.  </description>
 </property>
 <property>
    <name>hbase.regionserver.impl</name>
    <value>org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer</value>
    <description>Indexing is enabled for this hbase server.</description>
</property>
{code}

This results in the following exception in the log:
{noformat}
2009-03-31 12:33:35,993 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 8 on 45026, call getProtocolVersion(org.apache.hadoop.hbase.ipc.IndexedRegionInterface, 16) from 127.0.0.1:60854: error: java.io.IOException: Unknown protocol to name node: org.apache.hadoop.hbase.ipc.IndexedRegionInterface
java.io.IOException: Unknown protocol to name node: org.apache.hadoop.hbase.ipc.IndexedRegionInterface
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getProtocolVersion(HRegionServer.java:2146)
    at org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.getProtocolVersion(TransactionalRegionServer.java:92)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:912)
2009-03-31 12:33:35,994 WARN org.apache.hadoop.hbase.master.BaseScanner: Scan ROOT region
java.io.IOException: java.io.IOException: Unknown protocol to name node: org.apache.hadoop.hbase.ipc.IndexedRegionInterface
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getProtocolVersion(HRegionServer.java:2146)
    at org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.getProtocolVersion(TransactionalRegionServer.java:92)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:912)
{noformat}

There is also a mailing list post on this problem:
http://markmail.org/message/6pugle5uegiijjbc?q=Secondary+Indexes+problem

I think the solution is to implement {{public long getProtocolVersion(final String protocol, final long clientVersion)}} in {{org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer}} as follows:

{code:java}
  @Override
  public long getProtocolVersion(final String protocol, final long clientVersion)
      throws IOException {
    if (protocol.equals(IndexedRegionInterface.class.getName())) {
      return HBaseRPCProtocolVersion.versionID;
    }
    return super.getProtocolVersion(protocol, clientVersion);
  }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/09 22:57;kweiner;HBASE-1303_0.19.1.patch;https://issues.apache.org/jira/secure/attachment/12404400/HBASE-1303_0.19.1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25676,,,,,Thu Apr 02 07:33:22 UTC 2009,,,,,,,,,,"0|i0hchr:",99286,,,,,,,,,,,,,,,,,,,,,"01/Apr/09 01:40;kweiner;Index: src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegionServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegionServer.java	(revision 760746)
+++ src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegionServer.java	(working copy)
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion;
 import org.apache.hadoop.hbase.ipc.IndexedRegionInterface;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer;
@@ -48,6 +49,15 @@
   }
 
   @Override
+  public long getProtocolVersion(final String protocol, final long clientVersion)
+      throws IOException {
+    if (protocol.equals(IndexedRegionInterface.class.getName())) {
+      return HBaseRPCProtocolVersion.versionID;
+    }
+    return super.getProtocolVersion(protocol, clientVersion);
+  }
+
+  @Override
   protected HRegion instantiateRegion(final HRegionInfo regionInfo)
       throws IOException {
     HRegion r = new IndexedRegion(HTableDescriptor.getTableDir(super
;;;","01/Apr/09 02:11;nitay;+1 looks right to me Ken. 

Please upload a patch with your fix instead of adding a comment with it. See http://wiki.apache.org/hadoop/Hbase/HowToContribute for more information.;;;","01/Apr/09 22:57;kweiner;Please review my attached patch.;;;","02/Apr/09 07:33;stack;Committed branch and trunk.  Thanks for the patch Ken.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable.getRow() returns null if the row does no exist,HBASE-1301,12421637,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,rafan,rafan,rafan,31/Mar/09 10:10,23/Mar/11 17:13,01/Jul/25 07:49,23/Apr/09 16:20,0.19.0,0.19.1,,,,0.19.2,0.20.0,,Client,documentation,,,0,"The HBase API docs says when the row does not exist, getRow() returns

    RowResult is empty if row does not exist. 

However, in regionserver/HRegionServer.java's getRow():

      if (result == null || result.isEmpty())
        return null;
      return new RowResult(row, result);

It actually returns null. Either fix the code or the document.",,,,,,,,,,,,,,,,,,,,,,,HBASE-1292,HBASE-1837,HBASE-1028,,,,"04/Apr/09 07:22;rafan;1301.patch;https://issues.apache.org/jira/secure/attachment/12404624/1301.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25675,,,,,Thu Apr 23 16:20:56 UTC 2009,,,,,,,,,,"0|i0hchb:",99284,Fix API doc of HTable.getRow (non-existent row key returns null instead of empty RowResult) ,,,,,,,,,,,,,,,,,,,,"31/Mar/09 10:10;rafan;From the doc, I would expect we got a valid RowResult with RowResult.isEmpty() is true for non-existent row.;;;","31/Mar/09 18:01;apurtell;I remember we accepted a patch that changed getRow to return a null if no row exists, so the documentation is what is in error in my opinion. ;;;","31/Mar/09 21:56;stack;Thanks Andrew for the kick.  Looking at code, I see this change that went in:

{code}
$ svn diff -r722690:722707 src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 
Index: src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java    (revision 722690)
+++ src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java    (revision 722707)
@@ -1287,7 +1287,8 @@
   }
 
   public RowResult getRow(final byte [] regionName, final byte [] row, 
-    final byte [][] columns, final long ts, final long lockId)
+    final byte [][] columns, final long ts,
+    final int numVersions, final long lockId)
   throws IOException {
     checkOpen();
     requestCount.incrementAndGet();
@@ -1300,13 +1301,11 @@
       }
       
       HRegion region = getRegion(regionName);
-      Map<byte [], Cell> map = region.getFull(row, columnSet, ts,
-          getLockFromId(lockId));
-      if (map == null || map.isEmpty())
-        return null;
       HbaseMapWritable<byte [], Cell> result =
-        new HbaseMapWritable<byte [], Cell>();
-      result.putAll(map);
+        region.getFull(row, columnSet, 
+          ts, numVersions, getLockFromId(lockId));
+      if (result == null || result.isEmpty())
+        return null;
       return new RowResult(row, result);
     } catch (IOException e) {
       checkOOME(e);
{code}

The commit mentions ""HBASE-847 new API: HTable.getRow with numVersion specified"" but that doesn't seem to be the right issue.  This seems to be it HBASE-1028.

So, yeah, seems like its the doc thats wrong.;;;","04/Apr/09 07:22;rafan;Fix javadoc;;;","23/Apr/09 16:20;stack;Committed branch and trunk.

Thanks for the patch Rong-en.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JSPs don't HTML escape literals (ie: table names, region names, start & end keys)",HBASE-1299,12421574,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ndimiduk,hossman,hossman,30/Mar/09 17:52,07/Jul/14 07:26,01/Jul/25 07:49,03/Dec/12 19:40,0.19.0,0.19.1,,,,0.95.0,,,,,,,1,"similar to HBASE-1298, the various JSPs included with HBase for monitoring the system don't seem to do any HTML escaping when displaying user entered data which may contain special characters: table names, region names, start Keys, or end Keys",,,,,,,,,,,,,,,,,HBASE-5968,,,,,,HBASE-11356,,,,,,"01/Dec/12 00:45;ndimiduk;1299.patch;https://issues.apache.org/jira/secure/attachment/12555597/1299.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25674,Reviewed,,,,Mon Sep 23 18:30:21 UTC 2013,,,,,,,,,,"0|i02dnr:",11814,,,,,,,,,,,,noob,,,,,,,,,"04/Apr/09 18:31;larsgeorge;I would have done this too while working on HBASE-1298 but I am not sure if there is nowadays an equivalent to the URLEncode class that does HTML codepoint conversions. Or maybe there is one somewhere already in the project in another library? Do you know?

If not this seems not to warrant adding for example Commons Lang:
  http://commons.apache.org/lang/api-release/org/apache/commons/lang/StringEscapeUtils.html

We could simply add a local helper that does the encoding, but I would like to know first from the boss if that is advisable or what the general approach to this is. Please advise.;;;","04/Apr/09 22:53;hossman;it's been a while since i did anything with JSPs, but as i recall JSP 1.2 had a standard taglib for escaping variables when outputing them.

i would suggest that it might be worth while to convert all the JSPs to JSP 2.0 (aka: *.jspx) where escaping variables on output is the default, because both the templates and the output are garunteed to be wellformed XML (or xhtml if that's what you're goal is);;;","05/Oct/10 22:21;ryanobjc;with the binary escaping this issue is much abated, since by default we only get 0-9A-Za-z and a few others (eg: -_ etc).

;;;","16/Oct/10 06:01;stack;Moving out.;;;","01/Dec/12 00:44;ndimiduk;This is a hacky kind of fix to the existing JSP pages. The proper solution should be the replacement of these pages with jamon templates, a la MasterStatus. Thoughts?;;;","01/Dec/12 00:48;ndimiduk;steps to repro:

> create 't1', {NAME => 'f1'}
> put 't1', ""<script>alert('hello world');</script>"", 'f1:foo', 0
> split 't1', ""<script>alert('hello world');</script>""

open http://localhost:60010/table.jsp?name=t1

alerts will pop.
;;;","03/Dec/12 17:53;sershe;+1 on patch;;;","03/Dec/12 19:35;eclark;+1;;;","03/Dec/12 19:40;stack;Committed to trunk (tried to commit to 0.94 but failed... so passing for now).

Thanks for the patch Nick.;;;","03/Dec/12 19:55;ndimiduk;Fails 0.94 because these files moved on trunk. `git checkout 0.94 && cd src/main/resources/hbase-webapps/master && patch` < 1299.patch may work.;;;","03/Dec/12 20:38;hudson;Integrated in HBase-TRUNK #3587 (See [https://builds.apache.org/job/HBase-TRUNK/3587/])
    HBASE-1299 JSPs don't HTML escape literals (ie: table names, region names, start & end keys) (Revision 1416645)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
* /hbase/trunk/hbase-server/src/main/resources/hbase-webapps/master/tablesDetailed.jsp
;;;","04/Dec/12 00:02;hudson;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #284 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/284/])
    HBASE-1299 JSPs don't HTML escape literals (ie: table names, region names, start & end keys) (Revision 1416645)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
* /hbase/trunk/hbase-server/src/main/resources/hbase-webapps/master/tablesDetailed.jsp
;;;","23/Sep/13 18:30;stack;Marking closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
master.jsp & table.jsp do not URI Encode table or region names in links,HBASE-1298,12421573,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,larsgeorge,hossman,hossman,30/Mar/09 17:43,13/Sep/09 22:24,01/Jul/25 07:49,03/Apr/09 14:18,0.19.0,0.19.1,,,,0.20.0,,,,,,,1,"""UAZAAAAAZNaGnEKI+gC"" is a key in my ""userdata"" table which happens to be the start key for a region named ""userdata,UAZAAAAAZNaGnEKI+gC,1238170268268""

""/table.jsp?name=userdata"" lists a link to ""/regionhistorian.jsp?regionname=userdata,UAZAAAAAZNaGnEKI+gC,1238170268268"" which is incorrect because the "" + "" character is not properly URI Encoded.  This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. ""  manually escaping the "" + "" character as ""%2B"" produces the correct output.

A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp
","HBase Version	0.19.0, r735381
Hadoop Version	0.19.0, r713890",,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/09 10:40;larsgeorge;1298.patch;https://issues.apache.org/jira/secure/attachment/12404522/1298.patch","03/Apr/09 10:43;larsgeorge;regionhistorian.jsp.jpg;https://issues.apache.org/jira/secure/attachment/12404524/regionhistorian.jsp.jpg","03/Apr/09 10:41;larsgeorge;table.jsp.jpg;https://issues.apache.org/jira/secure/attachment/12404523/table.jsp.jpg",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25673,,,,,Fri Apr 03 14:18:40 UTC 2009,,,,,,,,,,"0|i0hch3:",99283,,,,,,,,,,,,,,,,,,,,,"30/Mar/09 17:46;hossman;(fixing wiki parsing in description);;;","30/Mar/09 17:49;hossman;Verified problem also exists in 0.19.1 and trunk.;;;","03/Apr/09 10:40;larsgeorge;Attached patch fixes href issues with non URL safe characters in region names.;;;","03/Apr/09 10:41;larsgeorge;Shows regions with unsafe characters.;;;","03/Apr/09 10:43;larsgeorge;Shows in the address bar how the region names are now encoded properly.;;;","03/Apr/09 14:18;stack;Thanks for the patch Lars.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hfile doesn't recycle decompressors,HBASE-1293,12419686,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,27/Mar/09 22:56,13/Sep/09 22:24,01/Jul/25 07:49,28/Mar/09 00:39,0.20.0,,,,,0.20.0,,,,,,,0,"The Compression codec stuff from hadoop has the concept of recycling compressors and decompressors - this is because a compression codec uses ""direct buffers"" which reside outside the JVM regular heap space.  There is a risk that under heavy concurrent load we could run out of that 'direct buffer' heap space in the JVM.

HFile does not call algorithm.returnDecompressor and returnCompressor.  We should fix that.


I found this bug via OOM crashes under jdk 1.7 - it appears to be partially due to the size of my cluster (200gb, 800 regions, 19 servers) and partially due to weaknesses in JVM 1.7.",- all -,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/09 22:58;ryanobjc;HBASE-1293.patch;https://issues.apache.org/jira/secure/attachment/12403853/HBASE-1293.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25671,Reviewed,,,,Sat Mar 28 00:39:06 UTC 2009,,,,,,,,,,"0|i0hcg7:",99279,,,,,,,,,,,,,,,,,,,,,"28/Mar/09 00:39;apurtell;Committed to trunk. Passes all local tests. Thanks for the patch Ryan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
php thrift's getRow() would throw an exception if the row does not exist,HBASE-1292,12419648,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,rafan,rafan,rafan,27/Mar/09 14:17,13/Sep/09 22:24,01/Jul/25 07:49,23/Apr/09 20:39,0.19.0,0.19.1,,,,0.19.2,0.20.0,,Thrift,,,,0,"I've been played with thrift recently, and observed an unexpected behavior: when getRow() encounters an non-existent row key, it throws an exception like this:

PHP Fatal error:  Uncaught exception 'Exception' with message 'getRow failed: unknown result' in pear/thrift/packages/Hbase/Hbase.php:715

Stack trace:
#0 pear/thrift/packages/Hbase/Hbase.php(666): HbaseClient->recv_getRow()
#1 htdocs/hbase/DemoClient.php(174): HbaseClient->getRow('demo_table', '00100-XXXX')
#2 {main} thrown in pear/thrift/packages/Hbase/Hbase.php on line 715

I would expect when we pass a non-existent key, it can throw something like NotFound (as in scanner) or one can test with RowResult.isEmpty() just like in java api.",,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1301,,,,"04/Apr/09 07:23;rafan;1292-0.19.patch;https://issues.apache.org/jira/secure/attachment/12404625/1292-0.19.patch","04/Apr/09 07:23;rafan;1292.patch;https://issues.apache.org/jira/secure/attachment/12404626/1292.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25670,Incompatible change,,,,Thu Apr 23 20:39:39 UTC 2009,,,,,,,,,,"0|i0hcfz:",99278,thrift's getRow*() now throws NotFound if the row key does not exist,,,,,,,,,,,,,,,,,,,,"30/Mar/09 11:01;stack;Agreed.  Do you have a patch Rong-en to fix this behavior?;;;","31/Mar/09 07:12;rafan;Nope, I just tried to patch but failed. Will see if I can produce one
sometime next week.
;;;","31/Mar/09 10:12;rafan;Depends on what we will have in java api, fix thrift according to it.;;;","04/Apr/09 07:23;rafan;Patch for 0.19 and trunk;;;","04/Apr/09 07:27;rafan;With the patch, it correctly throws NotFound exception.

{code}
HP Fatal error:  Uncaught exception 'NotFound' in pear/thrift/packages/Hbase/Hbase.php:4064
Stack trace:
#0 pear/thrift/packages/Hbase/Hbase.php(706): Hbase_getRow_result->read(Object(TBinaryProtocol))
#1 pear/thrift/packages/Hbase/Hbase.php(666): HbaseClient->recv_getRow()
#2 DemoClient.php(175): HbaseClient->getRow('demo_table', '00100-XXXX')
#3 {main} thrown in pear/thrift/packages/Hbase/Hbase.php on line 4064
{code};;;","04/Apr/09 21:34;ryanobjc;getRow on a non-existent row isnt really that exceptional - is there a way to return a null value instead?
;;;","05/Apr/09 04:46;rafan;From thrift mailing:

http://mail-archives.apache.org/mod_mbox/incubator-thrift-user/200806.mbox/%3C7e536b1f0806080534h183efef4t3f636c6101f6bb1b@mail.gmail.com%3E

It seems that thrift currently against using null as it may not be a native thing in some languages.

Moreover, in our thrift definition, we already uses NotFound as a signal for no value for a query and is used in methods like get() and scanner. So I think the best thing for now is to use NotFound to signal the caller.;;;","05/Apr/09 07:08;ryanobjc;Using exceptions instead of out of band null values and a simple if-else test is not ideal.  I'd like to see if at some point in the future we can eschew exceptions and only use them for real exceptional events.;;;","05/Apr/09 07:44;rafan;I agree.;;;","23/Apr/09 20:39;stack;Committed trunk and branch.  Thanks for the patch Rong-en.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table.jsp either 500s out or doesnt list the regions,HBASE-1290,12419425,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,25/Mar/09 21:28,13/Sep/09 22:24,01/Jul/25 07:49,26/Mar/09 01:14,0.20.0,,,,,0.20.0,,,,,,,0,"The table.jsp page either 500 errors out if you are viewing a .META. or -ROOT- table, or for user tables it doesn't list the regions.","jdk 1.7, linux",,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/09 21:37;ryanobjc;HBASE-1290.patch;https://issues.apache.org/jira/secure/attachment/12403642/HBASE-1290.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25668,Reviewed,,,,Fri Mar 27 06:14:42 UTC 2009,,,,,,,,,,"0|i0hcfj:",99276,,,,,,,,,,,,,,,,,,,,,"26/Mar/09 01:14;apurtell;Commited to trunk. Passes all local tests. Confirmed that table listings work. Thanks for the patch Ryan!;;;","27/Mar/09 06:14;jimk;+1 on patch. Nice work Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the way hostnames and IPs are handled,HBASE-1279,12417390,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,20/Mar/09 14:48,13/Sep/09 22:24,01/Jul/25 07:49,04/May/09 18:35,0.19.1,,,,,0.19.2,0.20.0,,,,,,0,"From the list by Yabo-Arber Xu,
{quote}
Yes, I've unlocked the port, and i am actually able to access from the web
UI with a client not running on EC2 to HBase at example.com:60010. It shows
all User Tables, but the Region Servers Address is the EC2 internal address:
domU-12-31-39-00-65-E5.compute-1.internal:60020.

I guess the client fails because it can not connect region server, which
serves only for an internal IP. However, in hbase-site.xml, I did configure
with region server explicitly in its external IP.

<property>
   <name>hbase.regionserver</name>
   <value>ec2-67-202-57-127.compute-1.amazonaws.com:60020</value>
   <description>The host and port a HBase region server runs at.
   </description>
 </property>
{quote}

In fact we completely bypass the hostname set in hbase.regionserver, also the hostnames in the web UI are not the good ones. We should do that part like hadoop does.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/09 14:50;jdcryans;hbase-1279.patch;https://issues.apache.org/jira/secure/attachment/12402669/hbase-1279.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25663,,,,,Mon May 04 18:35:01 UTC 2009,,,,,,,,,,"0|i0hcdb:",99266,"To tell HBase how to use your DNS configuration, set hbase.regionserver.dns.interface and hbase.regionserver.dns.nameserver in hbase-site.xml just like in hadoop-site.xml",,,,,,,,,,,,,,,,,,,,"20/Mar/09 14:50;jdcryans;Patch that fixes both issue. Will ask Arber to see if it helps him.;;;","22/Apr/09 19:00;stack;Whats the story on this one?  Did Arber ever get back to us?;;;","03/May/09 21:31;stack;I was going to make a 0.19.2 release J-D and then I saw this.  Do you want to include this in 0.19.2?;;;","04/May/09 11:42;jdcryans;Yes I'll commit it this afternoon.;;;","04/May/09 18:35;jdcryans;I tested my patch against 2 clusters at openplaces, one with dns problems and one with no problem at all. On the first by setting the new configs from hbase-default the hostnames were now correctly set and on the second one nothing was changed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HStoreKey: Wrong comparator logic,HBASE-1277,12417361,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apparition,apparition,apparition,20/Mar/09 09:42,13/Sep/09 22:24,01/Jul/25 07:49,20/Mar/09 09:55,0.19.1,,,,,0.20.0,,,build,regionserver,,,0,"During fixing fail of TestCompaction JUnit was found error in removing of row Cells. Reason was a error in comparator logic of HStoreKey.

Fix of HStoreKey also fixed removing of row Cell and and TestCompaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/09 09:46;apparition;HBASE-1277.patch;https://issues.apache.org/jira/secure/attachment/12402642/HBASE-1277.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25662,,,,,Fri Mar 20 09:55:10 UTC 2009,,,,,,,,,,"0|i0hcd3:",99265,,,,,,,,,,,,,,,,,,,,,"20/Mar/09 09:46;apparition;patch with solution and small optimisation ^^
thx for Nitay persistent :);;;","20/Mar/09 09:55;stack;Thanks for the patch Evgeny.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestTable.testCreateTable broken,HBASE-1275,12417359,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,20/Mar/09 09:03,13/Sep/09 22:24,01/Jul/25 07:49,20/Mar/09 09:55,0.20.0,,,,,0.20.0,,,,,,,0,"Test is broken, we seem to be able to create the same table 10x over.  ouch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/09 09:05;ryanobjc;HBASE-1275.patch;https://issues.apache.org/jira/secure/attachment/12402639/HBASE-1275.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25660,,,,,Fri Mar 20 09:55:56 UTC 2009,,,,,,,,,,"0|i0hccn:",99263,,,,,,,,,,,,,,,,,,,,,"20/Mar/09 09:05;ryanobjc;turns out that the meta comparator doesn't compare bytes when one side doesnt have a deliminator.  This is not normally expected, except in these rare cases fixed by the patch therein.  

Future:  If you search for a table in .META. you have to say: 'tableName,,' - don't forget the trailing double comma!;;;","20/Mar/09 09:55;stack;Thanks for the fix for the awkward bug Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
binary keys broken in trunk (again).,HBASE-1267,12417231,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,19/Mar/09 00:47,13/Sep/09 22:24,01/Jul/25 07:49,19/Mar/09 08:52,0.20.0,,,,,0.20.0,,,,,,,0,"Binary keys, specifically ones where the first byte of the key is nul '\0' don't work:

- Splits happen
- Logfile indicates everything normal

But the .META. doesnt list all the regions.  It only lists the 'basic' regions: 'table,,1234'.  The other regions with the binary keys in the middle just dont seem to be in .META....",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/09 01:36;ryanobjc;HBASE-1267-2.patch;https://issues.apache.org/jira/secure/attachment/12402540/HBASE-1267-2.patch","19/Mar/09 08:40;ryanobjc;HBASE-1267-3.patch;https://issues.apache.org/jira/secure/attachment/12402553/HBASE-1267-3.patch","19/Mar/09 00:49;ryanobjc;HBASE-1267.patch;https://issues.apache.org/jira/secure/attachment/12402537/HBASE-1267.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25655,Reviewed,,,,Thu Mar 19 08:52:31 UTC 2009,,,,,,,,,,"0|i0hcav:",99255,,,,,,,,,,,,,,,,,,,,,"19/Mar/09 00:49;ryanobjc;Turns out we werent using the correct comparator in the scanner for all contexts.  IF we dont use the meta comparator then we run into issues where we have 2 rows like so:

table,,1234
table,\0,12345

without the meta comparator, we compare the ',' and the '\0' and the scanner thinks the underlying store file is out of order.;;;","19/Mar/09 00:54;stack;Excellent.  Good one Ryan.  Thanks for the patch.;;;","19/Mar/09 01:21;nitay;Minor thing: Why create a Comparator on each HScanner construction? We could just call HStoreKey#getRawComparator(HRegionInfo), which should be fixed when HBASE-1262 gets put in. Furthermore, why not make all our Comparators have a private/protected constructor and a static instance everyone retrieves (i.e. Singleton pattern)?  ;;;","19/Mar/09 01:36;ryanobjc;nitay is right on here, we should use these convenience functions and reuse the static object.;;;","19/Mar/09 01:36;ryanobjc;see new patch that fixes nitay's concerns.;;;","19/Mar/09 03:19;stack;Resolve again.  Thanks for the review Nitay.  Good stuff lads.;;;","19/Mar/09 08:40;ryanobjc;turns out the memcache scanner was causing problems, which was causing an assignment loop because we didnt find values in the memcache while scanning, thus never knew about the new assignments.;;;","19/Mar/09 08:40;ryanobjc;still buggy, newest patch fixes.;;;","19/Mar/09 08:52;stack;Applied part 2 and 3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong return values of comparators for ColumnValueFilter,HBASE-1264,12416979,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,clint.morgan,schneidexe,schneidexe,16/Mar/09 15:45,13/Sep/09 22:24,01/Jul/25 07:49,13/May/09 17:04,0.19.1,,,,,0.19.2,0.20.0,,Filters,,,,0,The return values of the compareTo() method have to be changed against each other. The method has to return 0 if the regex matches and 1 if it does not.,"Ubuntu 8.04 (64 Bit), JDK 1.6",300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,"13/May/09 16:32;clint.morgan;1264-v3.patch;https://issues.apache.org/jira/secure/attachment/12408008/1264-v3.patch","04/May/09 21:49;apurtell;hbase-1264-2.patch;https://issues.apache.org/jira/secure/attachment/12407185/hbase-1264-2.patch","25/Mar/09 21:25;schneidexe;hbase-1264.patch;https://issues.apache.org/jira/secure/attachment/12403641/hbase-1264.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25653,Incompatible change,,,,Wed May 13 19:22:15 UTC 2009,,,,,,,,,,"0|i0hca7:",99252,Patch fixes the return values of org.apache.hadoop.hbase.filter.RegexStringComparator.compareTo() ,,,,,,,,,,,,,,,,,,,,"18/Mar/09 22:08;stack;Do you have a patch for this Thomas?;;;","18/Mar/09 22:31;schneidexe;Not yet. Because this is my first submission to JIRA, I am not exactly sure how to create one. I tried it with linux' diff but it only gives me the output of the patch file appended which is not much.;;;","19/Mar/09 09:11;stack;This is how it is in the file currently:

 71     return pattern.matcher(Bytes.toString(value)).find() ? 1 : 0;

Do you want the 0 and the 1 flipped?  So its:

<     return pattern.matcher(Bytes.toString(value)).find() ? 0 : 1;

Your patch is a bit odd in that it would seem to make the file into what it already has.

You might try 'diff -u FILENAME'.  Do the diff from $HBASE_HOME.  Thats how we apply them, at that location.  Otherwise, checkout hbase and do svn diff FILENAME.

Thanks Thomas.;;;","25/Mar/09 21:25;schneidexe;I recreated the patch usinf diff -u form $HBASE_HOME. Hope it's ok now?;;;","26/Mar/09 02:42;apurtell;Hi Thomas. Why do you think the return of compareTo() is wrong? Your change causes the unit test for this class to fail. ;;;","26/Mar/09 09:24;schneidexe;Hi Andrew,

as in [Comparable.compareTo()|http://java.sun.com/javase/6/docs/api/java/lang/Comparable.html#compareTo(T)] defined, the return values should be 0 if the compared objects equal. In the RegexStringComparator it is the case if the comaperd String matches the pattern. Then the method should return 0. If the String does not match, it should return 1.

I found this bug, when is used the RegexStringComparator as Comparator in a [ColumnValueFilter|http://hadoop.apache.org/hbase/docs/current/api/org/apache/hadoop/hbase/filter/ColumnValueFilter.html#constructor_detail] like {{ColumnValueFilter(byte[], ColumnValueFilter.CompareOp, WritableByteArrayComparable)}}. It filtered exactly the opposite values, those who did not match the given regex. But ColumnValueFilter is implemented as positive filter, meaning it returns the values who match the filter (other negative implementations elimate the values matching the filter and return the rest). I tested this behaviour by using the ColumnValueFilter with a simple value comparison like {{ColumnValueFilter(byte[], ColumnValueFilter.CompareOp, byte[])}}. After that I found the non-default return behaviour of the compareTo() method in RegexStringcomparator as cause for the wrong filtering.

;;;","03/May/09 22:13;stack;Moving to 0.20.0.;;;","04/May/09 21:49;apurtell;There was slightly more to this issue than the contributed patch, added patch -2. ;;;","04/May/09 21:53;apurtell;Commited to trunk and 0.19 branch. Passes all local tests. ;;;","12/May/09 18:57;clint.morgan;I found that this patch broke all my uses of ColumnValueFilter with the EQUAL and NOT_EQUAL operator.

It is now the case that a new ColumnFilter(""A:B"", EQUAL, ""FOO"") will filter out rows where column A:B has value of FOO. Whereas the past (and expected?) behavior is to filter out rows whose value is *not* ""FOO"" IE match rows where ""A:B""=""FOO"".

The relevant part of the patch is:

Index: src/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java
===================================================================
--- src/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java	(revision 771453)
+++ src/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java	(working copy)
@@ -172,9 +172,9 @@
     case LESS_OR_EQUAL:
       return compareResult < 0;
     case EQUAL:
+      return compareResult == 0;
+    case NOT_EQUAL:
       return compareResult != 0;
-    case NOT_EQUAL:
-      return compareResult == 0;
     case GREATER_OR_EQUAL:
       return compareResult > 0;
     case GREATER:

which I think should be reverted.;;;","12/May/09 20:30;clint.morgan;Reopening as I think the above part of the patch should not have made it in;;;","13/May/09 05:37;stack;Please add a patch to the issue Clint and we'll commit it.  Thanks.;;;","13/May/09 16:32;clint.morgan;Revert ColumnValueFilter to previous behavior, and fix the tests;;;","13/May/09 17:04;stack;Committed trunk and branch.  Thanks for the patch Clint (FYI, the filter interface has change in that now it takes byte array, offset, and length.  Old byte array only has been deprecated in 0.20.0).;;;","13/May/09 19:22;apurtell;Reassign to Clint for credit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ganglia metrics should have a common prefix so we can group easier,HBASE-1259,12416702,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,12/Mar/09 00:21,13/Sep/09 22:24,01/Jul/25 07:49,14/Mar/09 06:06,0.19.0,,,,,0.19.1,0.20.0,,,,,,0,"The metrics exported are intermixed with other ganglia metrics... some of the names used are very common like 'requests'.  Instead we should use a prefix like ""hbase_"" so they appear both grouped and separate in ganglia UIs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/09 00:27;ryanobjc;HBASE-1258-1259.patch;https://issues.apache.org/jira/secure/attachment/12401996/HBASE-1258-1259.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25652,,,,,Sat Mar 14 06:06:45 UTC 2009,,,,,,,,,,"0|i0hc93:",99247,,,,,,,,,,,,,,,,,,,,,"12/Mar/09 00:27;ryanobjc;this patch is also for 1258 (attached there as well).;;;","14/Mar/09 06:06;stack;Fixed by HBASE-1258 commit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ganglia metrics for 'requests' is confusing,HBASE-1258,12416701,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,12/Mar/09 00:19,13/Sep/09 22:24,01/Jul/25 07:49,14/Mar/09 06:05,0.19.0,,,,,0.19.1,0.20.0,,,,,,0,"the 'requests' metric is incremented for every request, but it is reset and published every interval.  Which means the number is actually 'requests per interval' which is a config value in hbase.  

HBase should export 'requests/second' instead.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/09 04:35;ryanobjc;HBASE-1258-1259-v2.patch;https://issues.apache.org/jira/secure/attachment/12402189/HBASE-1258-1259-v2.patch","12/Mar/09 00:26;ryanobjc;HBASE-1258-1259.patch;https://issues.apache.org/jira/secure/attachment/12401995/HBASE-1258-1259.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25651,,,,,Sat Mar 14 06:05:53 UTC 2009,,,,,,,,,,"0|i0hc8v:",99246,,,,,,,,,,,,,,,,,,,,,"12/Mar/09 00:26;ryanobjc;this patch is for both 1258 and 1259;;;","12/Mar/09 16:41;stack;I applied to a clean trunk and when I tried to build I got this:

{code}
jspc:
     [echo] Setting jspc.notRequired property. jsp pages generated once per ant session only

compile:
    [javac] Compiling 285 source files to /Users/stack/checkouts/hbase/trunk.clean/build/classes
    [javac] /Users/stack/checkouts/hbase/trunk.clean/src/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerMetrics.java:107: cannot find symbol
    [javac] symbol  : variable atomicIncrementTime
    [javac] location: class org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics
    [javac]       this.atomicIncrementTime.pushMetric(this.metricsRecord);
    [javac]           ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
{code}

You might also check out src/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerMetrics.java.  It has a toString method that summarizes regionserver metrics when called that is displayed in the regionserver UI.  Does it include your metrics naming changes?

Good on you Ryan.;;;","14/Mar/09 04:35;ryanobjc;oh im sorry, my bad. here is a fixed patch.;;;","14/Mar/09 06:05;stack;Committed branch and trunk.  Thanks for the patch Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
base64 encoded values are not contained in quotes during the HBase REST JSON serialization,HBASE-1257,12416660,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,macdiesel,macdiesel,macdiesel,11/Mar/09 17:23,13/Sep/09 22:24,01/Jul/25 07:49,21/May/09 15:56,0.20.0,,,,,0.20.0,,,REST,,,,0,"base64 encoded values are having the quotes put around the values before the base64 encoding takes place.  This renders improper JSON that some (all?) json parsers are unable to process.

The defect should be fixed in the agile-json jar available here:
http://github.com/gottesmm/agile-json-2.0/tree/master

I'm hoping to get to this before this weekend.
",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,"21/May/09 12:55;macdiesel;AgileJSON-2.0.jar;https://issues.apache.org/jira/secure/attachment/12408696/AgileJSON-2.0.jar","27/Mar/09 14:52;macdiesel;AgileJSON.jar;https://issues.apache.org/jira/secure/attachment/12403809/AgileJSON.jar","21/May/09 12:55;macdiesel;json.jar;https://issues.apache.org/jira/secure/attachment/12408697/json.jar",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25650,,,,,Thu May 21 15:56:35 UTC 2009,,,,,,,,,,"0|i0hc8n:",99245,,,,,,,,,,,,,,,,,,,,,"11/Mar/09 19:17;roughley;I've fixed the issue in github and pushed it up.  The fix is pretty simple, removing the quotes before base64 encoding and then adding them back to the result.  ;;;","19/Mar/09 20:48;stack;Whats status on this one lads? Want to add a patch here from github?;;;","27/Mar/09 14:52;macdiesel;Attached you will find the updated Jar file for the AgileJSON implementation.

Changes include that string values are now quoted correctly when base64 encoded.  

Also if base64 encoding is requested for the value the key is also base64 encoded as per the REST specification.;;;","27/Mar/09 14:53;macdiesel;This should finish up this issue as well as address the problem with the keys not being base 64 encoded in JSON.  

My apologies for the delay on this.;;;","30/Mar/09 11:00;stack;Hey Brian: Can you put a git revision number or date on the jar so we can associate it with a particular src snapshot (I'll add this as suffix to the jar when I check it in).  Thanks.;;;","30/Mar/09 15:20;macdiesel;ok, I forked the project and made the changes. I've also request a merge back to the trunk.

Changes can be found here:
http://github.com/macdiesel/agile-json-2.0/tree/master

the commit key is:
commit b3b9f78a0dc2b18e7906af5ee99bc23ee3df3bc9

though that seems a bit long, how about we just append the date as todays date since that's when I made the checkin: 2009-03-30 ;;;","20/May/09 06:21;stack;Has a patch included.  Lets apply.;;;","21/May/09 00:20;stack;Moving out of 0.20.0.

Brian, there is something wrong with the jar.  Its missing a bunch of stuff:

{code}
    [javac] /Users/stack/checkouts/hbase/cleantrunk/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java:179: cannot find symbol
    [javac] symbol  : class JSONObject
    [javac] location: class org.apache.hadoop.hbase.rest.parser.JsonRestParser
    [javac]       scannerDescriptor = new JSONObject(new String(input));
    [javac]                               ^
    [javac] /Users/stack/checkouts/hbase/cleantrunk/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java:198: cannot find symbol
    [javac] symbol  : class JSONException
    [javac] location: class org.apache.hadoop.hbase.rest.parser.JsonRestParser
    [javac]     } catch (JSONException e) {
    [javac]              ^
    [javac] /Users/stack/checkouts/hbase/cleantrunk/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java:214: cannot find symbol
    [javac] symbol  : class JSONArray
    [javac] location: class org.apache.hadoop.hbase.rest.parser.JsonRestParser
    [javac]     JSONArray a;
    [javac]     ^
    [javac] /Users/stack/checkouts/hbase/cleantrunk/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java:220: cannot find symbol
    [javac] symbol  : class JSONObject
    [javac] location: class org.apache.hadoop.hbase.rest.parser.JsonRestParser
    [javac]       JSONObject updateObject = new JSONObject(new String(input));
    [javac]       ^
    [javac] /Users/stack/checkouts/hbase/cleantrunk/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java:220: cannot find symbol
    [javac] symbol  : class JSONObject
    [javac] location: class org.apache.hadoop.hbase.rest.parser.JsonRestParser
    [javac]       JSONObject updateObject = new JSONObject(new String(input));
    [javac]                                     ^
    [javac] /Users/stack/checkouts/hbase/cleantrunk/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java:228: cannot find symbol
    [javac] symbol  : class JSONException
    [javac] location: class org.apache.hadoop.hbase.rest.parser.JsonRestParser
    [javac]     } catch (JSONException e) {
...
{code}

The size of this jar is only half that of the original.;;;","21/May/09 11:31;macdiesel;crap, I'll look into it today.;;;","21/May/09 12:53;macdiesel;ok you guys have choices.  

It seems that Micheal was packaging the json.org jar inside this jar and using it in the project.  The build script on his get project doesn't do this repackaging automatically.  

So I built the jar along with the needed json.org classes and will upload them as a patch.  I will also upload the json.jar that I used to do this.  So you can either:

a) use the new fully packaged jar
b) just add the json.jar to the lib directory along with the AgileJson-2.0.jar initially attached with this issue.

But I wanted to let you guys know what was up in the interest of full disclosure.;;;","21/May/09 12:55;macdiesel;Attached now you will find the newly updated and working AgileJSON-2.0.jar.

Also attached is the json.jar from json.org.;;;","21/May/09 15:56;stack;Committed json and new jar separately.  Added note to NOTICES about json.jar.  The license is 'different' but trawling on net would indicate its apache compatible.   Thanks for figuring this one out Brian.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HConnectionManager.getConnection(HBaseConfiguration) returns same HConnection for different HBaseConfigurations ,HBASE-1251,12416529,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,guiga,guiga,10/Mar/09 13:29,17/Aug/10 21:27,01/Jul/25 07:49,13/Mar/09 16:07,0.19.0,,,,,0.20.0,,,Client,,,,0,"This occurs when the following happens:

1. Consider a client that invokes HBaseAdmin.checkHBaseAvailable(config) before doing anything. Although this method copies the HBaseConfiguration object and sets hbase.client.retries.number to 1 (see HBaseAdmin, line 751), it creates an HBaseAdmin object, which invokes HConnectionManager.getConnection(conf). Please notice that this conf is that with hbase.client.retries.number equals to 1. 
2. HConnectionManager.getConnection then creates a HConnection using this conf and puts it into a static map (see HConnectionManager, line 93) indexed by hbase.rootdir. 
3. Then, if the same client now creates a HTable object (using, for instance, a HBaseConfiguration with  hbase.client.retries.number equals to 10 but the same hbase.rootdir), it will invoke HConnectionManager.getConnection(conf) again (see HTable, line 109). However, when it checks the static map for a HConnection it finds one - the one previously created by the HBaseAdmin object and using hbase.client.retries.number 1 - and returns it without creating a new one with the correct HBaseConfiguration.

However, the expected behavior is: HConnectionManager must return different HConnections for different HBaseConfigurations.  ",,,,,,,,,,,,,,,,,,,,,,,HBASE-1976,,HBASE-2027,HBASE-2925,,,"12/Mar/09 18:21;guiga;HBASE-1251.patch;https://issues.apache.org/jira/secure/attachment/12402072/HBASE-1251.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25647,,,,,Fri Mar 13 16:07:59 UTC 2009,,,,,,,,,,"0|i0hc7r:",99241,,,,,,,,,,,,,,,,,,,,,"10/Mar/09 13:41;guiga;I've edited HConnectionManager in order to use all HBaseConfiguration instead of only HBASE_DIR as key on the HBASE_INSTANCES map. To do so, I've implemented HBaseConfiguration.hashcode() - just like a HashMap.hashCode() is implemented.

code review is needed. ;;;","10/Mar/09 17:54;stack;Patch looks great.

Only downside I see is that if the HBC is changed -- a value added or edited -- then its hash will be different and we'll go build up a new TableServer but maybe this is your intent?

On GSOC, figure what interests you Guilherme and lets make it happy.  Write me off-list if you'd like.;;;","10/Mar/09 19:01;jimk;The biggest downside I see is that since a new TableServers is built up if the configuration changes, the old TableServers will
be orphaned if the old configuration is never used again. The old TableServers cannot be garbage collected because the
HBASE_INSTANCES map still holds a reference to it. Could potentially use up a not insignificant amount of memory if this
is done too often.

A couple of other comments:
- Why add hash codes in HBaseConfiguration.hashCode rather than use xor ? I would think using ^= rather than += would produce a better hash value.
- Some of the indentation is inconsistent. 4 spaces instead of two. That should be fixed.
;;;","11/Mar/09 17:18;guiga;Stack:
yes, this is my intent. I think the best scenario would be the hash value be affected only when the values that are used for the creation of connections are modified. However, it would be quite annoying to maintain the list of these values for every release.   

Jim:
I think the use of a WeakHashMap in HBASE_INSTANCES can solve the probem of old orphaned TableServers. What do you think? This way the old TableServers could be garbage collected. 
Related to the hash code, I've used addition because it is the way java.util.HashMaps compute their hash value (I have no argument against xor). ;;;","12/Mar/09 18:21;guiga;- indentation fixed
- hash is now computed using xor
- HBASE_INSTANCES is now a WeakHashMap in order to make orphan TableServers free to be garbage collected. ;;;","13/Mar/09 06:31;stack;Patch looks great.  Thanks.

If you want to figure out google summer of code project, write me off list; lets chat.;;;","13/Mar/09 16:07;stack;Applied.  Thank you for the patch Guilherme.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BloomFilter's use of BitSet is too inefficient,HBASE-1246,12416287,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,nspiegelberg,ryanobjc,ryanobjc,06/Mar/09 10:04,20/Nov/15 13:01,01/Jul/25 07:49,13/May/10 05:22,0.20.0,,,,,0.92.0,,,,,,,0,"From the logfile run of TestBloomFilter with special SizeOf agent jar:

Writing bloom filter for: hdfs://localhost:64003/user/ryan/testComputedParameters/1278366260/contents/6159869037185296839 for size: 100
2009-03-06 01:54:25,491 DEBUG [RegionServer:0.cacheFlusher] regionserver.StoreFile$StoreFileWriter(319): New bloom filter: vectorSize: 1175 hash_count: 5 numKeys: 100
Serialized bloomfilter size: 160
In memory bf size: 1248


As we can see, the bit vector is 1175 bits, and the serialized size is fairly compact - 160 bytes.

But the in-memory size is nearly 10x bigger than it has to be.  Looking in BloomFilter we see:

  BitSet bits;

is the only field.

Clearly it seems the BitSet is using 1 byte = 1 bit.  That is an 8 time expansion of where we should be.

Considering every HFile could potentially have a bloom filter, and bloom filters are more likely to have bit vector sizes of 10,000-100,000, we should do something about this.  Aka: write our own bit-set that uses byte[] and bit ops.","Java 1.6, OSX 64 bit",,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/09 20:48;streamy;ByteBloomFilter.java;https://issues.apache.org/jira/secure/attachment/12401643/ByteBloomFilter.java",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25644,,,,,Fri Nov 20 13:01:55 UTC 2015,,,,,,,,,,"0|i0hc6n:",99236,,,,,,,,,,,,,,,,,,,,,"06/Mar/09 17:00;stack;I thought this had been fixed?  See https://issues.apache.org/jira/browse/HBASE-427.  Maybe we lost Ian's fix down the line?

Also, up on IRC, Johano suggests:

{code}
09:59 < dj_ryan> hmm
09:59 < dj_ryan> we are going to have to write our own BitSet
09:59 < dj_ryan> the java one is too inefficient
09:59 < dj_ryan> it doesnt seem to actually use bits
09:59 < dj_ryan> it uses 1000 bytes to store 1000 bits
09:59 < dj_ryan> :-(
09:59 < dj_ryan> serialized the bitset is 160 bytes, but in memory its 10x bigger.
10:57 < johano> http://code.google.com/p/compressedbitset/
10:57 < johano> http://code.google.com/p/javaewah/
10:57 < johano> might be of use
{code};;;","06/Mar/09 17:00;stack;Oh, otherthing is make the bloomfilter interface take byte [] with offset and length if you can.  That'll work for current HSK and for the coming ByteBuffer.;;;","06/Mar/09 17:34;streamy;HBASE-427 fix is actually moving from a boolean[] to BitSet.  BitSet has the same space issues as the boolean[] so it didn't actually solve that.

compressedbitset is no go because it's GPL.  Looked at javaewah but not clear it's really what we want.  It's not so much that we want to _compress_ the bit array, we just want to store it efficiently.  Generally compressing a bit array retains efficiency in and/or operations but can be very poor at get() / containment checks.

Streamy has a custom, stripped down implementation of blooms in java.  We're currently using BitSet but now learning about this space inefficiency I'd like to move it to byte[]'s.  Would that be worthwhile doing?  Ryan, let me know what you're working at so we can combine efforts.;;;","06/Mar/09 20:20;streamy;This is a custom bloom implementation we have used internally.  I have modified it to use a byte[] to back the bit vector rather than a BitSet.

It also has an optimized/efficient Writable implementation (and static serialize/deserialize that we use).

The way I'm actually doing the hashing is probably non-optimal.  Our use of it always took a long as the unit.  We need (byte[],offset,len) as the unit.  This implementation generates an MD5 hash, puts that into a long, instantiates a Random with the md5 long as the seed, and then uses Random.nextInt() for each hash.  Should take this and pair it with Jenkins or whatever is being used up in Hadoop maybe.

main() contains some basic unit testing.  Everything does appear to work and is WAY more efficient.

{code}
Bloom filter initialized, 9585 bits, 1199 bytes, 7 hashes, 0.01 error rate, up to 1000 elements
Bloom bit vector contains 9585 bits
Serialized as 1230 bytes
{code}

In-memory size will be similarly efficient with the byte[] instead of the BitSet or boolean[] as before.;;;","06/Mar/09 20:28;streamy;Please note, there are a number of optimizations to be made in this code w.r.t. the hashing and also some optimizations for get/set using static arrays instead of doing shifts every time.;;;","06/Mar/09 20:48;streamy;Contains bitvals array for computation efficiency.  If I have a static final in a class, is it the case that we don't pay the memory overhead for each object instantiation?

This also now implements HeapSize.  Currently does not include size of lookup array.

{code}
Bloom bit vector contains 9585 bits
Serialized as 1230 bytes
HeapSize is 1255
{code};;;","31/Mar/09 15:43;jbellis;> BitSet has the same space issues as the boolean[] so it didn't actually solve that. 

That is not correct in the Sun SDK.  The version I have (1.6.10) definitely uses one bit per... bit and has the comment

 * @version 1.67, 04/07/06

so I imagine this is the case at least for all versions of 1.6.;;;","30/Jan/10 03:03;jdcryans;We don't use Bloom Filters at the moment, can we close this?;;;","30/Jan/10 04:32;stack;We'll be reviving bloomfilters in the near future.  Lets keep this issue around.  It'll inspire our verifying how much space a bit actually occupies in our implementation.;;;","13/May/10 04:21;stack;Can we resolve this Nicolas given hbase-1200?;;;","13/May/10 04:25;nspiegelberg;yup.  this hbase-1200 uses byte[] and bit math, so we should be fine with space efficiency;;;","13/May/10 04:41;stack;Moved from 0.21 to 0.22 just after merge of old 0.20 branch into TRUNK.;;;","13/May/10 05:22;stack;Resolving;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hfile meta block handling bugs,HBASE-1245,12416281,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,06/Mar/09 06:15,13/Sep/09 22:24,01/Jul/25 07:49,06/Mar/09 06:47,0.20.0,,,,,0.20.0,,,,,,,0,"HFile doesn't handle 'get meta block' when there are no meta blocks.  It throws an unhelpful exception ""meta index not loaded"", which is not the case.  No meta blocks = no meta index.  It should return null instead.

Additionally, hfile doesn't even get all meta names properly, due to the incorrect use of the file's comparator, instead of using just a bytes comparator in the index.  This is manifested by NPEs in some tests.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/09 06:39;ryanobjc;HBASE-1240.patch;https://issues.apache.org/jira/secure/attachment/12401595/HBASE-1240.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25643,,,,,Fri Mar 06 06:47:25 UTC 2009,,,,,,,,,,"0|i0hc6f:",99235,,,,,,,,,,,,,,,,,,,,,"06/Mar/09 06:39;ryanobjc;fixes;;;","06/Mar/09 06:47;stack;Committed.  Thanks for the patch Ryan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
input buffer reading in the REST interface does not correctly clear the character buffer each iteration,HBASE-1239,12416147,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,macdiesel,macdiesel,macdiesel,04/Mar/09 20:42,13/Sep/09 22:24,01/Jul/25 07:49,06/Mar/09 06:42,0.20.0,,,,,0.20.0,,,REST,,,,0,"when reading the input buffer in the REST interface the character buffer is not cleared for each iteration of the loop.  This can cause malformed data to be read from the input stream in cases where the input is greater than 640 characters.

See lines numbered 366-376 in org.apache.hadoop.hbase.rest.Dispatcher.java

I have prepared a patch for this.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/09 20:51;macdiesel;hbase-inputbuffer.patch;https://issues.apache.org/jira/secure/attachment/12401460/hbase-inputbuffer.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25641,,,,,Fri Mar 06 06:42:22 UTC 2009,,,,,,,,,,"0|i0hc53:",99229,,,,,,,,,,,,,,,,,,,,,"04/Mar/09 20:51;macdiesel;patch for the input buffer issue.;;;","06/Mar/09 06:42;stack;Committed.  Thanks for the fix Brian.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Under upload, region servers are unable to compact when loaded with hundreds of regions",HBASE-1238,12416128,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,jdcryans,jdcryans,04/Mar/09 18:56,13/Sep/09 22:24,01/Jul/25 07:49,06/Mar/09 20:01,0.19.0,,,,,0.19.1,0.20.0,,regionserver,,,,0,"We have a situation where each region server is loaded with 100+ regions, most of them in the same table. During a long upload of webpages, each memcache gets filled near equally fast so that the global memcache limit is usually triggered before the max memcache size. Since that emergency flush does not trigger compactions, the number of store files just keeps growing until it fails on all kinds of errors.

We need a better story for this as this is a ""normal"" situation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/09 20:53;jdcryans;hbase-1238.patch;https://issues.apache.org/jira/secure/attachment/12401462/hbase-1238.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25640,,,,,Fri Mar 06 20:01:40 UTC 2009,,,,,,,,,,"0|i0hc4v:",99228,,,,,,,,,,,,,,,,,,,,,"04/Mar/09 20:53;jdcryans;I'm going to try if this helps our job.;;;","06/Mar/09 00:34;jdcryans;Turns out the patch works well :

{code}
2009-03-05 00:37:54,348 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://camanomade.wordpress.com/,1236183697332 because global memcache limit of 676.7m exceeded; currently 646.4m and flushing till 422.9m
2009-03-05 00:38:01,738 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://astore.amazon.com/blogcarnival23-20/detail/0761104844/178-5338054-2231726,1235269774078 because global memcache limit of 676.7m exceeded; currently 622.2m and flushing till 422.9m
2009-03-05 00:38:02,556 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.fsprostab.com/english/index.htm,1235943861247 because global memcache limit of 676.7m exceeded; currently 600.9m and flushing till 422.9m
2009-03-05 00:38:48,789 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://blackbamboohawaii.com/infinity/index.html,1236182065945 because global memcache limit of 676.7m exceeded; currently 580.9m and flushing till 422.9m
2009-03-05 00:38:49,271 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://ask.metafilter.com/60882/MidSouth-vacation,1235821461371 because global memcache limit of 676.7m exceeded; currently 563.1m and flushing till 422.9m
2009-03-05 00:38:49,865 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.galapagosisland.net/travel_insurance.html,1236106016987 because global memcache limit of 676.7m exceeded; currently 545.2m and flushing till 422.9m
2009-03-05 00:38:50,449 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://backcountryblog.blogspot.com/2006/10/hucking-cliffs-is-soooo-last-week.html,1236167469547 because global memcache limit of 676.7m exceeded; currently 528.2m and flushing till 422.9m
2009-03-05 00:38:50,985 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.fletcherlake.com/index.htm,1236166974226 because global memcache limit of 676.7m exceeded; currently 511.3m and flushing till 422.9m
2009-03-05 00:38:51,803 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.eurailblog.com/archives/cat_wales.shtml,1235786159128 because global memcache limit of 676.7m exceeded; currently 495.4m and flushing till 422.9m
2009-03-05 00:38:52,242 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.island.net/~sointula/accom.html,1236183944276 because global memcache limit of 676.7m exceeded; currently 479.8m and flushing till 422.9m
2009-03-05 00:38:52,774 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.manukapoint.com/Contact__20Us/__;window.mm_menu_b25ee76db7ed98b94e5945a386e8b470_2.addMenuItem__,1235779612713 because global memcache limit of 676.7m exceeded; currently 465.8m and flushing till 422.9m
2009-03-05 00:38:53,252 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://wellsbeachme.com/wc/index.htm,1236106056942 because global memcache limit of 676.7m exceeded; currently 452.4m and flushing till 422.9m
2009-03-05 00:38:55,239 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.greeka.com/saronic/agistri/hotels/agistri-rosys-little-village/index.html,1236104694918 because global memcache limit of 676.7m exceeded; currently 439.3m and flushing till 422.9m
2009-03-05 00:38:55,525 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of web_pages,http://www.manukapoint.com/Contact__20Us/__;window.mm_menu_09fa2b8abdc5d7b0d6a4fd2cb2877ef1_2.addMenuItem__,1236219657625 because global memcache limit of 676.7m exceeded; currently 426.4m and flushing till 422.9m
2009-03-05 00:39:04,436 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting  compaction on region web_pages,http://camanomade.wordpress.com/,1236183697332
2009-03-05 00:39:08,996 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region web_pages,http://camanomade.wordpress.com/,1236183697332 in 4sec
2009-03-05 00:39:08,997 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting  compaction on region web_pages,http://astore.amazon.com/blogcarnival23-20/detail/0761104844/178-5338054-2231726,1235269774078
2009-03-05 00:39:13,673 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region web_pages,http://astore.amazon.com/blogcarnival23-20/detail/0761104844/178-5338054-2231726,1235269774078 in 4sec
...
{code}

I suggest we commit this for 0.19.1, maybe even for 0.20.0 until a better solution is found.;;;","06/Mar/09 06:00;stack;+1 on patch.

When committing, please add profuse comments that this is a hack and a better way needs to be figured.  That as is, compactions may be run prematurely.

We need to figure some means of bubbling up the result of HRegion#flushCache up out of MemcacheFlusher#flushRegion in emergencies.  If true, compaction is wanted.  If this came up out of MemcacheFlusher#flushRegion call, then we add to the flush-later queue you just added.

Good stuff J-D.;;;","06/Mar/09 15:44;jdcryans;This is committed to branch, +1 also for trunk?;;;","06/Mar/09 16:49;stack;Yes.  As you say, until we do something better...;;;","06/Mar/09 18:26;jdcryans;Committed to trunk.;;;","06/Mar/09 20:01;stack;Resolving. Committed to TRUNK and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
application/x-www-form-urlencoded is incorrectly handled in the REST interface,HBASE-1223,12415613,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,macdiesel,macdiesel,macdiesel,25/Feb/09 21:04,13/Sep/09 22:24,01/Jul/25 07:49,26/Feb/09 06:04,0.19.1,,,,,0.20.0,,,REST,,,,0,"currently if application/x-www-form-urlencoded form data is posted to the REST interface it does not appear in the query map.  Attached to this issue is a patch that fixes this and allows data posted with a form as application/x-www-form-urlencoded data to be read into the query map and used to complete the request.

I first noticed this while trying to create a scanner and I was unable to specify the start date/end date/columns if the query parameters were passed in as application/x-www-form-urlencoded.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/09 21:05;macdiesel;RestContentTypePatch.txt;https://issues.apache.org/jira/secure/attachment/12400963/RestContentTypePatch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25634,,,,,Thu Feb 26 15:01:53 UTC 2009,,,,,,,,,,"0|i0hc1r:",99214,,,,,,,,,,,,,,,,,,,,,"26/Feb/09 06:04;stack;Committed to TRUNK.

Thanks for the patch Brian (I added you as a committer and assigned this issue to you).

You had this pegged against 0.19.1 but your new stuff isn't in the 0.19 branch.  Correct me if I did this wrong.  Thanks.;;;","26/Feb/09 15:01;macdiesel;I had noticed that the rest changes are in the 
trunk_on_hadoop-0.19.1-dev_with_hadoop4379

And while I'm not exactly sure what that means, the patch should probably be applied there as well.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make port displayed the same as is used in URL for RegionServer table in UI,HBASE-1209,12415289,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,larsgeorge,larsgeorge,larsgeorge,22/Feb/09 01:05,13/Sep/09 22:24,01/Jul/25 07:49,26/Feb/09 17:49,0.19.0,,,,,0.20.0,,,,,,,0,"In master JSP file make the ports being used the same for these two adjacent lines:

  String url = ""http://"" +
    hsi.getServerAddress().getHostname().toString() + "":"" +
    hsi.getInfoPort() + ""/"";
  String hostname = hsi.getServerAddress().getHostname() + "":"" + hsi.getServerAddress().getPort();

The second line should be:

  String hostname = hsi.getServerAddress().getHostname() + "":"" + hsi.getInfoPort();

Or possibly do this?

  String hostname = hsi.getServerAddress().getHostname() + "":"" + hsi.getInfoPort();
  String url = ""http://"" + hostname + ""/"";

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/09 01:45;larsgeorge;1209.patch;https://issues.apache.org/jira/secure/attachment/12400681/1209.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25627,,,,,Thu Feb 26 17:49:38 UTC 2009,,,,,,,,,,"0|i0hbz3:",99202,,,,,,,,,,,,,,,,,,,,,"22/Feb/09 01:45;larsgeorge;Patch for port mismatch issue.;;;","26/Feb/09 17:49;stack;Committed.  Thanks for the patch Lars.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getRow does not always work when specifying number of versions,HBASE-1202,12414911,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,jimk,jimk,17/Feb/09 04:12,13/Sep/09 22:24,01/Jul/25 07:49,10/Apr/09 20:00,0.19.0,0.19.1,0.20.0,,,0.19.2,0.20.0,,regionserver,,,,0,"When a cell that exists is updated, getRow specifying number of versions does not work.
What is returned is the original value at that timestamp, instead of the updated value.

Note that this only applies when more than one version is specified. getRow with (implied) timestamp = latest does work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/09 04:14;jimk;TestGetRowVersions.java;https://issues.apache.org/jira/secure/attachment/12400307/TestGetRowVersions.java",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25623,,,,,Fri Apr 10 20:00:08 UTC 2009,,,,,,,,,,"0|i0hbxj:",99195,,,,,,,,,,,,,,,,,,,,,"17/Feb/09 04:14;jimk;This is a test program that demonstrates the problem.;;;","07/Mar/09 16:48;jimk;Downgrading to major and moving 0.19.2. It can be fixed when we do scanners with multiple versions.;;;","16/Mar/09 17:10;stack;I just noticed in memcache that numVersions is not per column but a count of all results found so far.  Maybe related?;;;","10/Apr/09 18:11;jimk;I don't think numVersions being a count of all results is the issue. (but it is a problem if it hasn't been fixed).

What the test does is:
1. store the value ""value1"" in column contents:contents at a specified timestamp.
2. shutdown and restart the cluster to force data to disk.
3. store the value ""value2"" in column contents:contents at the *same* timestamp as the first value.
4. call getRow(row) and it gets back ""value2"" as expected.
5. call getRow(row, HConstants.ALL_VERSIONS) and it gets back ""value1"" and not ""value2""

But now I understand what is going on.

Cell contains a SortedMap<Long, byte[]> (where Long is the timestamp). So what happens is that ""value2"" is fetched out of the memcache and then ""value1"" is fetched from disk and because the timestamps are the same, overwrites the entry containing ""value2"". 

I think when we are looking for multiple versions, we need to check if we already have a match for row/column/timestamp and not insert a second value if we already have one at that timestamp.
;;;","10/Apr/09 18:32;jimk;It turns out that HStore has the same problem as Memcache, i.e., keeping a count of versions on a per-column basis instead of per-cell.;;;","10/Apr/09 19:31;jimk;I was wrong about Memcache and HStore. After reading more closely, they do count numVersions on a per cell basis.;;;","10/Apr/09 20:00;jimk;Added new test case TestGetRowVersions.
Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeper ensureParentExists calls fail on absolute path,HBASE-1191,12414323,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,nitay,nitay,nitay,08/Feb/09 05:19,13/Sep/09 22:24,01/Jul/25 07:49,08/Feb/09 18:38,0.19.0,,,,,0.20.0,,,,,,,0,"If user specifies absolute path for one of the files in ZooKeeper, the following will not do what it's supposed to:

if (!ensureZNodeExists(parentZNode)) {
  ...

Because the user specified path is not a child of parentZNode, all operations on it will fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/09 05:30;nitay;hbase-1191.patch;https://issues.apache.org/jira/secure/attachment/12399748/hbase-1191.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25620,Reviewed,,,,Sun Feb 08 18:38:33 UTC 2009,,,,,,,,,,"0|i0hbvb:",99185,,,,,,,,,,,,,,,,,,,,,"08/Feb/09 18:38;jdcryans;I tested the patch and it works, committed to trunk. Thanks Nitay!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableInputFormatBase with row filters scan too far ,HBASE-1190,12414320,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,apurtell,davelatham,davelatham,08/Feb/09 01:51,20/Sep/12 22:10,01/Jul/25 07:49,08/Feb/09 19:39,0.19.0,,,,,0.19.1,0.20.0,,,,,,0,"When TableInputFormatBase has a non-null RowFilterInterface to apply, it creates combines the row filter with a StopRowFilter to get a scanner for each input split.  However, the StopRowFilter never indicates that fitlerAllRemaining is true, so each input split will end up scanning to the end of the table.  (Contrast with HTable.getScanner(byte[][] columns, byte[] starRow, byte[] stopRow, long timestamp) which uses a StopRowFilter wrapped in a WhileMatchRowFilter to ensure that scanning ends at the stop row.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/09 02:02;davelatham;HBASE-1190.patch;https://issues.apache.org/jira/secure/attachment/12399744/HBASE-1190.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25619,,,,,Sun Feb 08 19:39:00 UTC 2009,,,,,,,,,,"0|i0hbv3:",99184,,,,,,,,,,,,,,,,,,,,,"08/Feb/09 02:02;davelatham;Here's a simple patch, wrapping the StopRowFilter in a WhileMatchRowFilter.  It makes no difference in the output to test, but fixes the performance problem.;;;","08/Feb/09 19:39;apurtell;Thanks for the patch Dave! Passes all local tests, applied to trunk and 0.19 branch. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong request/sec in the gui reporting wrong,HBASE-1185,12414121,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,viper799,viper799,05/Feb/09 17:29,13/Sep/09 22:24,01/Jul/25 07:49,05/Mar/09 14:10,0.19.0,,,,,0.19.1,0.20.0,,Client,regionserver,,,0,"I am seeing lower number of request in the masters gui then I have seen in 0.18.0 while scanning.
I thank part of it is we moved to report per sec request not per 3 secs so the request should be 1/3 of the old numbers I was getting.

hbase.client.scanner.caching is not the reason the request are under reported.
I set hbase.client.scanner.caching = 1 and still get about 2K request a sec in the gui
but when the job is done I take records / job time and get 36,324/ records /sec. So
there must be some caching out side of the hbase.client.scanner.caching making the
request per sec lower then it should be. I know it running faster then reported just thought
it might give some new users the wrong impression that request/sec = read/write /sec.
",0.19.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/09 19:02;jdcryans;hbase-1185.patch;https://issues.apache.org/jira/secure/attachment/12401445/hbase-1185.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25617,,,,,Thu Mar 05 14:10:12 UTC 2009,,,,,,,,,,"0|i0hbtz:",99179,,,,,,,,,,,,,,,,,,,,,"02/Mar/09 21:36;larsgeorge;Hi Billy,

I am trying to understand what the issue is exactly. I can see what the UI does to report request per interval. You say you have changed yours from 3 to 1 sec and therefore would expect the number to be a third from your actual numbers?

Where do you get the second number from? Or in other words, how do you compute the ""records / job time""? Based on the number of the table you are scanning? For example, if you had 10,000 rows and scan it using a M/R job, you divide the number of records by the total job runtime? 

And what you want is that the UI's ""total requests"" number to be close to the above?

Thanks,
Lars;;;","02/Mar/09 21:44;larsgeorge;HI Stack,

When I look at the code in the HRegionServer.run() I see this:

            this.serverInfo.setLoad(new HServerLoad(requestCount.get(),
                onlineRegions.size()));
            this.requestCount.set(0);

Which effectively means that this number is aggregated during the ""message-interval"" timeframe of the default 3 seconds. Then it handed over to the master and reset. The master UI takes this number and display it divided by the message-interval to get the requests per second value.

But since this in not actually aggregating the total number of request *per* HRegionServer it is always just a snapshot of what happened in the last three seconds. Once the job ends and no further requests are done it drops down to zero within 3 seconds.

Which I think is OK for a gauge. But what Billy is asking for sounds like something different, i.e. the load as an average over time - similar maybe to the unix ""w"" output.

Am I wrong here?

Thanks,
Lars
;;;","03/Mar/09 22:31;viper799;I get my second number from a MR Job so yes number of records by the total job runtime

what I was saying about the 2 secs before in the past we reported 
request in the gui on a 3 sec interval now we take the count and report it per sec. pre 0.19.0 I thank 
0.18.0 maybe before that its been a while.

I thank the problem I am seeing is coming from the hbase.client.scanner.caching if it is set to 20 is that counted as 1 request or as 20 when we are scanning?
;;;","04/Mar/09 13:35;jdcryans;Billy, I can confirm that it's scanner pre-fetching. In HRS :

{code}
public RowResult[] next(final long scannerId, int nbRows) throws IOException {
    checkOpen();
    requestCount.incrementAndGet();
    ArrayList<RowResult> resultSets = new ArrayList<RowResult>();
    try {
      String scannerName = String.valueOf(scannerId);
      InternalScanner s = scanners.get(scannerName);
      if (s == null) {
        throw new UnknownScannerException(""Name: "" + scannerName);
      }
      this.leases.renewLease(scannerName);
      for(int i = 0; i < nbRows; i++) {
...
{code}

We increment for the whole batch of rows. I take the blame since it's my modification. The fix will be easy.;;;","04/Mar/09 14:48;viper799;no big deal just thought it should report correctly sense we where under reporting the speeds.;;;","04/Mar/09 19:02;jdcryans;Billy, try this patch (against 0.19 branch) to see if you have better numbers.;;;","04/Mar/09 21:38;viper799;+1 the numbers look good now
thanks for the patch;;;","04/Mar/09 21:49;viper799;I tested on 0.19.1 branch from a month or so ago.;;;","05/Mar/09 14:10;jdcryans;Committed to branch and trunk. Thanks trying it out Billy!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.19 branch jar is building as 0.20-dev instead of 0.19.1-dev,HBASE-1179,12413921,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,streamy,streamy,streamy,03/Feb/09 17:52,13/Sep/09 22:32,01/Jul/25 07:49,03/Feb/09 18:35,0.19.1,,,,,0.19.1,,,,,,,0,"Simple change to build.xml, will attach patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/09 17:54;streamy;hbase-1179-v1.patch;https://issues.apache.org/jira/secure/attachment/12399373/hbase-1179-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25615,,,,,Tue Feb 03 18:35:29 UTC 2009,,,,,,,,,,"0|i0hbsn:",99173,,,,,,,,,,,,,,,,,,,,,"03/Feb/09 17:54;streamy;-  <property name=""version"" value=""0.20.0-dev""/>
+  <property name=""version"" value=""0.19.1-dev""/>;;;","03/Feb/09 18:35;apurtell;Committed, thanks for pointing this out. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBA administrative tools do not work when specifying regionName,HBASE-1175,12413828,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,02/Feb/09 23:54,13/Sep/09 22:24,01/Jul/25 07:49,05/Feb/09 05:56,0.19.0,0.20.0,,,,0.19.1,0.20.0,,Client,master,,,0,"HBaseAdmin administrative functions allow tableName or regionName through the API.  Things are okay if we pass tableName, but when using regionName the code in HMaster is incorrect.  It is expecting to be passed tableName and startRow, but we are passing null and regionName.  Patch will fix master to handle this case properly.

Log for good measure:

{code}
[hbase@mb0 StyBase]$ java TableMaintenance chunks
Running maintenance on table 'chunks'
Table contains 2 regions
  > Flushing region {chunks,,1229390225893}
EXCEPTION FLUSHING REGION! [org.apache.hadoop.ipc.RemoteException: java.io.IOException: Invalid arguments to openScanner
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1695)
        at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:895)
Caused by: java.lang.NullPointerException: firstRow for scanner is null
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1692)
        ... 5 more

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:701)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:321)
        at $Proxy2.openScanner(Unknown Source)
        at org.apache.hadoop.hbase.master.HMaster.getTableRegionClosest(HMaster.java:725)
        at org.apache.hadoop.hbase.master.HMaster.modifyTable(HMaster.java:804)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:895)
]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/09 01:51;streamy;hbase-1175-v1.patch;https://issues.apache.org/jira/secure/attachment/12399322/hbase-1175-v1.patch","03/Feb/09 04:26;streamy;hbase-1175-v2.patch;https://issues.apache.org/jira/secure/attachment/12399332/hbase-1175-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25613,,,,,Thu Feb 05 05:56:08 UTC 2009,,,,,,,,,,"0|i0hbrz:",99170,,,,,,,,,,,,,,,,,,,,,"03/Feb/09 01:51;streamy;Tested and working now when specifying a regionName.  Wait until tomorrow to commit, I will be doing a lot with these functions and will give +1 after I know it works.

Please review patch to make sure I'm following proper conventions :)

Thanks.;;;","03/Feb/09 01:53;streamy;Have verified that patch applies against both 0.19 branch and 0.20 trunk;;;","03/Feb/09 02:20;jdcryans;Your splitRegionName method shares some code with HSK.stripStartKeyMeta but that one is private... maybe refactor all that in a util class? I would also change the name for parseRegionName, less confusing. Check also the lengths of your lines.

In getTableRegionFromName startKey and columns are never used.

Rest seems fine.;;;","03/Feb/09 04:26;streamy;Removed extra variables, changed function name, fixed line lengths.

Thanks jd;;;","03/Feb/09 09:17;apurtell;Code in master did not expect startRow, it expected only a key somewhere within the region. See comments in HBASE-902 as to why. I don't care if the semantics are changed, but at least the comment in the master UI at the bottom of the region view must be changed. Also do these changes break the shell surgery tools? ;;;","03/Feb/09 15:42;streamy;Andrew, will test today but this should not have broken the existing semantics.

Code in HMaster supports three possibilities now:  (null,regionName)  (tableName,rowKey)  or (tableName,null)

Whether or not that's what we want is another question, but as this patch stands this is all supported.  Not sure if we want to expose all three possibilities to web ui, HBA, shell.;;;","03/Feb/09 18:29;apurtell;Thanks for the clarification Jonathan.

+1
;;;","03/Feb/09 23:34;apurtell;Ok to go ahead and commit this Jonathan? ;;;","04/Feb/09 18:01;streamy;Yes.  Have run this on every region in one of my clusters a few times in the past two days without a problem.

+1 to commit;;;","05/Feb/09 05:56;apurtell;I've been running this also. Committed to trunk and 0.19 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If we do not take start code as a part of region server recovery, we could inadvertantly try to reassign regions assigned to a restarted server with a different start code",HBASE-1157,12413344,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,jimk,jimk,27/Jan/09 02:55,13/Sep/09 22:24,01/Jul/25 07:49,27/Mar/09 06:09,0.20.0,,,,,0.20.0,,,master,regionserver,,,0,,,,,,,,,,,,,,HBASE-1144,,,,,,,,,,,,HBASE-1158,HBASE-1156,,,"19/Mar/09 04:37;ryanobjc;HBASE-1157.patch;https://issues.apache.org/jira/secure/attachment/12402543/HBASE-1157.patch","19/Mar/09 06:25;ryanobjc;HBASE-1267-3.patch;https://issues.apache.org/jira/secure/attachment/12402549/HBASE-1267-3.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25605,,,,,Fri Mar 27 06:09:27 UTC 2009,,,,,,,,,,"0|i0hbnz:",99152,"Part of region server death recovery depends on knowing which instance (start code) of the server being recovered.

If we allow a new instance to join the cluster immediately (desirable) then we must know the difference between regions
served by a previous instance of a server with the same ipaddress:port and the current instance. We need to have the 
information about the startcode to know the difference.",,,,,,,,,,,,,,,,,,,,"27/Jan/09 03:01;jimk;As 0.19.x does not allow a server with the same ip:port number to join the cluster until the previous instance has been recovered, this is not an issue for 0.19.x. However, if we intend to allow a new instance of the same server to start serving regions, we need to be able to differentiate between the dead instance and the new one.

This is my primary reason for making the startcode a part of the regionserver identification.

While we might use a different mechanism after ZK integration, the ZK integration still needs to account for what instance
of the regionserver it is dealing with.
;;;","14/Mar/09 01:41;jimk;Passes tests. Committed.;;;","14/Mar/09 04:36;stack;Just wondering, did you talk w/ the ZKBoys about whether or not ZK already has a mechanism differentiating different regionservers in ZK?

Above you say ""While we might use a different mechanism after ZK integration, the ZK integration still needs to account for what instance of the regionserver it is dealing with.""  How does this patch help the ZK integration effort? 

(I ask because you just made a fairly large commit without general notes on what is in the commit).;;;","14/Mar/09 18:14;jimk;Sorry, meant to include the list of changes (which I had written down), but forgot. Here they are:

Summary of change:

Server name is now hostname_startcode_port so it is unique among multiple instances of a region server running at the same address

Files changed:

HServerInfo
- Cache server name once it's been computed so that subsequent gets of the server name are a simple dereference. This required setServerAddress, setStartCode to be synchronized as well as getServerName.
- Return a copy of the embedded HServerAddress since an HRegionInfo can get reused and the reference returned can change out from other methods that have stored that reference.
- Changed hashCode, compareTo to just use the serverName as that is now unique.
- new static methods to compute serverName given a HServerInfo; hostName (name:port String) and startCode; and one that takes a HServerAddress and startCode.

HRegionInfo
- Not actually needed for this issue, but I had at one point thought I needed to change it, and when I discarded the change, I left the following in
- make regionName transient as it is not serialized
- fix javadoc for parseRegionName, shouldSplit

HMaster
- regionServerStartup can now throw an exception which comes out of ServerManager.regionServerStartup

ServerManager
- remove unused import
- the key for serversToServerInfo is still a string but is the server name as described above, and not host:port as it was previously. This allows us to distinguish between different instances of a region server at the same address. Zookeeper still uses a ServerExpirer object, but the name is now the server name as described above rather than a host:port pair.
- removed Boolean from deadServers Map. It was not used and is no longer needed. deadServers is now a set. deadServers is still used to detect a server that has not restarted but reports in after its lease has expired (perhaps due to a network partitioning). It cannot be given work until its existing logs have been recovered. 
- removed checkForGhostReferences. deadServers now handles servers that report back in after lease expiration but have not restarted. Servers that have restarted will have a new server name and can consequently be assigned work immediately.
- regionServerStartup can now throw Leases.LeaseStillHeldException. Changed String 's' to String 'serverName'.
- regionServerReport can now throw Leases.LeaseStillHeldException.
- processRegionServerExit, processRegionServerAllsWell, processMsgs, processRegionOpen now take a HServerInfo rather than a string server name
- processSplitRegion no longer takes either a server name or a serverInfo because they were unused.
- getServersToServerInfo, getServersToLoad now return a Collections.unmodifyableMap instead of a new HashMap
- getLoadToServers removed. Unreferenced.

RegionManager
- regionsInTransition is now a Map<String, RegionState> instead of Map<byte[], RegionState> key is server name described above
- assignRegions now just takes an HServerInfo instead of HServerInfo, String as that was redundant and the server name can be obtained from HServerInfo
- unassignSomeRegions, assignRegionsToOneServer, assignRegionsToMultipleServers take an HServerInfo rather than String
- regionIsInTransition, regionIsOpening, isPendingOpen, setOpen, isOfflined, setPendingClose, setClosed take String as argument instead of byte[] due to change in regionsInTransition map

BaseScanner
- checkAssigned now composes a server name as described above from serverAddress and startCode. Additionally it no longer needs to compare start codes as ServerManager.getServerInfo uses a server name as described above and since nothing will be found if the names do not match exactly (since the startcode is part of the name) the comparison of start codes is redundant. Finally it calls new static function HLog.getHLogDirectoryName instead of duplicating the code.

TableOperation
- now builds a server name as described above to pass to isBeingServed and processScanItem
- isBeingServed, processScanItem now take a server name as described above rather than host:port and start code

ChangeTableState
- processScanItem now takes a server name as described above rather than host:port and start code

ModifyTableMeta
- added @SuppressWarnings(""unused"") for server name argument in processScanItem

TableDelete
- added @SuppressWarnings(""unused"") for server name argument in processScanItem

ColumnOperation
- added @SuppressWarnings(""unused"") for server name argument in processScanItem

ProcessRegionOpen
- now stores the HServerInfo passed to the constructor rather than the HServerAddress and start code contained therein
- added if (LOG.isDebugEnabled()) { around LOG.debug calls
- other changes are related to extracting the HServerAddress and start code fields from the HServerInfo

ProcessServerShutdown
- no longer needs the HServerAddress as it constructs a server name as described above for comparision with entries scanned from meta region.

HLog
- Added new static methods getHLogDirectoryName that take either a HServerInfo a host:port string and start code or a string which is a server name as described above
;;;","19/Mar/09 01:00;ryanobjc;The web UI is broken now.  The web UI has dependencies on the servers to server info map, and the key format has changed but the JSP code has not.;;;","19/Mar/09 02:07;ryanobjc;the problem seems to be the new serverToServerInfo() map format...

previously the key was 'server:port'.  Now it is: 'server_startcode_port'.

The problem seems to be that the only place one has access to the startcode is if you already have the HServerInfo, which if you did, you wouldn't need the serversToServerInfo map anyways...;;;","19/Mar/09 04:37;ryanobjc;here is a proposed patch - the other option is to add a parallel map in addition to serversToServerInfo.  Considering the major purpose of this map is to feed into the JSPs i went with this approach instead.;;;","19/Mar/09 06:25;ryanobjc;ok the previous approach does not work.  Here is a new patch with the other approach.;;;","27/Mar/09 05:50;jimk;> ryan rawson added a comment - 18/Mar/09 09:37 PM
> here is a proposed patch - the other option is to add a parallel map in addition to serversToServerInfo.
> Considering the major purpose of this map is to feed into the JSPs i went with this approach instead.

The major purpose of serversToServerinfo is to keep track of which servers are alive.
This will change as Zookeeper integration expands.;;;","27/Mar/09 06:09;ryanobjc;have a look at the work i patched in on HBASE-1290, i added a second map to provide data for the JSP.;;;","27/Mar/09 06:09;ryanobjc;fixed in HBASE-1290;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cleanup thrift server; remove Text and profuse DEBUG messaging",HBASE-1142,12412870,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,tim_s,tim_s,tim_s,20/Jan/09 22:27,13/Sep/09 22:32,01/Jul/25 07:49,26/Feb/09 06:07,0.18.0,0.18.1,0.19.0,0.19.1,,0.19.1,,,,,,,0,"Ambiguous issue name.. sorry.

The thrift server has loads of getText(..) calls. Which is a local function that checks for utf8 compliance, we don't need them anywhere, because we don't use Text anymore.

There is probably other things we missed last time we updated the api, that we should also clean up while we're at it. Open to suggestions.",,,,,,,,,,,,,HBASE-889,,,,,,,,,,,,,,,,"20/Jan/09 22:29;tim_s;HBASE-1142.patch;https://issues.apache.org/jira/secure/attachment/12398345/HBASE-1142.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25599,,,,,Thu Feb 26 06:07:49 UTC 2009,,,,,,,,,,"0|i0hbkn:",99137,,,,,,,,,,,,,,,,,,,,,"20/Jan/09 22:29;tim_s;This also fixes HBASE-889 (you can just pass it an empty list of columns to scan all).;;;","21/Jan/09 23:26;stack;Thanks Tim.  Patch looks great.  Let me commit...;;;","21/Jan/09 23:52;stack;Committed.  Thanks for the patch Tim.

We should consider whether to add this to 0.19.1?;;;","22/Jan/09 13:47;tim_s;It doesn't change the api at all, so 19.1 makes sense. As it stands it enforces utf8 content row keys, definitely a bug.;;;","22/Jan/09 16:10;stack;Moving to 0.19.1 (I'll commit it there in a while).  Tim describes it as a bug (its already marked as a Bug).;;;","22/Jan/09 20:06;viper799;Tim: Can we get some updates to the demo clients also there is a function in ThriftServer mutateRows and mutateRowsTS, I assume there there to do batch updates to multi rows in one call but can not find an example on how to setup rowBatches.;;;","22/Jan/09 22:40;tim_s;Billy: mutateRows and mutateRowsTs both take a list of BatchMutations.
The Ts version lets you provide a timestamp. We need separate functions because thrift doesn't allow method overloading.

Maybe I'll might have a quick look at the python democlient on the weekend. Feel free to work on any of them :)
That's a separate issue.;;;","24/Jan/09 23:42;viper799;Question can mutateRows update more then one row or is it to update more then one column for a single row in one call?;;;","25/Jan/09 00:14;tim_s;I made another jira (HBASE-1153) to update the python example for that. mutateRow is for changes over multiple columns of one row. mutateRows is for changes of multiple columns over multiple rows.;;;","26/Feb/09 06:07;stack;Committed to branch.  Thanks for the patch Tim.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ant clean test"" fails",HBASE-1140,12412781,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,nitay,nitay,nitay,19/Jan/09 22:52,13/Sep/09 22:24,01/Jul/25 07:49,19/Jan/09 23:05,0.19.0,,,,,0.20.0,,,build,,,,0,"shrek:hbase2 nitay$ ant clean test
Buildfile: build.xml

clean:

compile-test:

BUILD FAILED
/Users/nitay/code/hbase2/build.xml:411: destination directory ""/Users/nitay/code/hbase2/build/test"" does not exist or is not a directory

Total time: 0 seconds",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/09 22:54;nitay;hbase-1140.patch;https://issues.apache.org/jira/secure/attachment/12398268/hbase-1140.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25597,,,,,Mon Jan 19 23:05:17 UTC 2009,,,,,,,,,,"0|i0hbk7:",99135,,,,,,,,,,,,,,,,,,,,,"19/Jan/09 22:54;nitay;after patch:

shrek:hbase2 nitay$ ant clean test
Buildfile: build.xml

clean:
   [delete] Deleting directory /Users/nitay/code/hbase2/build

clover.setup:

clover.info:
     [echo] 
     [echo]       Clover not found. Code coverage reports disabled.
     [echo]     

clover:

init:
    [mkdir] Created dir: /Users/nitay/code/hbase2/build
    [mkdir] Created dir: /Users/nitay/code/hbase2/build/classes
    [mkdir] Created dir: /Users/nitay/code/hbase2/build/test
    [mkdir] Created dir: /Users/nitay/code/hbase2/build/examples
    [mkdir] Created dir: /Users/nitay/code/hbase2/build/webapps
     [copy] Copying 6 files to /Users/nitay/code/hbase2/build/webapps

...;;;","19/Jan/09 23:05;stack;Thanks for the patch Nitay.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Server never leaves the dead list though logs have all been processed if crashed server had -ROOT- (seemingly),HBASE-1123,12412269,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,,stack,stack,13/Jan/09 05:36,11/Jun/22 20:30,01/Jul/25 07:49,17/Jun/09 08:12,0.19.0,,,,,,,,,,,,0,"Cluster is just hung after host that had -ROOT- completed splitting its logs... old server is just stuck on the dead list and never comes off it.
{code}
..
2009-01-13 01:09:36,448 [HMaster] DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 6 of 6: hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/log_XX.XX.XX.142_1231717984112_60020/hlog.dat.1231718928939
2009-01-13 01:09:37,396 [IPC Server handler 4 on 60000] DEBUG org.apache.hadoop.hbase.master.ServerManager: Waiting on XX.XX.XX142:60020 removal from dead list before processing report-for-duty request
2009-01-13 01:09:38,591 [HMaster] DEBUG org.apache.hadoop.hbase.regionserver.HLog: Creating new log file writer for path hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/TestTable/712889985/oldlogfile.log and region TestTable,0040922294,1231559109829
2009-01-13 01:09:38,670 [HMaster] DEBUG org.apache.hadoop.hbase.regionserver.HLog: Creating new log file writer for path hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/TestTable/484208094/oldlogfile.log and region TestTable,0042007133,1231628296909
2009-01-13 01:09:45,096 [HMaster] INFO org.apache.hadoop.hbase.regionserver.HLog: log file splitting completed for hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/log_XX.XX.XX.142_1231717984112_60020
2009-01-13 01:09:47,317 [SocketListener0-2] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cache hit for row <> in tableName .META.: location serverXX.XX.XX.142:60020, location region name .META.,,1
2009-01-13 01:09:47,416 [IPC Server handler 4 on 60000] DEBUG org.apache.hadoop.hbase.master.ServerManager: Waiting on XX.XX.XX142:60020 removal from dead list before processing report-for-duty request
2009-01-13 01:09:47,518 [IPC Server handler 3 on 60000] INFO org.apache.hadoop.hbase.master.RegionManager: assigning region -ROOT-,,0 to server XX.XX.XX141:60020
2009-01-13 01:09:49,007 [IPC Server handler 6 on 60000] DEBUG org.apache.hadoop.hbase.master.ServerManager: Total Load: 430, Num Servers: 3, Avg Load: 144.0
2009-01-13 01:09:50,219 [SocketListener0-0] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cache hit for row <> in tableName .META.: location server XX.XX.XX.142:60020, location region name .META.,,1
2009-01-13 01:09:50,539 [IPC Server handler 2 on 60000] INFO org.apache.hadoop.hbase.master.ServerManager: Received MSG_REPORT_PROCESS_OPEN: -ROOT-,,0 from XX.XX.XX.141:60020
2009-01-13 01:09:50,539 [IPC Server handler 2 on 60000] INFO org.apache.hadoop.hbase.master.ServerManager: Received MSG_REPORT_OPEN: -ROOT-,,0 from 208.76.44.141:60020
2009-01-13 01:09:50,719 [SocketListener0-3] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cache hit for row <> in tableName .META.: location server XX.XX.XX.142:60020, location region name .META.,,1
2009-01-13 01:09:50,967 [SocketListener0-4] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cache hit for row <> in tableName .META.: location serverXX.XX.XX.142:60020, location region name .META.,,1
2009-01-13 01:09:52,117 [SocketListener0-5] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cache hit for row <> in tableName .META.: location server XX.XX.XX.142:60020, location region name .META.,,1
....
2009-01-13 01:09:57,426 [IPC Server handler 4 on 60000] DEBUG org.apache.hadoop.hbase.master.ServerManager: Waiting on XX.XX.XX.142:60020 removal from dead list before processing report-for-duty request
....
2009-01-13 01:10:45,156 [HMaster] DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessServerShutdown of XX.XX.XX142:60020
2009-01-13 01:10:45,156 [HMaster] INFO org.apache.hadoop.hbase.master.RegionServerOperation: process shutdown of server XX.XX.XX.142:60020: logSplit: true, rootRescanned: false, numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
2009-01-13 01:10:45,156 [HMaster] DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanRootRegion: process server shutdown scanning root region on XX.XX.XX.141
2009-01-13 01:10:45,182 [HMaster] DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: process server shutdown scanning root region on XX.XX.XX.141 finished HMaster
2009-01-13 01:10:45,183 [HMaster] DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions: process server shutdown scanning .META.,,1 on XX.XX.XX.142:60020
2009-01-13 01:10:47,496 [IPC Server handler 4 on 60000] DEBUG org.apache.hadoop.hbase.master.ServerManager: Waiting on XX.XX.XX.142:60020 removal from dead list before processing report-for-duty request
2009-01-13 01:10:49,320 [IPC Server handler 8 on 60000] DEBUG org.apache.hadoop.hbase.master.ServerManager: Total Load: 431, Num Servers: 3, Avg Load: 144.0
.....
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/09 18:31;jimk;1123.patch;https://issues.apache.org/jira/secure/attachment/12398250/1123.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25586,,,,,Wed Jun 17 08:12:48 UTC 2009,,,,,,,,,,"0|i0hbgn:",99119,,,,,,,,,,,,,,,,,,,,,"19/Jan/09 18:20;jimk;A dead server is not removed from the deadServer list until ProcessServerShutdown has split the logs and
scanned all the meta regions for regions that the dead server was serving.

The root region will get reassigned immediately after the log is split (if the dead server was serving the
root region).

If the dead server was serving other regions, ProcessServerShutdown is requeued to wait for the root
region to come back on-line.

Once the root region is back on-line, it can be scanned for any meta regions that the dead server was
serving. If there are any, they are marked as unassigned and ProcessServerShutdown is requeued to
wait for all the meta regions to be on-line.

Once all the meta regions are on-line, they can be scanned and any that were being served by the
dead server are marked as unassigned.

At this point, the dead server is removed from the dead server list.;;;","19/Jan/09 18:31;jimk;This patch changes deadServers from a Map back to a Set as logs split is not used.;;;","19/Jan/09 19:42;jimk;Anticipating a different approach post ZooKeeper integration.;;;","19/Jan/09 20:03;stack;Approach to what?;;;","19/Jan/09 20:08;jimk;@stack: different approach to lease management;;;","19/Jan/09 20:11;jimk;There are a number of problems here:

- Leases should not only be identified by server name and port number
  but also by the server start code.  Since HLog directory names
  include the start code, if a server should crash and restart before
  the old lease expires, there is no danger of the new incarnation of
  the server overwriting the old instance's HLog. It can then be put
  back to work immediately.

- If a server's lease does time out, (because it hasn't reported in)
  and the region server reports in, we should not wait in a region
  server report thread in the master because cleaning up after the
  server could take longer than IPC timeout.

- The order in which events happen when a region server receives a
  MSG_CALL_SERVER_STARTUP is incorrect. The region server should call
  reportForDuty before creating a new HLog.
;;;","19/Jan/09 20:13;stack;The above seem like good stuff but how do any of the above impinge on this issue?;;;","19/Jan/09 20:38;jimk;> stack - 19/Jan/09 12:13 PM
> The above seem like good stuff but how do any of the above impinge
> on this issue? 

Here's what I think happened:

- region server crashed
- lease timed out
- master starts recovery (can take quite a while to complete)
- region server restarts
- region server sends region server startup message to master
- master waits in rpc handler for old server cleanup (because it
  cannot differentiate the new instance from the old). 
- ipc from region server to master times out
- region server sends a new startup message. The master thread starts
  waiting in the rpc handler for old server cleanup.
- ipc from region server to master times out

...

This could easily result in all the master's region server rpc
handlers waiting for essentially the same event until
ProcessServerShutdown completes and removes the original dead server
from the dead servers list. (and if not all the master's region server
threads are tied up, it would severely impair the master's ability to
respond to other region server requests)

;;;","19/Jan/09 21:01;stack;What would 'ipc from region server to master times out' look like?;;;","20/Jan/09 00:27;jimk;I would expect to see rpc retry messages in the region server log, followed by a LOG.error or LOG.warning with an exception.
;;;","20/Jan/09 00:35;stack;Why retry?  Its connected fine and no exceptions thrown out of the server.

I don't have a scenario so you are better than I in this respect.  Can you replicate what you describe (Just hold on to a HRS when it reports into master).  See if you can manufacture what you speculate?;;;","20/Jan/09 00:45;jimk;I'd expect it to be similar to what happens when a region server does not respond to a client because it is too busy.
Don't we get rpc retries in that case?

I don't have the region server log, so I can't tell exactly what happened in this case. Is there a region server log still
available from when this happened?;;;","20/Jan/09 00:49;stack;Its not similar.  We're already connected (Retries happen when we get exception or can't connect).  I don't have the log.   Suggest you try replicating.  Should be easy enough... just hold on to regionserver when it registers and not let go of it.;;;","26/Jan/09 20:12;jimk;On hbase-0.19 branch, I could not reproduce this. I killed the server holding root while cluster was under load
and it exited the waiting state in 1:01 (min:secs):

{code}
2009-01-26 19:26:32,266 INFO org.apache.hadoop.hbase.master.RegionManager: assigning region -ROOT-,,0 to server 208.76.44.141:8020

2009-01-26 19:41:08,396 INFO org.apache.hadoop.hbase.master.ServerManager: 208.76.44.141:8020 lease expired
2009-01-26 19:42:10,757 DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: Removed 208.76.44.141:8020 from deadservers Map
{code}

I then waited for the cluster to rebalance, again put it under load, and killed the server holding the root region.
It took a little longer (2 min 19 sec) before the server was removed from the dead list.

{code}
2009-01-26 19:41:11,808 INFO org.apache.hadoop.hbase.master.RegionManager: assigning region -ROOT-,,0 to server 208.76.44.139:8020

2009-01-26 19:49:01,966 INFO org.apache.hadoop.hbase.master.ServerManager: 208.76.44.139:8020 lease expired
2009-01-26 19:51:20,354 DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: Removed 208.76.44.139:8020 from deadservers Map
{code}

However, if leases included the start code, we could have put the restarted server back into service much sooner, as it
would not interfere with the splitting of logs (which include the start code in their name).

;;;","26/Jan/09 20:52;jimk;Marking as trivial but leaving open as I was unable to reproduce the problem. If we don't see this again 
after a while, we can then close the issue out.;;;","17/Jun/09 08:12;stack;Marking as fixed by HBASE-1457;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in HStoreScanner.updateReaders,HBASE-1107,12411558,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,apurtell,apurtell,apurtell,02/Jan/09 01:37,13/Sep/09 22:32,01/Jul/25 07:49,20/Mar/09 20:33,0.19.1,0.20.0,,,,0.19.1,,,,,,,1,"2009-01-01 23:55:41,629 FATAL org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: content,cff13605e2ea6ce0b221ac864687bf08,1230777531253
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:880)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:773)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:227)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.run(MemcacheFlusher.java:137)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HStoreScanner.updateReaders(HStoreScanner.java:322)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:737)
        at org.apache.hadoop.hbase.regionserver.HStore.updateReaders(HStore.java:725)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:694)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:630)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:865)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,,,,HBASE-1076,,,,,,"02/Jan/09 19:39;apurtell;1107-1.patch;https://issues.apache.org/jira/secure/attachment/12397033/1107-1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25576,,,,,Fri Mar 20 20:33:45 UTC 2009,,,,,,,,,,"0|i0hbdj:",99105,,,,,,,,,,,,,,,,,,,,,"02/Jan/09 18:30;apurtell;Right above line 322 in HStoreScanner.java is this comment:
\\
{code}
// I think it is safe getting key from mem at this stage -- it shouldn't have
// been flushed yet
{code}

So I think this is no longer true.;;;","02/Jan/09 18:45;apurtell;Attached patch 1107-1 addresses the symptom. Is it enough?;;;","05/Jan/09 20:23;stack;I think this NPE is a symptom of our making the list of changed readers observers sloppier using CopyOnWrite.  I think the scanner has been closed.  The first thing done on close is that we remove ourselves from the list of changed readers observers but my guess is that there is an outstanding iteration going on.  It still has a reference and invoked the observer code AFTER the clearing of the memcache scanner.

I'm not sure your attached patch safe enough Andrew.  My worry is that between your new check on #320, this.keys[MEMS_INDEX] could be cleared before its use on #327 by another thread.  What you think?

The observer code -- updateReaders method -- shouldn't run if close has been invoked methinks.  I was going to add in a 'close' flag.  Seems like overkill but ain't sure what else to use as 'close' indicator:

{code}
Index: src/java/org/apache/hadoop/hbase/regionserver/HStoreScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HStoreScanner.java    (revision 731185)
+++ src/java/org/apache/hadoop/hbase/regionserver/HStoreScanner.java    (working copy)
@@ -26,6 +26,7 @@
 import java.util.Set;
 import java.util.SortedMap;
 import java.util.TreeMap;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
 import org.apache.commons.logging.Log;
@@ -60,6 +61,8 @@
   // Used around transition from no storefile to the first.
   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
   
+  private final AtomicBoolean closing = new AtomicBoolean(false);
+  
   /** Create an Scanner with a handle on the memcache and HStore files. */
   @SuppressWarnings(""unchecked"")
   HStoreScanner(HStore store, byte [][] targetCols, byte [] firstRow,
@@ -294,6 +297,7 @@
   }
 
   public void close() {
+    this.closing.set(true);
     this.store.deleteChangedReaderObserver(this);
     doClose();
   }
@@ -309,6 +313,9 @@
   // Implementation of ChangedReadersObserver
   
   public void updateReaders() throws IOException {
+    if (this.closing.get()) {
+      return;
+    }
     this.lock.writeLock().lock();
     try {
       MapFile.Reader [] readers = this.store.getReaders();
{code};;;","05/Jan/09 20:34;apurtell;Concur that the scanner is closed when the observer gets it.

+1

I'll commit. ;;;","05/Jan/09 21:28;apurtell;Committed. Passes all local tests. ;;;","25/Jan/09 04:08;apurtell;Seen again, on 0.19.0 release:

2009-01-25 03:56:39,324 FATAL org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Replay of hlog required. Forcing server shutdown org.apache.hadoop.hbase.DroppedSnapshotException: region: urls,http|img3.megavideo.com|80|a|8|8b3035357a222073d30d09f2c4ae85.jpg,1232408450783
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:896)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:789)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:227)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushSomeRegions(MemcacheFlusher.java:291)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.reclaimMemcacheMemory(MemcacheFlusher.java:261)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdates(HRegionServer.java:1614)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:895)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HStoreScanner.updateReaders(HStoreScanner.java:330)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:737)
        at org.apache.hadoop.hbase.regionserver.HStore.updateReaders(HStore.java:725)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:694)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:630)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:881)
        ... 10 more
;;;","25/Jan/09 06:54;stack;Thats crazy!  I must be digging the hole in the wrong spot?;;;","09/Feb/09 00:40;apurtell;Got another of these today.

2009-02-09 00:16:55,081 FATAL org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: content,26ac2c3fd24ac7ba1032e7b68973b9fa,1233995252171
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:896)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:789)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:227)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushSomeRegions(MemcacheFlusher.java:291)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.reclaimMemcacheMemory(MemcacheFlusher.java:261)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdates(HRegionServer.java:1619)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:895)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HStoreScanner.updateReaders(HStoreScanner.java:330)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:737)
        at org.apache.hadoop.hbase.regionserver.HStore.updateReaders(HStore.java:725)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:694)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:630)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:881)
        ... 10 more
;;;","13/Mar/09 23:43;davelatham;This problem has been taking down regionservers regularly for me as well.  Running hbase 0.19.0 on hadoop 0.19.0 with 4 slaves.  It's becoming a major stability issue for us.

2009-03-13 16:30:47,728 FATAL org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: sessions,^@^@^@^@^@^@^Cd^@^@^A^_ï¿½xï¿½q^@.IPHONEc8532c72874b66d1165c2f62ed91ef353eb9,1235185327583
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:896)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:789)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:227)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.run(MemcacheFlusher.java:137)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HStoreScanner.updateReaders(HStoreScanner.java:330)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:737)
        at org.apache.hadoop.hbase.regionserver.HStore.updateReaders(HStore.java:725)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:694)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:630)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:881)
        ... 3 more;;;","14/Mar/09 18:31;stack;The code in updateReaders was changed significantly for 0.19.1 (We don't open readers we already had open).

The NPE above as Andrew has already pointed out comes from here:

{code}
          // I think its safe getting key from mem at this stage -- it shouldn't have
          // been flushed yet
          this.scanners[HSFS_INDEX] = new StoreFileScanner(this.store,
              this.timestamp, this. targetCols, this.keys[MEMS_INDEX].getRow());
{code}

It looks like its the this.keys[MEMS_INDEX] is null.

In testing while doing the 0.19.1 changes I was able to make this NPE.   It happened when updateReaders was called after the both memory and the file scanners had been exhausted but the scanner had not yet been closed down fully.  The attempt to get from the memory key was failing because it had been set to null after its last next.

Now, we keep the last next key and use that instead of trying to go to memcache (thinking on it, memcache exhaustion can always happen ahead of store file scanner exhaustion).;;;","19/Mar/09 22:24;apurtell;So head of 0.19 branch includes a resolution for this?;;;","19/Mar/09 23:02;stack;Yeah, should be fixed by 0.19.1 by my reckoning.;;;","20/Mar/09 20:33;apurtell;Marking as resolved in 0.19.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DFS failures did not shutdown regionserver,HBASE-1087,12411261,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,streamy,streamy,23/Dec/08 18:36,13/Sep/09 22:26,01/Jul/25 07:49,26/Dec/08 20:21,0.19.0,,,,,0.19.0,,,,,,,0,"I lost three Datanodes, reasons of which are still being investigated, but it has left a number of regions unable to be written to.

Relevant logs:

2008-12-23 02:35:59,591 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcher.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:122)
        at sun.nio.ch.IOUtil.write(IOUtil.java:93)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:352)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:140)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
        at java.io.DataOutputStream.write(DataOutputStream.java:107)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2209)

2008-12-23 02:35:59,591 WARN org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_3615512604618056881_86411java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:197)
        at java.io.DataInputStream.readLong(DataInputStream.java:416)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2318)

2008-12-23 02:35:59,591 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_3615512604618056881_86411 bad datanode[0] 72.34.249.214:50010
2008-12-23 02:35:59,595 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_3615512604618056881_86411 in pipeline 72.34.249.214:50010, 72.34.249.213:50010, 72.34.249.219:50010: bad datanode 72.34.249.214:50010

2008-12-23 02:38:27,698 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
2008-12-23 02:38:27,698 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-3678518999439029831_86910
2008-12-23 02:38:27,711 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock 5392007859847346106 has been explicitly released by client
2008-12-23 02:38:30,048 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock -5905479324505886709 explicitly acquired by client
2008-12-23 02:38:33,700 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
2008-12-23 02:38:33,700 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-226119866881174578_86911
2008-12-23 02:38:34,908 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock 346704317670569896 explicitly acquired by client
2008-12-23 02:38:39,702 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
2008-12-23 02:38:39,702 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_1719395740576248920_86913
2008-12-23 02:38:40,945 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock 3819942931078736534 explicitly acquired by client
2008-12-23 02:38:45,572 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock 254119037927296402 explicitly acquired by client
2008-12-23 02:38:45,703 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
2008-12-23 02:38:45,703 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_2443399503093377808_86915
2008-12-23 02:38:49,092 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock 8573046623144113301 explicitly acquired by client
2008-12-23 02:38:49,385 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock 7686739650257547105 explicitly acquired by client
2008-12-23 02:38:49,512 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Row lock 5582966798894532276 explicitly acquired by client
2008-12-23 02:38:51,704 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2723)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:1997)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2183)

2008-12-23 02:38:51,704 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_2443399503093377808_86915 bad datanode[0] nodes == null
2008-12-23 02:38:51,704 WARN org.apache.hadoop.hdfs.DFSClient: Could not get block locations. Aborting...
2008-12-23 02:38:51,704 FATAL org.apache.hadoop.hbase.regionserver.HLog: Could not append. Requesting close of log
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
        at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:118)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2748)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2704)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:1997)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2183)
2008-12-23 02:38:51,706 ERROR org.apache.hadoop.hbase.regionserver.LogRoller: Log rolling failed with ioe:
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
        at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:118)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2748)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2704)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:1997)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2183)
2008-12-23 02:38:51,706 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: java.net.ConnectException: Connection refused
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/08 18:02;streamy;regionserver.threaddump.log;https://issues.apache.org/jira/secure/attachment/12396745/regionserver.threaddump.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25565,,,,,Fri Dec 26 20:21:58 UTC 2008,,,,,,,,,,"0|i0hb8v:",99084,,,,,,,,,,,,,,,,,,,,,"24/Dec/08 00:05;stack;I would have thought that '2008-12-23 02:38:51,704 FATAL org.apache.hadoop.hbase.regionserver.HLog: Could not append. Requesting close of log' should have triggered a hrs shutdown.;;;","24/Dec/08 18:02;streamy;Same thing happened again.  Managed to get a thread dump this time, though nothing jumped out at me in it.;;;","24/Dec/08 18:17;stack;Looking at Jon's thread dump, its a happy HRS all waiting and runnable.  The continual exceptions trying to append and roll log are not manifest here.

I'm committing below:

{code}
Index: src/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/LogRoller.java        (revision 729178)
+++ src/java/org/apache/hadoop/hbase/regionserver/LogRoller.java        (working copy)
@@ -69,6 +69,9 @@
       } catch (FailedLogCloseException e) {
         LOG.fatal(""Forcing server shutdown"", e);
         server.abort();
+      } catch (java.net.ConnectException e) {
+        LOG.fatal(""Forcing server shutdown"", e);
+        server.abort();
       } catch (IOException ex) {
         LOG.error(""Log rolling failed with ioe: "",
             RemoteExceptionHandler.checkIOException(ex));
{code}

... as a reason to restart.  We fail to append and schedule a log roll as a desperate attempt at getting up a new context.  The ConnectException you'd think would resolve itself but if its come up out of DFSClient, then it looks permanent.;;;","24/Dec/08 18:30;stack;Datanode did this and then shut itself down nicely:

{code}
2008-12-23 02:35:49,487 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(72.34.249.214:50010, storageID=DS-1476108653-72.34.249.214-50010-1229363688235, infoPort=50075, ipcPort=50020):DataXceiveServer: Exiting due to:java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:614)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
        at java.lang.Thread.run(Thread.java:636)
{code}
;;;","26/Dec/08 20:21;stack;Resolving this issue.  Lets reopen or open another if we seen another hang like this (or add it under hbase-1084 umbrella).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction fails if bloomfilters are enabled,HBASE-1039,12409512,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,apurtell,apurtell,30/Nov/08 18:44,13/Sep/09 22:26,01/Jul/25 07:49,03/Dec/08 23:14,0.18.1,,,,,0.19.0,,,regionserver,,,,0,"From Thibaut up on the list.

As soon as hbase tries to compact the table, the following exception appears in the logfile: (Other compactations also work fine without any errors)

2008-11-30 00:55:57,769 ERROR
org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region mytable,,1228002541526
java.lang.IllegalArgumentException: maxValue must be > 0
    at org.onelab.filter.HashFunction.<init>(HashFunction.java:84)
    at org.onelab.filter.Filter.<init>(Filter.java:97)
    at org.onelab.filter.BloomFilter.<init>(BloomFilter.java:102)
    at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Writer.<init>(HStoreFile.java:829)
    at org.apache.hadoop.hbase.regionserver.HStoreFile.getWriter(HStoreFile.java:436)
    at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:889)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:902)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:860)
    at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:83)

Because the region cannot compact and/or split, it is soon dead after (re)assignment.",,,,,,,,,,,,,,,,,,HBASE-1047,,,,,,,,,,,"03/Dec/08 21:31;stack;1039.patch;https://issues.apache.org/jira/secure/attachment/12395215/1039.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25535,,,,,Wed Dec 03 23:02:35 UTC 2008,,,,,,,,,,"0|i0hayf:",99037,,,,,,,,,,,,,,,,,,,,,"30/Nov/08 18:51;apurtell;Getting more information from Thibaut via email...;;;","30/Nov/08 21:11;midnightcoder;There seems to be some talk that HFS will incorporate bloom filter code. Anyone know the status on this or how it will impact the need for hbase to implement this?;;;","01/Dec/08 14:43;apurtell;One crucial detail it seems is that the bloomfilter related exception happens even when no bloomfilters are enabled in the schema.  There are also DFS related exceptions. 

From Thibaut:

I created all the tables from scratch and didn't change them at run time. The schema for all the tables right now is as followed. (data is a bytearray of a serialized google buffer object)

    {NAME => 'entries', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'data', BLOOMFILTER => 'false', COMPRESSION => 'NONE', VERSIONS => '3', LENGTH => '2147483647', TTL => '-1', IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}

I reran everything from scratch with the new table scheme and got the same exception again, just on a different table this time: (Disabling the bloomfilter, compression and the blockcache doesn't seem to have any effect)

2008-11-30 23:22:20,774 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region entries,,1228075277421
java.lang.IllegalArgumentException: maxValue must be > 0
    at org.onelab.filter.HashFunction.<init>(HashFunction.java:84)
    at org.onelab.filter.Filter.<init>(Filter.java:97)
    at org.onelab.filter.BloomFilter.<init>(BloomFilter.java:102)
    at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Writer.<init>(HStoreFile.java:829)
    at org.apache.hadoop.hbase.regionserver.HStoreFile.getWriter(HStoreFile.java:436)
    at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:889)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:902)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:860)
    at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:83)

The log file is also full of these kind of errors: (before and after)

2008-11-30 23:22:44,500 INFO org.apache.hadoop.ipc.Server: IPC Server handler 16 on 60020, call next(8976385860586379110) from x.x.x.203:52747: error: org.apache.hadoop.hbase.UnknownScannerException: Name: 8976385860586379110
    at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1077)
    at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:585)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:554)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)

Shortly afterwards I got the dfs error on the regionserver again, on a different table though (and might be completely unreleated and not important?):

2008-11-30 23:26:55,885 WARN org.apache.hadoop.dfs.DFSClient: Exception while reading from blk_-9066140877711029349_706715 of /hbase/webrequestscache/2091560474/data/mapfiles/1510543474646532027/data from x.x.x.204:50010: java.io.IOException: Premeture EOF from inputStream
    at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:102)
    at org.apache.hadoop.dfs.DFSClient$BlockReader.readChunk(DFSClient.java:996)
    at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:236)
    at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:191)
    at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)
    at org.apache.hadoop.dfs.DFSClient$BlockReader.read(DFSClient.java:858)
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1384)
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1420)
    at java.io.DataInputStream.readFully(DataInputStream.java:176)
    at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:64)
    at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:102)
    at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1933)
    at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1833)
    at org.apache.hadoop.io.MapFile$Reader.seekInternal(MapFile.java:463)
    at org.apache.hadoop.io.MapFile$Reader.getClosest(MapFile.java:558)
    at org.apache.hadoop.io.MapFile$Reader.getClosest(MapFile.java:541)
    at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader.getClosest(HStoreFile.java:761)
    at org.apache.hadoop.hbase.regionserver.HStore.get(HStore.java:1291)
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:1154)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1020)
    at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:585)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:554)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)

Datnode entries related to that block:

08/11/30 22:44:19 INFO dfs.DataNode: Receiving block blk_-9066140877711029349_706715 src: /x.x.x.204:44313 dest: /x.x.x.204:50010
08/11/30 22:44:41 INFO dfs.DataNode: Received block blk_-9066140877711029349_706715 of size 33554432 from /x.x.x.204
08/11/30 22:44:41 INFO dfs.DataNode: PacketResponder 3 for block blk_-9066140877711029349_706715 terminating
08/11/30 22:53:18 WARN dfs.DataNode: DatanodeRegistration(x.x.x.204:50010, storageID=DS-364968361-x.x.x.204-50010-1220223683238, infoPort=50075, ipcPort=50020):Got exception while serving blk_-9066140877711029349_706715 to /x.x.x.204: java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch :java.nio.channels.SocketChannel[connected local=/x.x.x.204:50010 remote=/x.x.x.204:46220]
        at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:185)
        at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)
        at org.apache.hadoop.dfs.DataNode$BlockSender.sendChunks(DataNode.java:1873)
        at org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1967)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1109)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1037)
        at java.lang.Thread.run(Thread.java:595)
;;;","01/Dec/08 17:30;jimk;You can neither enable nor disable bloomfilters once a column has data in it.

If you enable it on a table with existing data, compact assumes all stores have a bloom filter and will NPE because it cannot read it.

If you disable bloom filters on a table that has data in it, compact will fail because the store file knows it has a bloom filter.

It is easier to fix the latter than the former by going through the store files and deleting the bloom filter file.

Enabling bloom filters after the table has data in it is much harder as all store files must be read and a bloom filter created for each.

It would be better to disallow the enabling of bloom filters once the table has been created. This would at least prevent shooting yourself in the foot.;;;","01/Dec/08 18:54;apurtell;According to the reporter (Thibaut, on hbase-user@)), the table schema never uses bloomfilters yet the bloomfilter related exceptions occur. ;;;","01/Dec/08 19:03;stack;Why not have the bloom Writer just not build a bloom filter if ALL inputs don't already have blooms rather than NPE (in getReaders, if an input doesn't have nrows, set it to -1)?   Could output a warning and just carry on.  New flushes will include bloom filters so subsequent compactions will have bloom filters to hand.  Eventually all inputs will have bloom filters and only then on compaction, write out compacted file with blooms.

Adding disallow set/unset would be awkward in implementation; i.e. providing the appropriate context that determines when a flag is settable or not in HTD.





;;;","01/Dec/08 19:12;stack;bq. There seems to be some talk that HFS will incorporate bloom filter code. Anyone know the status on this or how it will impact the need for hbase to implement this?

Do you have an issue id where this is discussed Bruce?

That you'd get the bloom filter exception on table that doesn't have it enabled -- or that never had it enabled in the past is odd... difficult to explain.;;;","01/Dec/08 19:14;streamy;Correction from Thibaut on list:

{quote}
You are right. I just saw that that table (the only table) has indeed the bloomfilters and compression enabled (blockcache is disabled).
{NAME => 'entries', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'data', BLOOMFILTER => 'true', COMPRESSION => 'BLOCK', VERSIONS => '3', LENGTH => '2147483647', TTL => '-1', IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}

As for the load/dfs errors, thanks for explaining this. I was only starting up one region server to have all the log entries at one place, and it was indeed under heavy load. (Multiple threads setting/getting keys)
{quote}

He originally said 8 node cluster, but now makes mention of only single regionserver.  So one problem seems related to blooms, probably also seeing load-related issues.;;;","02/Dec/08 01:56;apurtell;+1 on the idea of allowing alteration of table schema to enable bloomfilters after the fact. Current situation with ""java.lang.IllegalArgumentException: maxValue must be > 0"" exceptions in that case violate principle of least surprise. ;;;","03/Dec/08 21:31;stack;Writing, if illegal filter parameters passed, continue with a warning.  If reading, if can't find the filter file, continue with warning.;;;","03/Dec/08 21:32;stack;Bring into 0.19.0.  Effects apurtell.  Compactions are failing and so his regions continue to grow.  Fellas can't undo bloomfilters w/o throwing away data w/o this patch.;;;","03/Dec/08 22:35;apurtell;+1 on the 1039 patch. I'll try keeping my data and seeing if this will clear the problem. At least if the compactions succeed, that's a good step forward.;;;","03/Dec/08 22:42;stack;Committed.  Waiting on confirmation that this patch actually works before closing.;;;","03/Dec/08 23:02;apurtell;Confirmed. I see the warning in the regionserver logs, then messages that indicate compaction completed successfully. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleting a column in MapReduce fails,HBASE-982,12407810,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,dogacan,dogacan,dogacan,04/Nov/08 20:56,20/Sep/12 22:10,01/Jul/25 07:49,04/Nov/08 21:40,0.19.0,,,,,0.19.0,,,,,,,0,"In latest trunk, deleting a column in BatchUpdate causes exception because BatchUpdate's copy constructor (or whatever they are called in java) directly calls BatchUpdate#put even in delete-s thus causing put to throw IllegalArgumentException(""Passed value cannot be null"").",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/08 20:57;dogacan;batchupdate-del.patch;https://issues.apache.org/jira/secure/attachment/12393328/batchupdate-del.patch","04/Nov/08 21:16;dogacan;batchupdate-delv2.patch;https://issues.apache.org/jira/secure/attachment/12393333/batchupdate-delv2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25509,,,,,Tue Nov 04 21:40:29 UTC 2008,,,,,,,,,,"0|i0hamv:",98985,,,,,,,,,,,,,,,,,,,,,"04/Nov/08 20:57;dogacan;Patch that calls BatchUpdate#delete if value is null.;;;","04/Nov/08 21:16;dogacan;Fixing my patch so that it calls bo.getValue only once. (Heh, I got a <10 line patch wrong :D);;;","04/Nov/08 21:40;stack;Thanks for the patch Doğacan;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable.commit no longer works with existing RowLocks though it's still in API,HBASE-950,12407040,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,streamy,streamy,22/Oct/08 21:59,13/Sep/09 22:26,01/Jul/25 07:49,22/Oct/08 23:49,0.19.0,,,,,0.19.0,,,,,,,0,"Introduced by HBASE-748, the RowLock passed into HTable.commit is now ignored.

This causes the update the hang until that rowlock expires, and then it proceeds with getting a new row lock.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/08 22:52;jdcryans;hbase-950.patch;https://issues.apache.org/jira/secure/attachment/12392683/hbase-950.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25493,,,,,Wed Oct 22 23:49:10 UTC 2008,,,,,,,,,,"0|i0hafz:",98954,,,,,,,,,,,,,,,,,,,,,"22/Oct/08 22:52;jdcryans;Temporary fix until 880 comes in. Please review.;;;","22/Oct/08 23:44;streamy;Tested, +1

Thanks JD;;;","22/Oct/08 23:49;jdcryans;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
major compaction period is not checked periodically,HBASE-938,12406772,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,rafan,rafan,20/Oct/08 02:25,28/Dec/09 19:24,01/Jul/25 07:49,15/Nov/08 00:36,0.18.0,0.18.1,,,,,,,regionserver,,,,0,"The major compaction period, hbase.hregion.majorcompaction, is not checked periodically. Currently, we only request major compaction when the region is open or split at which point we check whether the major compaction period is due.",HBase 0.18 branch (should be RC1) + Hadoop 0.18 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/08 22:37;stack;938-v4.patch;https://issues.apache.org/jira/secure/attachment/12393902/938-v4.patch","14/Nov/08 20:27;stack;938-v6.patch;https://issues.apache.org/jira/secure/attachment/12393957/938-v6.patch","15/Nov/08 00:29;stack;938-v7.patch;https://issues.apache.org/jira/secure/attachment/12393972/938-v7.patch","12/Nov/08 01:10;stack;938.patch;https://issues.apache.org/jira/secure/attachment/12393752/938.patch","11/Nov/08 23:17;stack;major.patch;https://issues.apache.org/jira/secure/attachment/12393743/major.patch",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25487,,,,,Fri Dec 19 20:15:56 UTC 2008,,,,,,,,,,"0|i0hadb:",98942,,,,,,,,,,,,,,,,,,,,,"20/Oct/08 02:56;rafan;In a ~400 regions cluster, I see 140 major compaction within 40 mins after I started the cluster, and this is still on-going. ;;;","20/Oct/08 06:01;rafan;After few hours, all regions got a major compaction.;;;","20/Oct/08 16:42;stack;Marking critical fix for 0.19.  Major compactions are expensive, especially in clusters that tend toward the large.  Seems an easy fix persisting last major compaction time so doesn't happen on every restart.;;;","20/Oct/08 20:51;viper799;Major compaction check is done in the compaction check so it should be getting checked when there is a memcache flush,open,or a splt.
There should not be major compaction triggered for all region on a restart that's not how the code is written to do the major compactions.

If a table gets no update to trigger a compaction check then the stale (over ttl and max_versions) data never gets removed form the table.
I have seen this happen on a idle table with no updates over a long time.

What we should be doing I thank is on the optional flush is queue up a compaction check where there is something to flush or not that way if a major compaction is needed it will run with in the optional flush time setting
This would allow us to check the hbase.hregion.majorcompaction more periodically on tables with little updates.
The way the code is now If there is no minor or major compaction needed then it will do do nothing so costing no extra resources to check if a major compaction is needed.
;;;","31/Oct/08 22:18;stack;I need to look into this but I just noticed how Billy made sure we never compact more than often than the major compaction period, even if restarts in between:

{code}
        long lowTimestamp = getLowestTimestamp(fs, mapdir);
        lastMajorCompaction = System.currentTimeMillis() - lowTimestamp;
        if (lowTimestamp < (System.currentTimeMillis() - majorCompactionTime) &&
            lowTimestamp > 0l) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(""Major compaction triggered on store: "" +
              this.storeNameStr + "". Time since last major compaction: "" +
              ((System.currentTimeMillis() - lowTimestamp)/1000) + "" seconds"");
          }
...
{code}

We look at file timestamps and if oldest file was written > major compaction period ago, then we run major compaction next time we check compactions.

So, for this issue, I need to confirm we only compact on a period and two, since optional flush was removed when we implemented appends, needs to be a thread or something that runs on a period looking to see if any regions in needs of major compaction.;;;","01/Nov/08 03:11;viper799;Looking at trunk my idea above will not work because we do not have the optional flush any more.

But yes the major compaction is based off the oldest timestamps from the mapfile's on the store level.
So restarts do not interfere with the Major compactions but will queue up over due Major Compactions because of the open.
;;;","11/Nov/08 23:17;stack;Patch to add a thread that checks for major compactions.   Still in need of startup.;;;","12/Nov/08 01:10;stack;More complete patch.  Testing now.;;;","12/Nov/08 05:27;stack;Patch seems to be working but it introduces a new issue in that it ensures that we check for major compaction every 24 hours and that a major compation will run every 24 hours, even if no flushes have come through meantime.   We don't want all hbase data rewritten every 24 hours.

Will try and fix as part of this issue.;;;","12/Nov/08 06:14;viper799;default I set my Major Compaction to once a week but on topic we need major compaction to run sometime even if no updates so we will able to remove expired ttl data in a timely fashion. not sure if timely = daily guess that depends on your setup and data.

Guess we need to decide whats acceptable ttl of expired ttl data.
;;;","12/Nov/08 06:36;stack;As is, if we did a major compaction at the start of the week and then if no updates during the whole week, at the end of the next week, we'll rewrite an already major compacted file.  I suppose even if no updates, ttls could have expired. Otherwise, Its a waste of CPU and network bandwidth.

You think we should up the major compaction time default to be a week?;;;","12/Nov/08 07:32;viper799;On a small cluster with little data it would not not matter so much but once someone got any real amount of data I would thank once a day it way to much for a default.
So once a week I thank will be more of the norm as data sets get more data.
;;;","12/Nov/08 15:05;stack;I will up the period, make it two days at least.

But was thinking too that we should mark files that have been major compacted -- write the fact into the files metadata or into the file name -- and before running another, check the column descriptor to see if TTL is forever; if it is, do not run another major compaction if only one file to compact.;;;","12/Nov/08 16:14;viper799;I like that idea sounds good to me. 
If we can write it in to the meta data then if only one file there and major compaction do as you said above
that would make the major compaction much smarter and save on cpu and bandwidth.;;;","13/Nov/08 00:32;stack;Looking at this, best for now is writing the fact that the HStoreFile is result of major compaction into the HSF info file.   When we change file formats, we'll clean all this up but this should work for now.  Then, yeah, when compacting, if only one file and if we're doing a major compaction, and if < column family TTL has passed, don't do a new major compaction.  Should save a bunch of CPU/nio.;;;","13/Nov/08 22:37;stack;This patch adds whether or not its a major compaction to the info file.  Then, when compacting, if major, will not do another major compaction if last compaction was one (or if time since last major compaction is < ttl).

Testing now.;;;","15/Nov/08 00:29;stack;Now you'll see messages like this if we try to do major compaction on a file that has already been major compacted:

{code}
2008-11-15 00:19:30,441 [regionserver/0:0:0:0:0:0:0:0:60020.compactor] DEBUG org.apache.hadoop.hbase.regionserver.HStore: Skipping major compaction because one major compacted file only and elapsedTime 1250119 is < ttl -1
{code};;;","15/Nov/08 00:36;stack;Committed.;;;","19/Dec/08 20:15;stack;Adding to 0.18.2 because of discussion up on list.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing hbase.regions.slop in hbase-default.xml for 0.18 branch,HBASE-933,12406604,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,stack,rafan,rafan,16/Oct/08 15:09,13/Sep/09 22:32,01/Jul/25 07:49,16/Oct/08 17:06,0.18.1,,,,,0.18.1,,,,,,,0,HBASE-920 introduces a new configuration variable: hbase.regions.slop. It is missing in hbase-default.xml of 0.18 branch while trunk has the correct information.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/08 15:19;rafan;933-0.18.1.patch;https://issues.apache.org/jira/secure/attachment/12392251/933-0.18.1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25484,,,,,Thu Oct 16 17:06:39 UTC 2008,,,,,,,,,,"0|i0hac7:",98937,,,,,,,,,,,,,,,,,,,,,"16/Oct/08 15:19;rafan;patch for 0.18 branch;;;","16/Oct/08 17:06;stack;Committed. Thanks for the patch Rong-en;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the version string in build.xml is wrong,HBASE-909,12405510,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,rafan,rafan,rafan,01/Oct/08 01:53,13/Sep/09 22:32,01/Jul/25 07:49,01/Oct/08 16:49,0.18.1,,,,,0.18.1,,,build,,,,0,The version string should be 0.18.1-dev since 0.18.0 is released. This affects 0.18 branch only.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/08 01:53;rafan;909.patch;https://issues.apache.org/jira/secure/attachment/12391259/909.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25470,,,,,Wed Oct 01 16:49:41 UTC 2008,,,,,,,,,,"0|i0ha6v:",98913,,,,,,,,,,,,,,,,,,,,,"01/Oct/08 16:49;stack;Committed.  Thanks you Rong-en for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regionserver memory leak causing OOME during relatively modest bulk importing,HBASE-900,12405107,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,streamy,streamy,24/Sep/08 23:53,13/Sep/09 22:26,01/Jul/25 07:49,19/Dec/08 19:03,0.18.1,0.19.0,,,,0.19.0,,,,,,,0,"I have recreated this issue several times and it appears to have been introduced in 0.2.

During an import to a single table, memory usage of individual region servers grows w/o bounds and when set to the default 1GB it will eventually die with OOME.  This has happened to me as well as Daniel Ploeg on the mailing list.  In my case, I have 10 RS nodes and OOME happens w/ 1GB heap at only about 30-35 regions per RS.  In previous versions, I have imported to several hundred regions per RS with default heap size.

I am able to get past this by increasing the max heap to 2GB.  However, the appearance of this in newer versions leads me to believe there is now some kind of memory leak happening in the region servers during import.",,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1018,HBASE-1019,,,"19/Dec/08 02:27;stack;900-p4.patch;https://issues.apache.org/jira/secure/attachment/12396452/900-p4.patch","12/Dec/08 01:11;stack;900-part2-v4.patch;https://issues.apache.org/jira/secure/attachment/12395894/900-part2-v4.patch","12/Dec/08 20:18;stack;900-part2-v5.patch;https://issues.apache.org/jira/secure/attachment/12395969/900-part2-v5.patch","12/Dec/08 22:28;stack;900-part2-v7.patch;https://issues.apache.org/jira/secure/attachment/12395975/900-part2-v7.patch","11/Dec/08 01:26;stack;900-part2.patch;https://issues.apache.org/jira/secure/attachment/12395784/900-part2.patch","07/Dec/08 09:21;stack;900.patch;https://issues.apache.org/jira/secure/attachment/12395502/900.patch","16/Dec/08 06:37;stack;hbase-900-part3.patch;https://issues.apache.org/jira/secure/attachment/12396162/hbase-900-part3.patch","15/Oct/08 18:01;stack;memoryOn13.png;https://issues.apache.org/jira/secure/attachment/12392193/memoryOn13.png",,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25466,,,,,Sat Dec 20 00:49:01 UTC 2008,,,,,,,,,,"0|i0ha4v:",98904,,,,,,,,,,,,,,,,,,,,,"15/Oct/08 03:49;stack;I got 185 PE regions into single HRS before HDFS went bad; about 32M rows of 1k values and 10byte keys.  Profiling, only thing that grows in memory is the count of HSKs.  Each store file we open has an index of long to HSK.  As the upload progresses, more index is in memory.

Was going to move this out of 0.18.1 since its not obviously broken but then talked to Rong-en.  He says that it was reading where he was seeing memory issues.  Will try a read test.;;;","15/Oct/08 03:56;stack;Looks like it was actually 225 regions before things went bad (need to be more patient).;;;","15/Oct/08 09:09;rafan;I see some possible memory leaks from regionserver after running it w/ ~200 regions per node for few days (it keeps receive read traffic with very few write). My used swap grows slowly. Region servers occupies around 3G memory (both virtual and rss shown in top). Once I restart regionserver, swap space is also freed.

This is with hbase 0.2.x on hadoop 0.17.x.;;;","15/Oct/08 18:01;stack;I ran randomread test over night w/ gc logging enabled.  Here are snippets from the gc log from different times during the night showing full gcs:

{code}
3738.529: [Full GC 107893K->86326K(220480K), 0.3393940 secs]
3944.907: [Full GC 110079K->90694K(212160K), 0.3828950 secs]
...
43142.078: [Full GC 105996K->82458K(139840K), 0.3558530 secs]
43339.019: [Full GC 102767K->86387K(190656K), 0.3512450 secs]
43490.046: [Full GC 105187K->87709K(212288K), 0.3523640 secs]
43735.589: [Full GC 107799K->88233K(174784K), 0.3547080 secs]
...
25003.983: [Full GC 105412K->87523K(205312K), 0.3559230 secs]
25139.998: [Full GC 106102K->80911K(131712K), 0.3432420 secs]
..
47924.811: [Full GC 105487K->80566K(148864K), 0.3392500 secs]
48088.641: [Full GC 98025K->86603K(212736K), 0.3439750 secs]
48338.127: [Full GC 105214K->87088K(159872K), 0.3481490 secs]
..
{code}

Its holding pretty steady.

I also attached memory graph from ganglia over night.  Shows nothing untoward.;;;","15/Oct/08 18:03;stack;Let me move this issue out of 0.18.1.  There is something going on here given its reported by different people but my attempts at replication using simple schema fail.  We need more info.  We can do a 0.18.2 later after we figure whats leaking.;;;","22/Nov/08 05:34;apurtell;This is a recurring issue presently causing pain on current trunk. Seems to be worse now than 0.18.1. Heap gets out of control (> 1GB) for regionservers hosting only ~20 regions or so on. Much of the heap is tied up in byte referenced by HSKs referenced by the WritableComparable[] arrays used by MapFile indexes.

From a jgray server:

class [B 	3525873 	615313626
class org.apache.hadoop.hbase.HStoreKey 	1605046 	51361472
class java.util.TreeMap$Entry 	1178067 	48300747
class [Lorg.apache.hadoop.io.WritableComparable; 	56 	4216992

Approximately 56 mapfile indexes were resident. Approximately 15-20 regions were being hosted at the time of the crash. 

On an apurtell server, >900MB of heap was observed to be consumed by mapfile indexes for 48 store files corresponding to 16 regions.
;;;","22/Nov/08 06:59;stack;One thought: I wonder if fixing the indexing interval so its actually 32 rather than default 128 helped make this issue worse?;;;","22/Nov/08 19:52;apurtell;Last night under Heritrix hbase-writer stress I had a regionserver with 2GB heap go down with an OOME. It was serving 4 regions only. This was with 0.18.1, so the line numbers won't match up with trunk. 

class [B 27343 2008042041
class [C 11714 966164
class org.apache.hadoop.hbase.HStoreKey 9781 312992
class java.util.TreeMap$Entry 7596 311436
class [Lorg.apache.hadoop.io.WritableComparable; 17 139536

Incidentally the RS was also hosting ROOT so the whole cluster went down. I agree with jgray this combined with the ROOT SPOF is deadly.

Stack trace of the OOME:

2008-11-22 09:24:40,950 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting compaction on region content,29308276c599f8a0baca15c224c22ad2,1227337259738
2008-11-22 09:24:56,429 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: Set stop flag in regionserver/0:0:0:0:0:0:0:0:60020.compactor
java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2786)
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:78)
        at org.apache.hadoop.io.compress.CompressorStream.write(CompressorStream.java:71)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.hbase.io.ImmutableBytesWritable.write(ImmutableBytesWritable.java:116)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
        at org.apache.hadoop.io.SequenceFile$RecordCompressWriter.append(SequenceFile.java:1131)
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:980
)
        at org.apache.hadoop.io.MapFile$Writer.append(MapFile.java:198)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Writer.append(HStoreFile.java:846)
        at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:988)
        at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:893)
        at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:902)
        at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:860)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:83);;;","23/Nov/08 20:09;apurtell;Another RS went down this morning. This time somehow I ended up with 1,790,757,470 bytes in 9760 instances of byte[] on the heap. Scrolling through the list of these objects, most are < 256 bytes, some are <= 5K. Only 2727 HSKs. I also see 2426 instances of Hashtable$Entry, only 148 instances of TreeMap$Entry. 12 HStores with 22 HStoreFiles. 

Found a number of 100MB instances of byte[], e.g. referenced from a ByteArrayOutputStream referenced from an o.a.h.i.compress.CompressionOutputStream. Another referenced from both o.a.h.h.io.DataOutputBuffer and o.a.h.h.io.DataInputBuffer referenced from a SequenceFile$Reader. Looks like a Cell has a reference to a copy of this. Found more with local/weak references from Server$Handler. Did a couple of big files (~100MB) and copies thereof take down the RS? 


;;;","23/Nov/08 22:17;stack;You using compression in your instance Andrew?
St.Ack;;;","23/Nov/08 22:51;apurtell;Yes, RECORD compression on 'content' family, which will have up to two cells per row: 'content:raw' will contain the response body written by a custom Heritrix hbase writer, and if the mimetype is text/*, another cell 'content:document' containing a serialized Document object produced by MozillaHtmlParser (http://sourceforge.net/projects/mozillaparser/). Some binary content can be very large, e.g. 100MB zip, tgz, etc. Row index is SHA1 hash of content object. There is also an 'info' family, not compressed, that stores attributes. Finally there is a 'urls' family, not compressed, that will have a cell for each unique URL corresponding to the content object. ;;;","25/Nov/08 00:29;stack;Andrew, what if you disabled compression?  See if you still have issue.  How many heritrix instances?  If one, how many Writers?  5 is default IIRC?  A byte array of 100MB is kinda crazy.  Was there a big page crawled by heritrix?  YOu can check its log.  It outputs sizes.  Maybe you need upper bound on page sizes in heritrix if not there already?;;;","25/Nov/08 00:44;apurtell;The file involved was a 105MB Win32 executable. Using compression compouded the heap charge already taken by several copies from RPC to Cell to ByteArrayOutputStream, etc. I will use a file size limit of 20MB going forward. Also I filed HBASE-1024. ;;;","04/Dec/08 04:57;apurtell;Here is a scenario that guarantees a flurry of regionserver OOMEs on my cluster, which is now running latest trunk on top of Hadoop 0.18.2-dev + Ganglia 3.1 patch:

1) Start up heritrix with hbase-writer. 25 TOEs should do it. Start a long running job.

2) Build up content until there are ~20 regions per regionserver.

3) Run a mapreduce job that walks a metadata column of the content table -- not all columns, not the family storing the content itself, just some small auxiliary metadata.

4) Simultaneously to the scanning read (#3), perform what amounts to a bulk import with 5 concurrent writers. (Typical for my load is 4-8GB in maybe a few 10K updates.) Specifically I am using MozillaHtmlParser to build Document objects from text content and am then storing back serialized representations of those Document objects.

After an invocation of #4, heap usage has balooned across the cluster and it is only a matter of time. Memcache is within limits and for my configuration represents 25% of heap max (I run with 2G heap), so the remaining data is something else. Heap histograms from jhat show a very large number of allocations of [B which can be as much as 1.5GB in total. Soon the regionservers will start to compact or do other heap intensive activities and will fall over. 

A flurry of OOMEs can confuse the master. It will reject region opens thinking they are closing and the regions will remain offline until a manual restart of the cluster. Disable/enable of the table only makes that particular wrinkle worse. 

After restart, invariably a number of regions want to (and do) split. ;;;","04/Dec/08 20:14;stack;Datapoint: tim sell is having similar OOME'ing issues doing an import.  He reports that he disabled blockcaching to no apparent change in behavior.

Trying to run a simplified replica of Andrew's receipe above, the global memflusher -- with new hbase-1027 in place -- got stuck here around an OOME down in DFSClient build up response:

{code}
Exception in thread ""ResponseProcessor for block blk_5165224789834035674_1602"" java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.util.Arrays.copyOf(Unknown Source)
        at java.lang.AbstractStringBuilder.expandCapacity(Unknown Source)
        at java.lang.AbstractStringBuilder.append(Unknown Source)
        at java.lang.StringBuilder.append(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2319)
{code}

{code}
""IPC Server handler 0 on 60020"" daemon prio=10 tid=0x00007f7f501a4400 nid=0x2b31 in Object.wait() [0x0000000042b00000..0x0000000042b01b00]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3026)
        - locked <0x00007f7f8615cd50> (a java.util.LinkedList)
        - locked <0x00007f7f8615c9c0> (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3104)
        - locked <0x00007f7f8615c9c0> (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3053)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:59)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:79)
        at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:959)
        - locked <0x00007f7f8615c858> (a org.apache.hadoop.io.SequenceFile$Writer)
        at org.apache.hadoop.io.MapFile$Writer.close(MapFile.java:183)
        - locked <0x00007f7f86157370> (a org.apache.hadoop.hbase.io.BloomFilterMapFile$Writer)
        at org.apache.hadoop.hbase.io.BloomFilterMapFile$Writer.close(BloomFilterMapFile.java:212)
        - locked <0x00007f7f86157370> (a org.apache.hadoop.hbase.io.BloomFilterMapFile$Writer)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:680)
        - locked <0x00007f7f5de99c88> (a java.lang.Integer)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:627)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:863)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:772) 
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:220)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushSomeRegions(MemcacheFlusher.java:284)
        - locked <0x00007f7f5dc0f828> (a org.apache.hadoop.hbase.regionserver.MemcacheFlusher)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.reclaimMemcacheMemory(MemcacheFlusher.java:254)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdates(HRegionServer.java:1455)
        at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:634)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)
{code};;;","05/Dec/08 04:38;stack;Here is one theory.  Looking at a heap that OOME'd here on test cluster using jprofiler, there were a bunch of instances of SoftValue (30 or 40k).   I was able to sort them by deep size and most encountered held byte arrays of 16k in size.  This would seem to indicate elements of the blockcache.  Odd thing is that you'd think the SoftValues shouldn't be in the heap on OOME; they should have been cleared by the GCor.  Looking, each store file instance has a Map of SoftValues.  They are keyed by position into the file.  The GC does the job of moving the blocks that are to be cleared onto a ReferenceQueue but unless the ReferenceQueue gets processed promptly, we'll hold on to the SoftValue references (JProfiler has a button which says 'clean References' and after selecting this, the SoftValues remained).  The ReferenceQueue gets processed when we add a new block to the cache or if we seek to a new location in a block that we got from the cache (only).  Otherwise, blocks to be removed are not processed.  If random-reading or only looking at certain stores in a regionserver, all other storefiles, unless they are accessed, will continue to hold on to blocks via their uncleared ReferenceQueue.

I tried adding in check of the ReferenceQueue everytime anything was accessed on a file but I still OOME'd using a random read test.

Next thing to try is a single Map that holds all blockcache entries.  Will be lots of contention on this single Map but better than going to disk any day.  All accesses will check the ReferenceQueue.

Only downer is that Tim Sell says his last test was run without blockcache enabled and that it made no difference.  Maybe try it Andrew?

Meantime, I'll try the above suggestion.  Andrew, any chance of a copy of your heap dump?  Tim the same?;;;","06/Dec/08 00:07;stack;Looking more at my local heap, I see ReferenceQueues with links to megabytes of unreleased data.  More evidence that we are not processing ReferenceQueues fast enough.  Need to fix.  Might be able to go with bigger blockcache size if one Map of all blockcaches.

Looking at Andrew Purtell heap dump, it does not have the same character as mine where we are holding on to blockcache items.  His heap has 30 instances of stack-based ByteArrayOutputStreams; together they add up to 690MBs of data.  Trying to figure which BAOS is prob.  Our use in hbase is innocent.  At moment the ipc Server use is suspect.  Digging.;;;","06/Dec/08 01:09;stack;Yeah, I think its the stack based BAOS in Server.  Rather than allocate a new one each time, its reset.   Reset looks like it keeps the old buffer -- it just resets buffer length.  Saves on allocations.  Means that if we ever return a big Cell in a RowResult, then the server buffer stays that big (I see also that hbase.regionserver.handler.count is set to 30 which jibes with the 30 I saw in the heap dump).  Handlers live the life of the application.

Working on a patch for Andrew to try.

;;;","07/Dec/08 09:21;stack;Patch that brings down Server and Client from hadoop ipc.  We now have bulk of hadoop ipc local.  Classes have been renamed to have an HBase prefix to distingush them from their hadoop versions.  Had to bring at least Server local because fix needed meddling in private class (Server.Handler).  Added check on size of stack-based ByteArrayOutputStream size after every use.  It used to always reset.  Now, if BAOS is > initial buffersize, we allocate a new BAOS instance rather than reset.

Verified in testbed it does the right thing.  Unit tests pass.  Tempted to commit but maybe Andrew you can give it a spin first?

Next will work on the blockcache leak.;;;","07/Dec/08 13:56;apurtell;Running with the patch now. There is an improvement. Will have to run for a while to see what the impact on stability is. 

;;;","07/Dec/08 20:29;stack;HADOOP-4797 is intriguing though not our direct problem (I think -- we can pull it in if we think it can help).  I made HADOOP-4802 to fix the apurtell issue up in hadoop.;;;","07/Dec/08 23:50;apurtell;A scenario where I'm sure I would have seen OOMEs succeeds. However another occurrence of HBASE-1046 might have compromised the testing.

Definitely +1 on the patch. Heap use on my regionservers is much better. ;;;","08/Dec/08 01:15;stack;It wasn't an OOME on a regionserver that brought on the hbase-1046?  If you think not, and no OOMEs, great.  I'll commit.;;;","08/Dec/08 01:40;stack;nm.  I'll just go w/ your +1 above.  Will work on the 1046 next.;;;","08/Dec/08 01:45;stack;I committed above patch as part 1 of this issue.  Thanks for testing Andrew.  Part 2 will be fixing blockcache.  There may be a part 3 (Tim Sell just manufactured an hprof for his OOME'ing cluster -- need to figure whats up w/ his failures) and even a part 4 (jgray's failure -- though I think this fixed by hbase-1027).;;;","09/Dec/08 07:47;stack;Have been looking at Tim Sell heaps over last day.   The anomaly is hundreds of thousands of BatchUpdates.  The write rate at OOME is about 15k/20k a second.  Its like we're retaining arrays of BatchUpdates -- something in the rpc invocation code -- but it looks right when I read it.  I'm missing something obvious.  Will keep at it.;;;","10/Dec/08 05:28;stack;Ran a MR job using TableOutputFormat -- batches of BatchUpdate -- and got a heap that looked like Tim Sells with 100k BatchUpdate instances.  Its not obvious to me how we're doing this.  Adding instrumentation to help me narrow in on the issue.  Tim Sell is running a test that avoids TOF.;;;","10/Dec/08 06:13;apurtell;We use TOF also.;;;","10/Dec/08 18:13;stack;Studying my replica of Tim Sell job -- i.e. using TOF and seeing 100k+ BatchUpdates in an array held in the HBaseRPC#Invocation#parameters field -- I now conclude that TOF is operating ""as-advertised"".  Default is that client marshalls 10MB of data. In PE case, this is 12k edits (We measure the BU to be of size 1039 bytes which is probably low-ball looking at BU up in jhat but near-enough).  If server is running 10 handlers, then a common case is 10x10MB of edits just sitting around while the batch of edits are being processed server-side.  We should set the client-side 10MB down to maybe 2MB as default but this is not the root cause of the Tim Sell OOME (Avoiding TOE, he ran longer but still OOME'd).  In his case, the10MB holds even more edits -- 70k for 10MB seems viable after he described his data format -- and that allowing that our accounting of object sizes is coarse, that the 'deep size' reported in the profiler of 318MB is probably about right.

So, TODO, set the client-side batch of edits flush size down from 10MB to 2MB.

Now to look at latest Tim Sell heap dump.;;;","10/Dec/08 21:53;stack;Our calculation of MemCache sizes is way off.  Our math says the aggregate of all Memcaches is 200MB.  In the profiler, the 153 Memcaches present on OOME have 800MBs accumulated.  Working on a better memcache sizer.

Other items, the presence of compressors/decompressors is 'normal'.  A mapfile index is block compressed.  Bad news is that though the index file is closed as soon as possible, allocated buffers for decompressors stick around (MapFile keeps reference to the index SequenceFile so its not GC'd).  OK news is that in scheme of things, accounts for small amount of heap -- about 10MB in tim's case.;;;","11/Dec/08 01:26;stack;First cut at new sizing.  Adds new ByteSize interface that things like HSK, BU, and BO implement making estimates of size that is not just count of payload.  Left the flush on client side at 10MB; number of edits should be a good bit smaller now we do things like count the BU row and size of BU+BO when summing to see if we've hit the flush boundary.  

I checked our estimates against files output to the filesystem and they seem close enough.  Doing same comparing memcache size to that given by profiler is a bit tougher but trying.;;;","12/Dec/08 00:59;tim_s;running test of part2 patch now. I'll post the results in my morning.;;;","12/Dec/08 01:11;stack;v4 of part 2 of this issue.  Want to do a bit more testing before I commit.;;;","12/Dec/08 03:12;apurtell;I'm testing part 2 v4 now also. ;;;","12/Dec/08 08:23;tim_s;Ran with 900 part2. 2 gig heap. using table output format. 
OOME'd. 18 of 88 maps completed.
Running test again with 900 part2 v4.
stack I'll email you a link to the dump / logs.;;;","12/Dec/08 20:18;stack;v5 just adds a new Memcache to the Memcache#main.  I did some more testing and we are coming in close enough on Memcache sizes.  Would like to commit this part2.

Tim's run w/ v4 ran into hdfs issues -- twice.  Didn't OOME.  What about you Andrew?

In a 1G heap I OOME'd and it was not blockcache retention nor Memcache size so at least two other fixes coming on this issue.;;;","12/Dec/08 22:28;stack;v7 adjusts our BatchUpdate sizing (we were a little under).  It also sets down the default for the client write buffer from 10M to 2M.  If 10 handlers, then when it gets to serverside thats 10MB x 10 which is 1/10th of your heap if you are 1G.;;;","13/Dec/08 01:14;stack;Applying v7.     Can improve on it in later patches as get more info. 

+ Downs the client-side batch write heap default from 10MB to 2MB
+ Adds ByteSize interface with a heapSize member.  BatchUpdate, HStoreKey, etc., implement it.
+ The sizes returned out of heapSize favor 64-bit JVMs.  Were obtained from study of heaps made by running HRS and from runs of the new BU and Memcache mains which have little scripts to generate heaps with arrays of BUs and different Memcaches which can then be heap-dumped and studied.;;;","13/Dec/08 05:29;apurtell;+1 No OOME with part 1 and v7 of part 2, even with heavy write load.;;;","13/Dec/08 15:23;tim_s;ditto;;;","13/Dec/08 16:38;stack;Thanks for the +1s.

I can still make it OOME here locally if I use small BatchOperations -- cells of size 10 bytes -- and if I put up lots of clients.  Investigating.  And I still need to fix the OOME that happens randomreading because the blockcache is not getting processed comprehensively.;;;","13/Dec/08 23:04;stack;Around flush we make a snapshot.  As soon as the snapshot is made, we zero the memcache size.  Suspicious.  The snapshot hangs out until flush is completed.  Can take anything from millisecond to ten+ seconds.  Its usually 64MB in size.  In 1G heap fielding a withering upload, could be what throws us over.;;;","16/Dec/08 06:37;stack;part 3 takes size of region memcaches at start of flush and then subtracts the size on flush completion rather than set things to zero as soon as the cache starts up.   Its still not enough for case where cells are 10bytes in size but it goes half-as far again before OOME'ing.  Might be enough for 0.19.0.;;;","16/Dec/08 21:14;stack;Committed part3.  Its an improvement.  Will look at a few more heaps but this might be good enough for 0.19.0 for writing.  Next up, part 4, making sure blockcache gets cleared promplty; i.e. oome when writing and reading at same time.;;;","17/Dec/08 19:37;stack;Looking more at why I can make an OOME writing small cells, I see the MapFile indices starting to come to the fore.  I counted 90MB of indices in a heap of 11 regions and 45 storefiles.  A few were up in the 20+MB range.  Accounting for this size, I'll leave aside for 0.19.0 release (As is, can't get at the index anyways in current MapFile, not unless we brought MapFile local -- lets not do that for 0.19.0 release).;;;","19/Dec/08 02:27;stack;Patch that adds scheduled Excecutor to BlockFSInputStream.  Runs on a period to check for any entries in the Soft Values Reference Queue.  Testing it seems to work.  It has hard-coded values though which is kinda ugly but alternative -- passing in Configuration -- is not viable down here low in io classes.;;;","19/Dec/08 02:32;stack;I'm going to close this issue after p4 goes in.  Enough work has been done on it at least for 0.19.0 time frame even though we will continue to have memory issues until we start counting the size of loaded MapFile indices.  I'll open a new issue to do this for 0.20.0 timeframe.  To fix, will require our bringing down MapFile into hbase or putting in place the new file format.;;;","19/Dec/08 02:37;stack;Added comment to hbase-70.  Its about fixing up our memory management story.;;;","19/Dec/08 19:03;stack;Resolving with commit of part 4.

Ran more tests with small cells.  We run for longer if hbase.regionserver.globalMemcache.upperLimit is set down from the 0.4 default.  Set it down to 0.3 or even 0.25 to make more room for indices (Means we can carry more regions before OOME).;;;","19/Dec/08 19:19;stack;To generate OOMEs, change the PE so that cells are 10 bytes in size instead of the default 1000 bytes in size.;;;","19/Dec/08 22:04;stack;Other thing to do to ameliorate memory usage when small cells is to up the indexing interval from default 32 to 245 or 1024, etc.  Makes a difference for sure, more than changing the upperLimit does.;;;","20/Dec/08 00:49;apurtell;Should the indexing interval be set higher by default? At least until MapFile is brought down or a custom file format replacement is put in?;;;"
Cell iteration is broken,HBASE-892,12404800,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,dogacan,dogacan,dogacan,20/Sep/08 18:20,13/Sep/09 22:26,01/Jul/25 07:49,22/Sep/08 22:01,0.19.0,,,,,0.19.0,,,io,,,,0,Cell implements Iterable<Cell> but its iteration is broken since it will always go one past the edge and throw an ArrayIndexOutOfBoundsException,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/08 18:21;dogacan;cell.patch;https://issues.apache.org/jira/secure/attachment/12390579/cell.patch","22/Sep/08 21:17;dogacan;cell_svn.patch;https://issues.apache.org/jira/secure/attachment/12390682/cell_svn.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25461,,,,,Mon Sep 22 22:01:21 UTC 2008,,,,,,,,,,"0|i0ha3b:",98897,,,,,,,,,,,,,,,,,,,,,"20/Sep/08 18:21;dogacan;Changes CellIterator to properly check for bounds.;;;","22/Sep/08 19:48;jimk;Several comments on this patch.
- patch does not apply.
- both the current code and the patch do essentially the same thing. Both work provide you follow the Iterator idiom.

{code}
Iterator<Cell> it = cell.iterator();
while (it.hasNext()) {
  Cell c = it.next();
  ...
}
{code}
;;;","22/Sep/08 20:53;dogacan;Hi Jim,

Thanks for the comments.

About applying patch: I develop on git, so you have to apply my patches with -p1. I guess I should migrate to svn to make things easier  :)

This code throws on exception without this patch for me:

{code}

    byte[][] vals = Bytes.toByteArrays(new String[] { ""cell"", ""iteration"", ""testing""});
    long now = System.currentTimeMillis();
    long[] ts = {now, now + 10, now + 20};
    Cell cell = new Cell(vals, ts);

    Iterator<Cell> it = cell.iterator();
    while (it.hasNext()) {
      Cell c = it.next();
      System.out.println(Bytes.toString(c.getValue()));
    }

{code}

Assume currentValue == values.length - 1. We first call hasNext and since currentValue is less than values.length it returns true. Now, during next(), we increase currentValue by one thus making it values.length. So attempting to access values[values.length] causes an exception.;;;","22/Sep/08 21:17;dogacan;svn-ified version of the same patch.;;;","22/Sep/08 22:01;jimk;Committed to trunk. Thanks for the patch Doğacan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HRS.validateValuesLength throws IOE, gets caught in the retries",HBASE-891,12404756,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,19/Sep/08 17:14,13/Sep/09 22:26,01/Jul/25 07:49,22/Sep/08 21:48,0.18.0,0.2.1,,,,0.18.1,0.19.0,0.2.2,Client,,,,0,"When HRS.validateValuesLength throws a IOE, it gets caught in the retries because it does not use a DoNotRetryIOException.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/08 17:24;jdcryans;hbase-891.patch;https://issues.apache.org/jira/secure/attachment/12390523/hbase-891.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25460,,,,,Mon Sep 22 21:48:24 UTC 2008,,,,,,,,,,"0|i0ha33:",98896,,,,,,,,,,,,,,,,,,,,,"19/Sep/08 17:24;jdcryans;Adds a new class of Exception. Now TestBatchUpdate takes 50 sec instead of 250. Please review.;;;","22/Sep/08 21:48;stack;Applied to 0.2 and 0.18 branches as well as to trunk.  Thanks for the patch J-D.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The current Thrift API does not allow a new scanner to be created without supplying a column list unlike the other APIs.,HBASE-889,12404680,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tim_s,charliem,charliem,18/Sep/08 20:52,13/Sep/09 22:24,01/Jul/25 07:49,05/May/09 03:58,0.2.0,0.2.1,,,,0.20.0,,,Thrift,,,,0,"The current Thrift API does not allow a new scanner to be created without supplying a column list, unlike the REST api. I posted this on the HBase-Users mailing list. Others concurred that it appears to have been an oversight in the Thrift API. 

Its quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner.",,,,,,,,,,HBASE-1142,,,HBASE-1153,,,,,,,,,,,,,,,,"24/Jan/09 20:40;tim_s;HBASE-889v1.patch;https://issues.apache.org/jira/secure/attachment/12398665/HBASE-889v1.patch","05/May/09 00:43;tim_s;HBASE-889v2.patch;https://issues.apache.org/jira/secure/attachment/12407202/HBASE-889v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25459,,,,,Tue May 05 03:58:51 UTC 2009,,,,,,,,,,"0|i0ha2n:",98894,,,,,,,,,,,,,,,,,,,,,"19/Sep/08 12:56;tim_s;Oops. I'm happy to look at this. I'll post a patch for 0.2.* and trunk in a jiffy.
There's probably at least few more little things like this in the thrift api.;;;","19/Sep/08 14:14;tim_s;actually I was reading the REST code (branch 0.2), opening a scanner, it checks if the columns are specified, and if not 

      // TODO: Need to put into the scanner all of the table's column
      // families.  TODO: Verify this returns all rows.  For now just fail.
      doMethodNotAllowed(response, ""Unspecified columns parameter currently not supported!"");

Are you sure the REST api allows it? Unless I'm looking at the wrong code.

The obvious way to do this is if it's null we get the columns list from the table.;;;","19/Sep/08 14:16;tim_s;A patch will not be up in a jiffy after all. I do this over the weekend maybe.;;;","21/Jan/09 23:35;stack;Fixed by HBASE-1142;;;","24/Jan/09 20:40;tim_s;I was wrong, this wasn't fixed by HBASE-1142 after all. I didn't test it properly. Sorry.
You could pass it an empty list, but the scanner would just select zero columns.

Attached is a patch that addresses this, it includes a getAllColumns function which looks them up from the table descriptor.

I'm not convinced this is how it should be done though, because the shell has similar code. Presumably so does the REST api. Maybe the HTable.getScanner functions should do this themselves if passed null or an empty list for columns?;;;","24/Jan/09 20:49;stack;Agreed Tim.  Null columns list should return all.  Shall we apply your patch meantime and then open new issue to do as you suggest?;;;","24/Jan/09 20:56;tim_s;That sounds good, we should remove the hack-arounds in that issue.;;;","05/May/09 00:42;tim_s;Here's a slightly cleaner version of the patch against the latest trunk.
btw this does not require changes to the generated code at all.;;;","05/May/09 03:58;stack;Committed.  Thanks for the patch Tim (Did you see hbase-1360 and related where upping trunk to use thrift trunk?);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If a server's lease times out or the server dies, All regions will get reassigned even split or offline ones.",HBASE-881,12404104,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jimk,jimk,10/Sep/08 18:47,13/Sep/08 23:22,01/Jul/25 07:49,10/Sep/08 19:25,0.18.0,0.2.0,0.2.1,,,0.18.0,0.2.1,,,,,,0,"If a server's lease times out or a server dies (essentially the same thing), when the master tries to find the regions it was serving, it does not check to see if the region has been offlined or split.

In ProcessServerShutdown.scanMetaRegion, the code:
{code}
        } else {
          // Get region reassigned
          regions.add(info);
        }
{code}

should be:
{code}
        } else {
          if (!info.isOffline() && !info.isSplit()) {
            // Get region reassigned
            regions.add(info);
          }
        }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/08 19:01;jdcryans;hbase-881.patch;https://issues.apache.org/jira/secure/attachment/12389852/hbase-881.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25457,,,,,Wed Sep 10 19:25:18 UTC 2008,,,,,,,,,,"0|i0ha0v:",98886,,,,,,,,,,,,,,,,,,,,,"10/Sep/08 19:01;jdcryans;Fixes what Jim said.;;;","10/Sep/08 19:04;jimk;+1 on patch. Commit it.;;;","10/Sep/08 19:25;jdcryans;Committed to branch and trunk. Thanks Jim for the ""inspiration"".;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCM is unable to find table with multiple regions which contains binary,HBASE-877,12403869,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,08/Sep/08 02:35,13/Sep/08 23:22,01/Jul/25 07:49,10/Sep/08 19:39,0.18.0,0.2.1,,,,0.18.0,0.2.1,,,,,,0,"HCM can not find the table with exception:

org.apache.hadoop.hbase.TableNotFoundException: items
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:508)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:460)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:420)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:130)
        at HBaseRef.<init>(HBaseRef.java:29)
        at Import.<init>(Import.java:20)
        at Import.main(Import.java:26)

I have a fix already for this.  But the problem re-appeared after some time.  I have no recreated it yet, but will post results in the morning.",,,,,,,,,,,,,,,,,,,,HBASE-868,,,,,,,,,"08/Sep/08 02:37;streamy;hbase-877-v1.patch;https://issues.apache.org/jira/secure/attachment/12389649/hbase-877-v1.patch","10/Sep/08 02:30;streamy;hbase-877-v10.patch;https://issues.apache.org/jira/secure/attachment/12389799/hbase-877-v10.patch","08/Sep/08 16:35;streamy;hbase-877-v2.patch;https://issues.apache.org/jira/secure/attachment/12389684/hbase-877-v2.patch","08/Sep/08 17:16;streamy;hbase-877-v3.patch;https://issues.apache.org/jira/secure/attachment/12389691/hbase-877-v3.patch","08/Sep/08 20:22;streamy;hbase-877-v4.patch;https://issues.apache.org/jira/secure/attachment/12389699/hbase-877-v4.patch","08/Sep/08 21:58;streamy;hbase-877-v5.patch;https://issues.apache.org/jira/secure/attachment/12389702/hbase-877-v5.patch","08/Sep/08 21:59;streamy;hbase-877-v6.patch;https://issues.apache.org/jira/secure/attachment/12389704/hbase-877-v6.patch","09/Sep/08 00:24;streamy;hbase-877-v8.patch;https://issues.apache.org/jira/secure/attachment/12389710/hbase-877-v8.patch","09/Sep/08 01:46;streamy;hbase-877-v9.patch;https://issues.apache.org/jira/secure/attachment/12389717/hbase-877-v9.patch",,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25455,,,,,Wed Sep 10 17:47:35 UTC 2008,,,,,,,,,,"0|i0h9zz:",98882,,,,,,,,,,,,,,,,,,,,,"08/Sep/08 02:37;streamy;Adds new HStoreKey.HStoreKeyWritableComparator(info) when initializing candidateKeys sorted map in HStore.

I believe that this needs to be added other places as well in addition to this and where we added it in HBASE-868;;;","08/Sep/08 02:42;streamy;Just confirmed the reappearance.

Following a memcache flush of META, the identical exception is thrown:

org.apache.hadoop.hbase.TableNotFoundException: items
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:508)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:460)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:420)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:130)
        at HBaseRef.<init>(HBaseRef.java:29)
        at HBaseRef.main(HBaseRef.java:15)

Either something is wrong with the flush itself, or someting is wrong when reading from the flushed store.  I imagine this will be another one or two line fix adding the comparator to some SortedMaps.;;;","08/Sep/08 17:00;stack;Patch needs to fix test failed compile.

Trying to replicate, ran doctered PerformanceEvalution that makes binary keys.... Then shutdown cluster and brought it back up.    Trying to scan table, it just looped and looped over whole table.  Applied patch after hacking in fix for tests and looping stopped.  Retrying.;;;","08/Sep/08 17:16;streamy;Includes TestHSF fixes.

Have run test including META memcache flush, this has resolved the issue.;;;","08/Sep/08 20:22;streamy;Replaces new HStoreKey() empty constructors with HStoreKey(HConstants.EMPTY_BYTE_ARRAY, HRegionInfo) when available;;;","08/Sep/08 21:58;streamy;Removes some of the new HStoreKey() changes made in v4.

As of now I have no errors with this patch.  Have seen a memcache flush and can still fully scan root, meta, and binary split table with about 8 regions.;;;","08/Sep/08 21:59;streamy;Forgot to clean up conf changes from the last patch.;;;","08/Sep/08 22:22;stack;I'm seeing that if I run with the patch, soon as an optional .META. flushes, then I can't get regions, can't scan -ROOT-, nor .META.

Looking.;;;","09/Sep/08 00:24;streamy;Seems that the issue is in an infinite loop in:  rowAtOrBeforeCandidate(map,...

This adds two more HSK initializer changes to pass HRI as is done elsewhere in this patch;;;","09/Sep/08 01:46;streamy;Adds additional new HSK changes.

v8 patch is confirmed working on my cluster.  Will run additional testing on v9 patch tonight.;;;","09/Sep/08 06:42;stack;v9 worked for me where v8 was obviously stuck;;;","09/Sep/08 12:31;streamy;I've got a table with 24 regions now.

With v9 I'm stuck on a full table scan about 6 regions in.  Looks just like the freezing we were seeing before.;;;","09/Sep/08 13:09;streamy;Definitely seeing same issue as I did with previous versions.  I can still scan META and ROOT, but after my scan of 'items' broke down I'm not unable to scan any of my other tables from the shell.

Getting an HTable reference to _any_ table does not work from my java client code.

As before, restarting HBase does fix things.;;;","10/Sep/08 02:30;streamy;So far so good.  Running large scale test overnight, report back in the morning.;;;","10/Sep/08 15:04;stack;The change passing a comparator to Mapfile broke the BeforeThisStoreKey trick.  BeforeThisStoreKey is an HStoreKey instance that has no equal; it returns -1 when compared to an ""equal"" instance.  Its passed to Mapfile.next when we want to find the closest before.  Without this trick, the call to get closest before always returned same result -- the infinite looping that we were seeing.

My overnight tests all completed (The amended PE to make binary keys, an optional flush set to ten minutes, doing sequential write followed by random read suite; previous we'd fail after second optional flush about 20 minutes in).;;;","10/Sep/08 16:55;streamy;+1 on v10;;;","10/Sep/08 17:47;stack;Applied branch and trunk.  Thanks for the patch Jon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There are a large number of Java warnings in HBase,HBASE-876,12403844,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,apparition,jimk,jimk,07/Sep/08 08:09,13/Sep/09 22:24,01/Jul/25 07:49,22/Jan/09 05:44,0.18.0,,,,,0.20.0,,,,,,,0,"There are a large number of Java warnings in the current HBase code base including:
- exceptions that do not define serialVersionUID
- classes that use the raw type WritableComparable instead of WritableComparable<T>
- classes or interfaces that declare public members that are not a part of the public API. In this case they should be moved to a place where their visibility needs not be public. Additionally, there are a number of classes that declare public members that need not be. Make them protected or private or default as needed
- methods that have unnecessary else clauses
- potential null pointer access
- inner classes that are public that should be default or protected (e.g. RegionHistoryInformation)
- assignment to an input parameter

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/09 02:57;apparition;876_1.patch;https://issues.apache.org/jira/secure/attachment/12398279/876_1.patch","20/Jan/09 02:57;apparition;876_2.patch;https://issues.apache.org/jira/secure/attachment/12398280/876_2.patch","20/Jan/09 02:58;apparition;876_3.patch;https://issues.apache.org/jira/secure/attachment/12398281/876_3.patch","20/Jan/09 20:53;apparition;876_4.patch;https://issues.apache.org/jira/secure/attachment/12398331/876_4.patch","20/Jan/09 20:54;apparition;876_5.patch;https://issues.apache.org/jira/secure/attachment/12398333/876_5.patch","20/Jan/09 20:55;apparition;876_6.patch;https://issues.apache.org/jira/secure/attachment/12398334/876_6.patch","20/Jan/09 21:22;apparition;876_7.patch;https://issues.apache.org/jira/secure/attachment/12398336/876_7.patch","20/Jan/09 21:52;apparition;876_8.patch;https://issues.apache.org/jira/secure/attachment/12398338/876_8.patch",,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25454,,,,,Thu Jan 22 05:44:39 UTC 2009,,,,,,,,,,"0|i0h9zr:",98881,,,,,,,,,,,,,,,,,,,,,"07/Sep/08 22:21;stack;Are you talking about eclipse warnings?  You think we should run findbugs or some of the other tools against code base?  Might find more substantial issues?;;;","08/Sep/08 01:37;jimk;Yes, I am talking about eclipse warnings.

I was hoping that eventually we could start running findbugs on Hudson as Hadoop does. But running it locally, and fixing those along with what eclipse complains about would certainly be a good thing.;;;","29/Oct/08 18:30;stack;Moving out of 0.19.0.;;;","10/Jan/09 01:13;apparition;So now I am fighting with warnings.  There already more than 100 left. And my patch will affect on almost every source java file :)

I am also fighting with warnings for Annotations like this: 
 - Unnecessary @SuppressWarnings(""unused"")

There still warnings like this:
- Access to enclosing method ""myMethod"" from the type ""myClass"" is emulated by a synthetic accessor method. Increasing its visibility will improve your performance;;;","10/Jan/09 01:24;stack;Evgeny:

I noticed that when I went from 3.3 eclipse to 3.4 (ganymede), it started saying 'Unnecessary @SuppressWarnings(""unused"")' are unnecessary.  You are using ganymede eclipse?  I'd say remove them.

On second warning type, I suppose we should do as suggested.;;;","10/Jan/09 14:08;apparition;876_1.patch fixes all warnings in package:
org.apache.hadoop.hbase;;;","10/Jan/09 14:18;apparition;I decide to fixing warnings by package, so there will be one patch per each package. I think it's a best way to make so much changes.;;;","10/Jan/09 17:48;stack;Agree that per package is way to go.  I won't apply till we branch 0.19... shouldn't be long;;;","10/Jan/09 22:12;apparition;876_2.patch is for packages:
org.apache.hadoop.hbase.client
org.apache.hadoop.hbase.client.tableindexed
org.apache.hadoop.hbase.client.transactional
org.apache.hadoop.hbase.filter;;;","20/Jan/09 02:58;apparition;876_3.patch is for package:
org.apache.hadoop.hbase.io
;;;","20/Jan/09 06:34;stack;I applied parts 1, 2, and 3.  I left out changes to SequenceFile and MapFile from part 3 since they are from hadoop itself and we don't want to change these since it'll make it harder going forward ensuring we pick up changes that have happened in parent versions (these should be going away soon in next version or so of hbase anyways).

Thanks Evgeny for the great cleanup.  Do you have more?  If not, I'll close the issue.;;;","20/Jan/09 20:55;apparition;876_4.patch is for:
org.apache.hadoop.hbase.ipc

876_5.patch is for:
org.apache.hadoop.hbase.mapred

876_6.patch is for:
org.apache.hadoop.hbase.master;;;","20/Jan/09 21:22;apparition;876_7.patch is for:

org.apache.hadoop.hbase.master.metrics
org.apache.hadoop.hbase.metrics.file
org.apache.hadoop.hbase.regionserver
org.apache.hadoop.hbase.regionserver.metrics
org.apache.hadoop.hbase.regionserver.tableindexed
org.apache.hadoop.hbase.regionserver.transactional
org.apache.hadoop.hbase.rest;;;","20/Jan/09 21:52;apparition;876_8.patch is for:
org.apache.hadoop.hbase.thrift
org.apache.hadoop.hbase.util
org.onelab.filter

The last one.
;;;","22/Jan/09 05:07;stack;I didn't apply fixup for ipc package Evgeny.  These too are copied down from hadoop and the less differences the better.  Otherwise all others look good.  Testing now.;;;","22/Jan/09 05:44;stack;Committed. Thank you for the patch Evgeny.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Getting exceptions in shell when creating/disabling tables,HBASE-872,12403763,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,05/Sep/08 15:50,13/Sep/08 23:22,01/Jul/25 07:49,05/Sep/08 23:57,0.18.0,0.2.1,,,,0.18.0,0.2.1,,,,,,0,"On the list from Dru Jensen:

I am testing the release candidate with hadoop 0.17.2.1 release.  I am curious if others are seeing this or if I have something mis-configured.

I reformatted the dfs and recreated everything from scratch.

hbase(main):009:0> version
Version: 0.2.1, r691710, Wed Sep  3 11:50:24 PDT 2008

I occasionally get the following error performing a create table.  However when I do a list, the table was successfully created.

hbase(main):007:0> create 'test', 'avg', 'std', 'max'
NativeException: org.apache.hadoop.hbase.client.NoServerForRegionException: No server address listed in .META. for region test,,1220628716239
       from org/apache/hadoop/hbase/client/HConnectionManager.java:536:in `locateRegionInMeta'
       from org/apache/hadoop/hbase/client/HConnectionManager.java:459:in `locateRegion'
       from org/apache/hadoop/hbase/client/HConnectionManager.java:419:in `locateRegion'
       from org/apache/hadoop/hbase/client/HBaseAdmin.java:157:in `createTable'
       from sun/reflect/NativeMethodAccessorImpl.java:-2:in `invoke0'
       from sun/reflect/NativeMethodAccessorImpl.java:39:in `invoke'
       from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
       from java/lang/reflect/Method.java:585:in `invoke'
       from org/jruby/javasupport/JavaMethod.java:250:in `invokeWithExceptionHandling'
       from org/jruby/javasupport/JavaMethod.java:219:in `invoke'
       from org/jruby/javasupport/JavaClass.java:416:in `execute'
       from org/jruby/internal/runtime/methods/SimpleCallbackMethod.java:67:in `call'
       from org/jruby/internal/runtime/methods/DynamicMethod.java:78:in `call'
       from org/jruby/runtime/CallSite.java:329:in `call'
       from org/jruby/evaluator/ASTInterpreter.java:649:in `callNode'
       from org/jruby/evaluator/ASTInterpreter.java:324:in `evalInternal'
... 121 levels...
       from ruby.opt.hbase_minus_0_dot_2_dot_1.bin.hirbInvokermethod__23$RUBY$startOpt:-1:in `call'
       from org/jruby/internal/runtime/methods/DynamicMethod.java:74:in `call'
       from org/jruby/internal/runtime/methods/CompiledMethod.java:48:in `call'
       from org/jruby/runtime/CallSite.java:123:in `cacheAndCall'
       from org/jruby/runtime/CallSite.java:298:in `call'
       from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:351:in `__file__'
       from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `__file__'
       from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `load'
       from org/jruby/Ruby.java:512:in `runScript'
       from org/jruby/Ruby.java:432:in `runNormally'
       from org/jruby/Ruby.java:312:in `runFromMain'
       from org/jruby/Main.java:144:in `run'
       from org/jruby/Main.java:89:in `run'
       from org/jruby/Main.java:80:in `main'
       from /opt/hbase/bin/../bin/hirb.rb:228:in `create'
       from (hbase):8:in `binding'hbase(main):008:0>

And occasionally, I get this exception when trying to disable a table. However it was successfully disabled.

hbase(main):002:0> disable 'test'
NativeException: java.io.IOException: unable to disable table test
       from org/apache/hadoop/hbase/client/HBaseAdmin.java:418:in `disableTable'
       from org/apache/hadoop/hbase/client/HBaseAdmin.java:379:in `disableTable'
       from sun/reflect/NativeMethodAccessorImpl.java:-2:in `invoke0'
       from sun/reflect/NativeMethodAccessorImpl.java:39:in `invoke'
       from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
       from java/lang/reflect/Method.java:585:in `invoke'
       from org/jruby/javasupport/JavaMethod.java:250:in `invokeWithExceptionHandling'
       from org/jruby/javasupport/JavaMethod.java:219:in `invoke'
       from org/jruby/javasupport/JavaClass.java:416:in `execute'
       from org/jruby/internal/runtime/methods/SimpleCallbackMethod.java:67:in `call'
       from org/jruby/internal/runtime/methods/DynamicMethod.java:78:in `call'
       from org/jruby/runtime/CallSite.java:155:in `cacheAndCall'
       from org/jruby/runtime/CallSite.java:332:in `call'
       from org/jruby/evaluator/ASTInterpreter.java:649:in `callNode'
       from org/jruby/evaluator/ASTInterpreter.java:324:in `evalInternal'
       from org/jruby/evaluator/ASTInterpreter.java:620:in `blockNode'
... 121 levels...
       from ruby.opt.hbase_minus_0_dot_2_dot_1.bin.hirbInvokermethod__23$RUBY$startOpt:-1:in `call'
       from org/jruby/internal/runtime/methods/DynamicMethod.java:74:in `call'
       from org/jruby/internal/runtime/methods/CompiledMethod.java:48:in `call'
       from org/jruby/runtime/CallSite.java:123:in `cacheAndCall'
       from org/jruby/runtime/CallSite.java:298:in `call'
       from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:351:in `__file__'
       from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `__file__'
       from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `load'
       from org/jruby/Ruby.java:512:in `runScript'
       from org/jruby/Ruby.java:432:in `runNormally'
       from org/jruby/Ruby.java:312:in `runFromMain'
       from org/jruby/Main.java:144:in `run'
       from org/jruby/Main.java:89:in `run'
       from org/jruby/Main.java:80:in `main'
       from /opt/hbase/bin/../bin/hirb.rb:254:in `disable'

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/08 22:34;jdcryans;hbase-872-v1-branch-0.2.patch;https://issues.apache.org/jira/secure/attachment/12389596/hbase-872-v1-branch-0.2.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25452,,,,,Fri Sep 05 23:57:53 UTC 2008,,,,,,,,,,"0|i0h9yv:",98877,,,,,,,,,,,,,,,,,,,,,"05/Sep/08 15:51;jdcryans;I saw that too, pretty easy to recreate. Maybe we should up the waiting time a bit or add some retry logic?;;;","05/Sep/08 21:17;apurtell;The client pause period was lowered from 5 seconds to 2 seconds as part of HBASE-830. Enable and disable table in HBaseAdmin already has wait and retry logic. Not sure what the shell does exactly. ;;;","05/Sep/08 21:38;stack;Making a blocker on 0.2.1

Only pity is that I can't reproduce on machines here -- but Daniel Leffel reported this yesterday up on his ec2 cluster.;;;","05/Sep/08 21:48;stack;Dru, since you can reproduce easily, would you mind changing retries to see if that fixes the issue for you?

In bin/hirb.rb, look for:

{code}
@configuration.setInt(""hbase.client.retries.number"", 3)
@configuration.setInt(""ipc.client.connect.max.retries"", 3)
{code}

Try commenting the two lines above out by prefixing the lines with a '#' character.  See if that fixes it.;;;","05/Sep/08 22:34;jdcryans;Fixes both problems. First, patch handles more exceptions in createTable by introducing a new parent class for regions handling related exceptions. Second problem isn't really one, the table always gets disabled (after some time) but we exhausted the retries we were given so I just changed to message.;;;","05/Sep/08 23:57;stack;Committed branch and trunk.  Thanks for the patch j-d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Major compaction periodicity should be specifyable at the column family level, not cluster wide",HBASE-871,12403717,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,stack,stack,04/Sep/08 21:40,13/Sep/08 23:22,01/Jul/25 07:49,07/Sep/08 22:17,0.18.0,0.2.1,,,,0.18.0,0.2.1,,,,,,0,jon gray has a table of ten rows and a couple of columns that is constantly being updated.  Has max versions of 2.  This table is growing fast because all versions written are kept until a major compaction.  The way this table is being used is different than that of others.  Would be good if he could have major compactions run more often than the default once a day.,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-800,,,,"05/Sep/08 18:19;streamy;hbase-871-v1.patch;https://issues.apache.org/jira/secure/attachment/12389575/hbase-871-v1.patch","05/Sep/08 18:45;streamy;hbase-871-v2.patch;https://issues.apache.org/jira/secure/attachment/12389578/hbase-871-v2.patch","05/Sep/08 20:41;streamy;hbase-871-v3.patch;https://issues.apache.org/jira/secure/attachment/12389586/hbase-871-v3.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25451,,,,,Sun Sep 07 22:17:18 UTC 2008,,,,,,,,,,"0|i0h9yn:",98876,,,,,,,,,,,,,,,,,,,,,"04/Sep/08 22:27;stack;A patch should do like the following. 

In the HStore constructor, it does this to get the major compaction time:

{code}
    this.majorCompactionTime = conf.getLong(""hbase.hregion.majorcompaction"", 86400000);
{code}

Just below it, it does this to get the config. for compression from HCD:

{code}
family.getCompression()
{code}

I'd think you'd do something like:

{code}
family.getValue(""hbase.hregion.majorcompaction"")
{code}

... if it came back null, you'd use the default.  Otherwise, use its value.

Add a define to HConstants for ""hbase.hregion.majorcompaction"" to make your life easier.

That takes care of the reading.

For setting it, the addtion to HConstants will help.  See the head of bin/HBase.rb.  See how we define constants there, constants that can then be used specifying stuff in the shell..;;;","05/Sep/08 18:19;streamy;Per stacks recommendations, this patch just adds a check in HStore to see if a MAJOR_COMPACTION_PERIOD / hbase.hregion.majorcompaction was set for the family, and sets it if so.  Otherwise it remains at the default.

Also added MAJOR_COMPACTION_PERIOD to HConstants => ""hbase.hregion.majorcompaction"";;;","05/Sep/08 18:21;streamy;Confirmed that patch applies to 0.2 branch and trunk;;;","05/Sep/08 18:45;streamy;Previous patch did not compile.

HCD.getValue/setValue works only with Strings/byte[] so we must store period as String representation of long and do conversion when we fetch.

Could also add getLong/setLong to HCD but this will suffice for me.;;;","05/Sep/08 19:58;stack;Suggest that this piece of your patch...

{code}
    if (family.getValue(MAJOR_COMPACTION_PERIOD) != null) {
      String strCompactionTime = family.getValue(HConstants.MAJOR_COMPACTION_PERIOD);
      this.majorCompactionTime = (new Long(strCompactionTime)).longValue();
    }
{code}

... go in under this line of code:

{code}
    this.majorCompactionTime = conf.getLong(""hbase.hregion.majorcompaction"", 86400000);
{code}

Also, use the new define in the line above now you've added it to HConstants.

We need to change the shell so you can pass arbitrary key/values to HCD and HTD but let that be another issue, HBASE-873;;;","05/Sep/08 20:41;streamy;Fixes described by stack;;;","07/Sep/08 22:17;stack;Applied trunk and branch.  Thanks for patch Jon;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incrementing binary rows cause strange behavior once table splits,HBASE-868,12403692,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,04/Sep/08 17:08,13/Sep/08 23:22,01/Jul/25 07:49,08/Sep/08 17:17,0.18.0,0.2.1,,,,0.18.0,0.2.1,,,,,,0,"We're now using incrementing binary row keys and to this point had only been doing small tests with them, never having actually had a table split.

I'm still working through the logs but it seems that there's a problem somewhere with startKey and endKeys.

Binary in general is not well supported (inconsistent in display in the logs, very odd rendering in the web ui, hard to interpret in the shell, etc..)  Once we figure out these serious bugs we will spend some time trying to clean that up.  But right now this makes things even harder to debug.

The actual symptoms are that my import eventually started to throw (in the client and on the region server):

org.apache.hadoop.hbase.regionserver.WrongRegionException: org.apache.hadoop.hbase.regionserver.WrongRegionException: Requested row out of range for HRegion sources,,1220546297947, startKey='', getEndKey()='
ï¿½', row='cï¿½'
        at org.apache.hadoop.hbase.regionserver.HRegion.checkRow(HRegion.java:1775)
        at org.apache.hadoop.hbase.regionserver.HRegion.obtainRowLock(HRegion.java:1831)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchUpdate(HRegion.java:1387)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1145)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)


There are 3 regionservers, but this error only happens on one of them (while the other two always continue normally, allowing updates to this same table).

The regionserver that this happens on is special for two reasons, one it is hosting the META table.  And secondly it also hosts the first region in the table, startkey = ''.  I'm unsure which is the cause, I have a clue leading to both.

After about 15 minutes, the regionserver sees:

2008-09-04 09:52:57,948 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region .META.,,1. Current region memcache size 24.5k
2008-09-04 09:52:58,003 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/.META./1028785192/historian/mapfiles/8699673838203663799 with 106 entries, sequence id 25341510, data size 8.9k, file size 10.6k
2008-09-04 09:52:58,050 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/.META./1028785192/info/mapfiles/1791564557665476834 with 96 entries, sequence id 25341510, data size 14.2k, file size 15.8k
2008-09-04 09:52:58,050 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region .META.,,1 in 102ms, sequence id=25341510, compaction requested=true
2008-09-04 09:52:58,050 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region: .META.,,1
2008-09-04 09:52:58,051 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting compaction on region .META.,,1
2008-09-04 09:52:58,055 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 1028785192/historian: 41.9k; Skipped 1 files , size: 21957
2008-09-04 09:52:58,088 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/.META./compaction.dir/1028785192/historian/mapfiles/6948796056606699674
2008-09-04 09:52:58,128 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/.META./compaction.dir/1028785192/historian/mapfiles/6948796056606699674 to /hbase/.META./1028785192/historian/mapfiles/75733875840914142
2008-09-04 09:52:58,175 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 1028785192/historian store size is 41.1k; time since last major compaction: 5426 seconds
2008-09-04 09:52:58,179 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 1028785192/info: 61.9k; Skipped 0 files , size: 0
2008-09-04 09:52:58,192 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 3 files into /hbase/.META./compaction.dir/1028785192/info/mapfiles/7781013568996125923
2008-09-04 09:52:58,260 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/.META./compaction.dir/1028785192/info/mapfiles/7781013568996125923 to /hbase/.META./1028785192/info/mapfiles/2187291308709057119
2008-09-04 09:52:58,290 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 1028785192/info store size is 61.0k; time since last major compaction: 32534 seconds
2008-09-04 09:52:58,296 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region .META.,,1 in 0sec
2008-09-04 09:53:09,620 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Scanner -2085474968086468199 lease expired
2008-09-04 09:54:35,449 INFO org.apache.hadoop.hbase.regionserver.LogRoller: Rolling hlog. Number of entries: 30009


Following this, insertion continues normally.  This leads me to believe there's an issue with the META table memcache, but oddly the other regions of this table on other regionservers continue on fine.

As for the hosting the first region of the table on this region server, it seems to be consistent that I get the row out of range errors when looking for a region with startkey = '', although there are 5 other regions on this RS.

Will attach full logs from master and three RS.  Also a couple screenshots showing weird behavior in listing the regions in the table.","4 nodes: 1 master/namenode, 3 regionserver/datanodes",,,,,,,,,,,,,,,,,HBASE-877,,,,,,,,,,,"06/Sep/08 03:22;streamy;hbase-868-0.2-v2.patch;https://issues.apache.org/jira/secure/attachment/12389609/hbase-868-0.2-v2.patch","06/Sep/08 20:32;streamy;hbase-868-0.2-v3.patch;https://issues.apache.org/jira/secure/attachment/12389621/hbase-868-0.2-v3.patch","07/Sep/08 01:24;streamy;hbase-868-0.2-v4.patch;https://issues.apache.org/jira/secure/attachment/12389625/hbase-868-0.2-v4.patch","08/Sep/08 01:29;streamy;hbase-868-0.2-v5-fix.patch;https://issues.apache.org/jira/secure/attachment/12389647/hbase-868-0.2-v5-fix.patch","07/Sep/08 22:48;streamy;hbase-868-0.2-v5.patch;https://issues.apache.org/jira/secure/attachment/12389645/hbase-868-0.2-v5.patch","04/Sep/08 17:55;jdcryans;hbase-868-branch-0.2.patch;https://issues.apache.org/jira/secure/attachment/12389524/hbase-868-branch-0.2.patch","04/Sep/08 17:48;streamy;hbase-hbase-regionserver-web3.streamy.com.log;https://issues.apache.org/jira/secure/attachment/12389523/hbase-hbase-regionserver-web3.streamy.com.log","04/Sep/08 18:06;streamy;master_regionsinsources.png;https://issues.apache.org/jira/secure/attachment/12389526/master_regionsinsources.png","04/Sep/08 18:06;streamy;regionserver_regionlist.png;https://issues.apache.org/jira/secure/attachment/12389527/regionserver_regionlist.png",,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25448,,,,,Mon Sep 08 17:17:22 UTC 2008,,,,,,,,,,"0|i0h9xz:",98873,,,,,,,,,,,,,,,,,,,,,"04/Sep/08 17:48;streamy;This is the region hosting META, the first region in the sources table, and the one that throws the exceptions row not in region.;;;","04/Sep/08 17:55;jdcryans;I left those three Bytes.compareTo in Memcache thinking it was not going to cause problems. Well maybe it is. Let's see with Jonathan.;;;","04/Sep/08 18:06;streamy;Shows odd behavior in web ui;;;","05/Sep/08 04:00;streamy;One note, I accidentally brought up a clean hdfs on hbase 0.2.1 rc1 and including all the patches I made with JD today.  First insert to a table immediately caused:

Exception in thread ""main"" java.io.IOException: java.io.IOException: java.io.IOException: java.lang.                                                                                               NegativeArraySizeException
        at org.apache.hadoop.hbase.HStoreKey.stripStartKeyMeta(HStoreKey.java:532)
        at org.apache.hadoop.hbase.HStoreKey.compareTwoRowKeys(HStoreKey.java:494)


This does not happen on 0.2.1 rc1 without our patches.  I'm to do plenty of inserting of whatever until I reach a split.  Once it finishes splitting, the next request immediately starts receiving:

 org.apache.hadoop.hbase.regionserver.WrongRegionException: Requested row out of range;;;","06/Sep/08 03:22;streamy;Using this patch, I don't see any of the added debug output anywhere.

Here is where it breaks.  During the first memcache flush of META, Flusher says Replay of hlog required and then DroppedSnapshot/Key out of order exceptions:

2008-09-05 19:09:12,202 INFO org.apache.hadoop.hbase.regionserver.LogRoller: Rolling hlog. Number of entries: 30001
2008-09-05 19:09:12,215 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Closing current log writer /hbase/log_72.34.249.110_1220665441919_60020/hlog.dat.1220666931124
2008-09-05 19:09:12,218 INFO org.apache.hadoop.hbase.regionserver.HLog: New log writer created at /hbase/log_72.34.249.110_1220665441919_60020/hlog.dat.1220666952215
2008-09-05 19:14:02,219 INFO org.apache.hadoop.hbase.regionserver.LogRoller: Rolling hlog. Number of entries: 14234
2008-09-05 19:14:02,230 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Closing current log writer /hbase/log_72.34.249.110_1220665441919_60020/hlog.dat.1220666941412
2008-09-05 19:14:02,233 INFO org.apache.hadoop.hbase.regionserver.HLog: New log writer created at /hbase/log_72.34.249.110_1220665441919_60020/hlog.dat.1220667242230
2008-09-05 19:14:22,342 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region .META.,,1. Current region memcache size 23.6k
2008-09-05 19:14:22,362 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library
2008-09-05 19:14:22,363 INFO org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2008-09-05 19:14:22,364 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor
2008-09-05 19:14:22,392 FATAL org.apache.hadoop.hbase.regionserver.Flusher: Replay of hlog required. Forcing server restart
org.apache.hadoop.hbase.DroppedSnapshotException: region: .META.,,1
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1081)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:979)
        at org.apache.hadoop.hbase.regionserver.Flusher.flushRegion(Flusher.java:174)
        at org.apache.hadoop.hbase.regionserver.Flusher.run(Flusher.java:91)
Caused by: java.io.IOException: key out of order: items,1220665885921/historian:assignment/1220665888554 after items,,1220665885921/historian:split/1220665886459
        at org.apache.hadoop.io.MapFile$Writer.checkKey(MapFile.java:205)
        at org.apache.hadoop.io.MapFile$Writer.append(MapFile.java:191)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Writer.append(HStoreFile.java:846)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:625)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:578)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1068)
        ... 3 more
2008-09-05 19:14:22,393 INFO org.apache.hadoop.hbase.regionserver.Flusher: regionserver/0:0:0:0:0:0:0:0:60020.cacheFlusher exiting
2008-09-05 19:14:24,583 DEBUG org.apache.hadoop.hbase.RegionHistorian: Offlined
2008-09-05 19:14:24,583 INFO org.apache.hadoop.ipc.Server: Stopping server on 60020
2008-09-05 19:14:24,583 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 60020: exiting
2008-09-05 19:14:24,584 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 60020: exiting
2008-09-05 19:14:24,584 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 60020
2008-09-05 19:14:24,584 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Stopping infoServer
2008-09-05 19:14:24,584 INFO org.mortbay.util.ThreadedServer: Stopping Acceptor ServerSocket[addr=0.0.0.0/0.0.0.0,port=0,localport=60030]
2008-09-05 19:14:24,584 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 60020: exiting
2008-09-05 19:14:24,584 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 60020: exiting
2008-09-05 19:14:24,584 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 60020: exiting
2008-09-05 19:14:24,585 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 60020: exiting
2008-09-05 19:14:24,585 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 60020: exiting
2008-09-05 19:14:24,585 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 60020: exiting
2008-09-05 19:14:24,585 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 60020: exiting
2008-09-05 19:14:24,585 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2008-09-05 19:14:24,587 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 60020: exiting
2008-09-05 19:14:24,587 INFO org.mortbay.http.SocketListener: Stopped SocketListener on 0.0.0.0:60030
2008-09-05 19:14:25,254 INFO org.mortbay.util.Container: Stopped HttpContext[/static,/static]
2008-09-05 19:14:25,762 INFO org.mortbay.util.Container: Stopped HttpContext[/logs,/logs]
2008-09-05 19:14:25,763 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.servlet.WebApplicationHandler@6b754699
2008-09-05 19:14:26,418 INFO org.mortbay.util.Container: Stopped WebApplicationContext[/,/]
2008-09-05 19:14:26,418 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.Server@177ba38f
2008-09-05 19:14:26,419 DEBUG org.apache.hadoop.hbase.regionserver.HLog: closing log writer in hdfs://web1:9000/hbase/log_72.34.249.110_1220665441919_60020
2008-09-05 19:14:26,419 INFO org.apache.hadoop.hbase.regionserver.LogRoller: LogRoller exiting.
2008-09-05 19:14:26,419 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: regionserver/0:0:0:0:0:0:0:0:60020.compactor exiting
2008-09-05 19:14:26,432 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: On abort, closed hlog
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: closing region .META.,,1
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Compactions and cache flushes disabled for region .META.,,1
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Scanners disabled for region .META.,,1
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: No more active scanners for region .META.,,1
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region .META.,,1
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: No more row locks outstanding on region .META.,,1
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HStore: closed 1028785192/historian
2008-09-05 19:14:26,433 DEBUG org.apache.hadoop.hbase.regionserver.HStore: closed 1028785192/info
2008-09-05 19:14:26,433 INFO org.apache.hadoop.hbase.regionserver.HRegion: closed .META.,,1
...
2008-09-05 19:14:26,435 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: aborting server at: 72.34.249.110:60020
2008-09-05 19:14:30,719 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: worker thread exiting
2008-09-05 19:14:30,719 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: regionserver/0:0:0:0:0:0:0:0:60020 exiting
2008-09-05 19:14:32,341 INFO org.apache.hadoop.hbase.Leases: regionserver/0:0:0:0:0:0:0:0:60020.leaseChecker closing leases
2008-09-05 19:14:32,341 INFO org.apache.hadoop.hbase.Leases: regionserver/0:0:0:0:0:0:0:0:60020.leaseChecker closed leases
2008-09-05 19:14:32,342 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Starting shutdown thread.
2008-09-05 19:14:32,342 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Shutdown thread complete

;;;","06/Sep/08 20:32;streamy;Comparator is called now, thanks JD.

Current issue with this patch is that following first split, META table eventually loses all references to the new split regions.;;;","07/Sep/08 01:24;streamy;This patch contains a ton more switches of Bytes.compare and Bytes.equals to HStoreKey.compareTwoRowKeys and HStoreKey.equalsTwoRowKeys

That fixes a great deal of the issues...

Right now, we can successfully get past the first split.  After the second split, the first region gets lost from the META table.  Often the historian family will exist but the info is gone completely.

There is also a lot of debug output in this spread across Memcache + HScanner, SFS, and a few other places.  This is attempting to figure out when we do our scans if we are missing something.

JD and I originally thought that the row for the first region was still in META but for some reason scanner was missing it.  Looking at it now with the debug output, it seems that it sees old META rows with the splitA splitB info in them, and then a start row META row for the split table, but only historian info.  That may have been throwing us off.

I'm a bit stumped at this point as to whether the scanner is messed up or somehow when we remove the ""mother"" region we might be deleting too much, or something else all together.;;;","07/Sep/08 02:03;jimk;Jonathan,

WRT to the DroppedSnapshotException, can you confirm that you have hadoop-0.17.2.1 in the libs directory and that you are using 0.17.2.1 for Hadoop?

I noted that some native libraries were loaded in your log above. If you just have hadoop-0.17.2, be advised that the native libs are incorrect.

If you are using 0.17.2.1, then this is just a red herring.;;;","07/Sep/08 20:04;streamy;Jim, this is actually running off of 0.18.

We're not seeing the dropped snapshot anymore, so not sure if that is relevant anymore.;;;","07/Sep/08 22:06;stack;I was able to reproduce the jgray failures by doing this in PE:

{code}
Index: src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java
===================================================================
--- src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java (revision 692728)
+++ src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java (working copy)
@@ -481,7 +481,8 @@
     byte [] b = new byte[10];
     int d = Math.abs(number);
     for (int i = b.length - 1; i >= 0; i--) {
-      b[i] = (byte)((d % 10) + '0');
+      // b[i] = (byte)((d % 10) + '0');
+      b[i] = (byte)((d % 10));
       d /= 10;
     }
     return b;
{code}

We need to add a unit test with binary keys... as part of HBASE-859;;;","07/Sep/08 22:48;streamy;Patch appears to fix all issues.  Will confirm.;;;","07/Sep/08 23:38;stack;Reviewed the patch.

I added javadoc here:

{code}
+  // WritableComparator that takes account of meta keys.
+  public static class HStoreKeyWritableComparator extends WritableComparator {
+    private final HRegionInfo hri;
{code}

... and removed the creation of new objects done in the below:

{code}
Index: src/java/org/apache/hadoop/hbase/master/MetaRegion.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/MetaRegion.java (revision 692702)
+++ src/java/org/apache/hadoop/hbase/master/MetaRegion.java (working copy)
@@ -20,7 +20,10 @@
 package org.apache.hadoop.hbase.master;

 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.HServerAddress;
+import org.apache.hadoop.hbase.HStoreKey;
 import org.apache.hadoop.hbase.util.Bytes;


@@ -90,7 +93,9 @@
   public int compareTo(MetaRegion other) {
     int result = Bytes.compareTo(this.regionName, other.getRegionName());
     if(result == 0) {
-      result = Bytes.compareTo(this.startKey, other.getStartKey());
+      HRegionInfo hri = new HRegionInfo(HTableDescriptor.META_TABLEDESC, 
+          HConstants.EMPTY_END_ROW, HConstants.EMPTY_END_ROW);
+      result = HStoreKey.compareTwoRowKeys(hri, this.startKey, other.getStartKey());
       if (result == 0) {
         // Might be on different host?
         result = this.server.compareTo(other.server);
Index: src/java/org/apache/hadoop/hbase/client/MetaScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/MetaScanner.java    (revision 692702)
+++ src/java/org/apache/hadoop/hbase/client/MetaScanner.java    (working copy)
@@ -5,6 +5,7 @@
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HStoreKey;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.util.Bytes;

@@ -47,6 +48,7 @@
           HRegionInfo.createRegionName(tableName, null, ZEROES);

     // Scan over each meta region
+    HRegionInfo hri;
     do {
       ScannerCallable callable = new ScannerCallable(connection,
         META_TABLE_NAME, COLUMN_FAMILY_ARRAY, startRow, LATEST_TIMESTAMP, null);
@@ -62,12 +64,13 @@
         } while(visitor.processRow(r));
         // Advance the startRow to the end key of the current region
         startRow = callable.getHRegionInfo().getEndKey();
+        hri = new HRegionInfo(callable.getHRegionInfo());
       } finally {
         // Close scanner
         callable.setClose();
         connection.getRegionServerWithRetries(callable);
       }
-    } while (Bytes.compareTo(startRow, LAST_ROW) != 0);
+    } while (HStoreKey.compareTwoRowKeys(hri, startRow, LAST_ROW) != 0);
{code}

Otherwise, the patch looks fine.

This kinda change looks unnecessary in SFS but I'm afraid to change it since the patch works:
{code}
-      keys[i] = new HStoreKey();
+      keys[i] = new HStoreKey(HConstants.EMPTY_BYTE_ARRAY, this.store.getHRegionInfo());
{code}

I'm running unit tests now.  I'll then run the amended PE test to ensure binary keys work.;;;","08/Sep/08 00:15;stack;Committed branch and trunk.

Thanks Jon Gray and J-D for marathon weekend debugging session that produced this patch.

Passes all unit tests.

I also amended PE with binary keys.  Below is scan of table with binary keys.  Notice that there are more than one.

{code}
2008-09-07 16:58:31,982 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region TestTabl,1220831867463. Current region memcache size 64.0m
2008-09-07 16:58:32,600 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {regionname: .META.,,1, startKey: <>, server: 127.0.0.1:61450}
2008-09-07 16:58:32,604 DEBUG org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner REGION => {NAME => 'TestTable,,1220831633849', STARTKEY => '', ENDKEY => '          ', ENCODED => 377629314, TABLE => {{NAME => 'TestTable', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'false', IN_MEMORY => 'false', VERSIONS => '3', BLOCKCACHE => 'false', LENGTH => '2147483647', TTL => '-1', COMPRESSION => 'NONE'}]}}}, SERVER => '127.0.0.1:61450', STARTCODE => 1220831422560
2008-09-07 16:58:32,605 DEBUG org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner REGION => {NAME => 'TestTable,              ,1220831723306', STARTKEY => '          ', ENDKEY => '  ', ENCODED => 1215868091, TABLE => {{NAME => 'TestTable', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'false', IN_MEMORY => 'false', VERSIONS => '3', BLOCKCACHE => 'false', LENGTH => '2147483647', TTL => '-1', COMPRESSION => 'NONE'}]}}}, SERVER => '127.0.0.1:61450', STARTCODE => 1220831422560
2008-09-07 16:58:32,606 DEBUG org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner REGION => {NAME => 'TestTable,      ,1220831779573', STARTKEY => '  ', ENDKEY => ', ENCODED => 2091937218, TABLE => {{NAME => 'TestTable', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'false', IN_MEMORY => 'false', VERSIONS => '3', BLOCKCACHE => 'false', LENGTH => '2147483647', TTL => '-1', COMPRESSION => 'NONE'}]}}}, SERVER => '127.0.0.1:61450', STARTCODE => 1220831422560
2008-09-07 16:58:32,607 DEBUG org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner REGION => {NAME => 'TestTable,1220831779573', STARTKEY => ', ENDKEY => '', ENCODED => 1789154425, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'TestTable', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'false', IN_MEMORY => 'false', VERSIONS => '3', BLOCKCACHE => 'false', LENGTH => '2147483647', TTL => '-1', COMPRESSION => 'NONE'}]}}}, SERVER => '127.0.0.1:61450', STARTCODE => 1220831422560
2008-09-07 16:58:32,607 DEBUG org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner REGION => {NAME => 'TestTable,1220831867463', STARTKEY => ', ENDKEY =>', ENCODED => 1298791821, TABLE => {{NAME => 'TestTable', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'false', IN_MEMORY => 'false', VERSIONS => '3', BLOCKCACHE => 'false', LENGTH => '2147483647', TTL => '-1', COMPRESSION => 'NONE'}]}}}, SERVER => '127.0.0.1:61450', STARTCODE => 1220831422560
2008-09-07 16:58:32,608 DEBUG org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner REGION => {NAME => 'TestTabl,1220831867463', STARTKEY =>', ENDKEY => '', ENCODED => 639105844, TABLE => {{NAME => 'TestTable', IS_ROOT => 'false', IS_META => 'false', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'false', IN_MEMORY => 'false', VERSIONS => '3', BLOCKCACHE => 'false', LENGTH => '2147483647', TTL => '-1', COMPRESSION => 'NONE'}]}}}, SERVER => '127.0.0.1:61450', STARTCODE => 1220831422560
{code};;;","08/Sep/08 00:19;streamy;Thanks again to JD and stack for their help over the past four days, you guys definitely went above and beyond to help get this bug squashed.  Much appreciated.;;;","08/Sep/08 01:13;streamy;There's still something broken here.  It's not related to the splitting and all that, that still appears to be fixed.

Now there is an issue with obtaining HTable reference to the same table in question.  Exception is:

org.apache.hadoop.hbase.TableNotFoundException: items
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:508)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:460)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:420)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:130)
        at HBaseRef.<init>(HBaseRef.java:29)
        at Import.<init>(Import.java:20)
        at Import.main(Import.java:26)

Will post more info as I discover it.;;;","08/Sep/08 01:28;streamy;Needs another one-line patch to fix HCM.  Patch is actually in HStore.;;;","08/Sep/08 02:04;streamy;This does fix it.  But eventually it broke again with the same error.

There are quite a few references around the code for a synchronizedSortedMap/TreeMap instantiation that may also need to be passed the HSK comparator.;;;","08/Sep/08 02:12;jimk;Doesn't HSK.compareTo do the ""right thing""? If not, why can't it?;;;","08/Sep/08 02:16;streamy;It does do the right thing.  The issue is that it's not always being used.

We have almost certainly changed every Bytes.compareTo/Bytes.equals to HSK.compareTwoRowKeys/HSK.equalsTwoRowKeys necessary

The issue now is that many things use a SortedMap/TreeMap to iterate through results.  When those are instantiated, if it will be used with row keys, they must be passed the HSK comparator that was added with this patch.;;;","08/Sep/08 02:24;streamy;Could issue be in BloomFilterMapFile.Reader and/or HbaseMapWirtable (which it extends)?

HMW implementes a SortedMap<byte[],V> which is the culprit before.  We have nothing pointing it to our custom comparator.;;;","08/Sep/08 17:17;streamy;Already fixed, remaining issue in 877;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix javadoc warnings,HBASE-865,12403639,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,jimk,jimk,03/Sep/08 21:24,13/Sep/09 22:33,01/Jul/25 07:49,04/Sep/08 22:59,0.2.0,0.2.1,,,,0.18.0,0.2.2,,,,,,0,There are javadoc warnings in both the 0.2 branch and in trunk. They must be fixed before 0.2.2 or 0.18.0 are released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/08 14:53;rafan;865-0.2.patch;https://issues.apache.org/jira/secure/attachment/12389503/865-0.2.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25445,,,,,Thu Sep 04 22:59:33 UTC 2008,,,,,,,,,,"0|i0h9xb:",98870,,,,,,,,,,,,,,,,,,,,,"04/Sep/08 14:54;rafan;this patch should do the trick.;;;","04/Sep/08 22:15;jimk;Committed to trunk. Thanks for the patch Rong-En

(waiting for 0.2.1 to be tagged before committing to branch);;;","04/Sep/08 22:32;stack;I'd say go ahead and commit on branch.  RC1 at a minimum has been sunk by HBASE-868 (I'd add 866 too).;;;","04/Sep/08 22:59;jimk;Committed to 0.2 branch per Stack;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get with timestamp will return a value if there is a version with an earlier timestamp,HBASE-861,12403558,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,jimk,jimk,02/Sep/08 20:55,13/Sep/09 22:24,01/Jul/25 07:49,06/Jun/09 20:46,0.18.0,0.2.0,0.2.1,,,0.20.0,,,,,,,0,"When an explicit timestamp is specified, no results should be returned if there is no value stored at that timestamp.

A value should be returned (as it currently is) if the timestamp is defaulted or is LATEST_TIMESTAMP (which is the same thing). This works correctly.

Table name ""web"", columns: ""contents:"", ""anchor:""

store com.cnn.www/contents:/5 value = ""t5""
store com.cnn.www/anchor:my.look.ca:/8 value = ""CNN.com""
com.cnn.www/anchor:cnnsi.com/9 value = ""CNN""

get(com.cnn.www/contents:/8) should return nothing but returns value=""t5"", timestamp=5
get(com.cnn.www/anchor:my.look.ca:/9) should return nothing, but returns value=""CNN.com"", timestamp=8
",,,,,,,,,,,,,HBASE-899,,,,,HBASE-1249,HBASE-1304,,,,,,,,,,"02/Sep/08 20:57;jimk;TestGetTimestamp.java;https://issues.apache.org/jira/secure/attachment/12389375/TestGetTimestamp.java",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25441,,,,,Sat Jun 06 20:46:51 UTC 2009,,,,,,,,,,"0|i0h9wf:",98866,,,,,,,,,,,,,,,,,,,,,"02/Sep/08 20:56;jimk;Program that demonstrates the problem;;;","02/Sep/08 20:57;jimk;Test program that demonstrates the problem.;;;","02/Sep/08 21:01;apurtell;I noticed this behavior back at 0.1.0. I thought the semantics were ""give me whatever is most current for timestamp X"", then backwards from there up to max_versions? ;;;","02/Sep/08 21:09;jimk;No, it is definitely a bug. See the HBase Architecture document on the wiki.;;;","03/Sep/08 20:05;stack;Moved this out of 0.2.1.;;;","10/Sep/08 22:19;irubin;Has there been a consensus on whether this is actually a problem or not?  Last I heard, Stack thinks this is not a problem and Jim thinks it is.  Let me know what gets decided.;;;","28/Apr/09 17:00;streamy;Will be solved as part of 1249 related issues.;;;","26/May/09 16:14;streamy;Already fixed as part of HBASE-1304 a la TimeRange / Get.setTimeRange(min,max);;;","03/Jun/09 05:58;stack;New API doesn't have this issue.  I added test for it to TestTimestamp.  Search TimestampTestBase for hbase-861.;;;","03/Jun/09 07:23;stack;My test was wrong; this problem seems to be in place even in hbase-1304.;;;","06/Jun/09 20:46;streamy;Fixed as part of HBASE-1304 commit.

Verified in passing unit test:  org.apache.hadoop.hbase.client.TestClient.testJIRAs.jiraTest861();;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexTableReduce doesnt write the column name as the lucene index field properly.,HBASE-860,12403480,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,osm,osm,01/Sep/08 10:35,20/Sep/12 22:10,01/Jul/25 07:49,01/Sep/08 21:47,0.18.0,0.19.0,0.2.0,0.2.1,0.2.2,0.18.0,0.2.1,,,,,,0,"Instead of using the table column name as the field in the lucene index, the byte array jvm object id is written to the lucene index.  i.e.  [B@234DE3 instead of ""myColFamily:myCol""

In the class IndexTableReduce, essentially one line of code needs to be changed as far as i can see to fix this issue.  I will be submitting a patch here within the hour.",n/a,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,"01/Sep/08 10:58;osm;860.patch;https://issues.apache.org/jira/secure/attachment/12389282/860.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25440,,,,,Tue Sep 02 00:29:32 UTC 2008,,,,,,,,,,"0|i0h9w7:",98865,,,,,,,,,,,,,,,,,,,,,"01/Sep/08 10:58;osm;This patch is for the trunk.;;;","01/Sep/08 21:47;stack;Resolved.  Thanks for the patch Ryan (FYI, don't include the CHANGES.txt mod in your patch; it makes your patch brittle.  If anyone adds to CHANGES.txt, patch won't apply).;;;","01/Sep/08 22:08;osm;This appears to be step #3 under ""Creating a patch""

""Edit the CHANGES.txt file, adding a description of your change, including the bug number it fixes. If this is a new feature, or other enhancement that doesn't currently have a ticket please create one for it, then use it's number when adding your note to CHANGES.txt. You'll need this ticket to submit your patch anyway.""

Possibly the wiki should be updated?

http://wiki.apache.org/hadoop/Hbase/HowToContribute;;;","02/Sep/08 00:29;stack;Ok.  Thanks.   I removed it from the contrib doc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase-799 Created an error in util.migration.v5.HColumnDescriptor,HBASE-858,12403404,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,irubin,irubin,29/Aug/08 23:35,13/Sep/08 23:22,01/Jul/25 07:49,30/Aug/08 21:27,0.2.0,,,,,0.2.1,,,util,,,,0,"The patch for HBASE-799 accidentally introduced an error in org.apache.hadoop.hbase.util.migration.v5.HColumnDescriptor.  Line 345 of this file is currently:

this.name = Bytes.readByteArray(in);

This should be replaced by the following three lines, which were accidentally replaced by the HBASE-799 patch:

Text t = new Text();
t.readFields(in);
this.name = t.getBytes();

So, to summarize, line 345 of util.migration.v5.HColumnDescriptor should be replaced by the above 3 lines.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/08 02:07;jdcryans;hbase-858.patch;https://issues.apache.org/jira/secure/attachment/12389224/hbase-858.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25439,,,,,Sat Aug 30 21:27:00 UTC 2008,,,,,,,,,,"0|i0h9vr:",98863,,,,,,,,,,,,,,,,,,,,,"29/Aug/08 23:43;stack;The removal of these three lines look like they'd break migrations.  This issue should be a blocker on 0.2.1 (Oh, it already is).;;;","30/Aug/08 02:07;jdcryans;Fixes what Izaak described. Can you confirm that it solved the problem?;;;","30/Aug/08 21:27;stack;Committed (Checked previous version of HCD to make sure patch restored it to what it was previous).  Thanks for patch J-D.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compaction can return less versions then we should in some cases,HBASE-855,12403351,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,viper799,viper799,viper799,29/Aug/08 07:45,13/Sep/08 23:22,01/Jul/25 07:49,31/Aug/08 04:21,0.18.0,0.2.1,,,,0.18.0,0.2.1,,regionserver,,,,0,"say we have a column with max version = 3 and we have 3 records  
we insert a new record with a old timestamp.

What happeds in the compaction is the the new record with the old timestamp get read first and could push out some of our 
versions if the new record(s) with the old timestamp has a expired ttl.

This happens because we track the total times we see a row/column but do not reduce this count if the cell is expired
and sense we pass the cell in order of the newest HStoreFile first with the newest records passed might not be the newest timestamps.

Got to wait for HBASE-834 to be committed then I can add a patch for this bug. will be a simple fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/08 03:09;viper799;855-patch.txt;https://issues.apache.org/jira/secure/attachment/12389225/855-patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25437,,,,,Sun Aug 31 05:05:33 UTC 2008,,,,,,,,,,"0|i0h9v3:",98860,,,,,,,,,,,,,,,,,,,,,"29/Aug/08 07:50;viper799;This also effects 0.18.0 we can fix it in 0.18.0 or 0.18.1 eather way is fine with me;;;","30/Aug/08 03:09;viper799;This should fix a problem I seen in the max version and ttl

stack commented on HBASE-834 that he still seeing old version of data in .META. historian

I looked around and thank I might have found a second bug that might be the problem and included it in the patch

When we see a new row/column we where using the var timesSeen to track the times we have seen the same row/column.
but on a new row/column we set it to 0 on the first new row/column seen
then below we where check for max versions limit like this
if (timesSeen <= family.getMaxVersions()

With the timesSeen set to 0 that would allow 1 extra cell to pass on to the HStoreFile then the max versions setting
We should have been setting timesSeen to 1 in place of 0 sense its the first row/column seen not 0

I thank this might be your problem you are seeing stack when the record is 
deleted there could still be one old cell hidden in the table that will not show up until the newest cell is deleted.
;;;","30/Aug/08 03:09;viper799;patch should apply to trunk and branch;;;","30/Aug/08 19:14;stack;Thanks Billy.  TestCompactions is failing since we changed how compactions are done.  I was looking at it yesterday and was thinking that 855 was not fully to blame.  This new bug in our max versions logic might explain it.  Let me test.;;;","31/Aug/08 04:21;stack;Ran tests (took a while).  It works.  Applied to trunk and branch.  Thanks for the patch Billy.  ;;;","31/Aug/08 04:35;stack;This patch also fixes failing compaction test.;;;","31/Aug/08 05:05;stack;Hudson just passed; build #294.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleting and recreating a table in a single process does not work,HBASE-843,12403016,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,25/Aug/08 18:44,13/Sep/08 23:22,01/Jul/25 07:49,25/Aug/08 19:19,0.18.0,0.2.1,,,,0.18.0,0.2.1,,,,,,0,"When you delete and then recreate/enable the same table in the same process, when you get the HTable reference to the new table you are actually given the old table.

The connection information is never deleted/invalidated.

To fix, we add a call to HConnectionManager.deleteConnectionInfo(conf) at the end of HBaseAdmin.deleteTable().  This information will then be re-loaded with the latest table references once the client asks for it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/08 18:50;streamy;hbase-843-v1.patch;https://issues.apache.org/jira/secure/attachment/12388866/hbase-843-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25430,,,,,Mon Aug 25 19:19:21 UTC 2008,,,,,,,,,,"0|i0h9sf:",98848,,,,,,,,,,,,,,,,,,,,,"25/Aug/08 18:50;streamy;Tested and this patch appears to fix the issue described.;;;","25/Aug/08 18:51;streamy;Patch tested, applies to trunk;;;","25/Aug/08 18:53;streamy;Appears to apply both to trunk and 0.2 branch;;;","25/Aug/08 19:19;jimk;Committed to branch and trunk. Thanks for the patch Jonathan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update thrift examples to work with changed IDL (HBASE-697),HBASE-836,12402512,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,stack,stack,16/Aug/08 22:16,13/Sep/09 22:33,01/Jul/25 07:49,11/Sep/08 20:14,0.18.0,0.2.1,,,,0.18.0,,,Thrift,,,,0,Examples are now out of date since HBASE-697 went in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/08 17:39;jdcryans;hbase-836.patch;https://issues.apache.org/jira/secure/attachment/12389950/hbase-836.patch","28/Aug/08 12:55;tow21;hbase-thrift-python.diff;https://issues.apache.org/jira/secure/attachment/12389089/hbase-thrift-python.diff",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25428,,,,,Thu Sep 11 20:14:00 UTC 2008,,,,,,,,,,"0|i0h9qv:",98841,,,,,,,,,,,,,,,,,,,,,"28/Aug/08 12:55;tow21;This updates the python DemoClient to the new thrift API;;;","29/Aug/08 23:58;stack;Committed to TRUNK.  Thanks or the patch Toby.;;;","30/Aug/08 10:34;tow21;This should be re-opened: the c++, php, and ruby examples are still out-of-date; only java & python have been updated.;;;","30/Aug/08 18:48;stack;Sorry.  My fault.  Reopening.;;;","30/Aug/08 22:05;viper799;I ma seeing a error in 2.0 branch with the democlient.php
{code}
<html>
<head>
<title>DemoClient</title>
</head>
<body>
<pre>
scanning tables...
  found: webdata
creating table: demo_table
column families in demo_table:
  column: entry, maxVer: 10
  column: unused, maxVer: 3
PHP Fatal error:  Uncaught exception 'TApplicationException' with message 'Invalid method name: 'put'' in /var/www/compspy.com/thrift/packages/Hbase/Hbase.php:653
Stack trace:
#0 /var/www/compspy.com/thrift/packages/Hbase/Hbase.php(617): HbaseClient->recv_put()
#1 /var/www/compspy.com/includes/thriftdemo.php(114): HbaseClient->put('demo_table', 'foo', 'entry:foo', 'foo-??????')
#2 {main}
  thrown in /var/www/compspy.com/thrift/packages/Hbase/Hbase.php on line 653
{code}

changed affected version and fix version to include 2.1;;;","31/Aug/08 03:56;stack;Moving this out of 0.2.1.;;;","05/Sep/08 21:33;irubin;HBASE-874 will have to be resolved before a patch for this issue can be committed - the thrift tests create the problem described in HBASE-874.;;;","11/Sep/08 17:26;jimk;Making blocker per JD.;;;","11/Sep/08 17:39;jdcryans;Following discussion Izaak. Please review.;;;","11/Sep/08 18:38;jimk;+1 on patch. Pls commit and we can start working on an RC for hbase-0.18.0;;;","11/Sep/08 20:14;jdcryans;Committed to trunk. Thanks to Izaak for the insights.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doing an insert with an unknown family throws a NPE in HRS,HBASE-833,12402470,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,jdcryans,jdcryans,jdcryans,15/Aug/08 18:36,13/Sep/08 23:22,01/Jul/25 07:49,15/Aug/08 19:03,0.2.0,,,,,0.18.0,0.2.1,,regionserver,,,,0,"When I added the validation of value's length, I did not check if the family existed. Throws an ugly:

{code}
08/08/15 14:15:55 DEBUG client.HConnectionManager$TableServers: reloading table servers because: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.validateValuesLength(HRegionServer.java:1161)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1136)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)
{code}
with some retries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/08 18:40;jdcryans;hbase-833.patch;https://issues.apache.org/jira/secure/attachment/12388327/hbase-833.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25427,,,,,Fri Aug 15 19:03:51 UTC 2008,,,,,,,,,,"0|i0h9q7:",98838,,,,,,,,,,,,,,,,,,,,,"15/Aug/08 18:40;jdcryans;Checks if the family is null. The rest was already taken care of.;;;","15/Aug/08 18:41;stack;Tim Sell just found same issue over in HBASE-657;;;","15/Aug/08 18:45;stack;On patch:

I think it should throw... NoSuchColumnFamilyException
[11:44]	<st^ack>	This is a DoNotRetryIOException
[11:44]	<st^ack>	Then we don't retry it.
[11:44]	<st^ack>	Fixes tim's issue then.;;;","15/Aug/08 18:51;jdcryans;It already does in HRegion.checkColumn, my NPE was just hiding it.;;;","15/Aug/08 19:03;stack;You are right J-D.  I see it down in localput.

Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Problem with row keys beginnig with characters < than ',' and the region location cache",HBASE-832,12402396,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,14/Aug/08 17:28,13/Sep/08 23:22,01/Jul/25 07:49,31/Aug/08 05:21,0.2.0,,,,,0.18.0,0.2.1,,Client,regionserver,,,0,"We currently have a problem the way we design .META. row keys. When user table row keys begin with characters lesser than ',' like a '$', any operation will fail when:

- A client has a certain set of regions in cache
- One region with the faulty row key splits 
- The client receives a request for a row in the split region

The reason is that it will first get a NSRE then it will try to locate a region using the passed row key. For example: 
Row in META: entities,,1216750777411
Row passed: entities,$-94f9386f-e235-4cbd-aacc-37210a870991,99999999999999

The passed row is lesser then the row in .META.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/08 19:48;jdcryans;hbase-832-v1.patch;https://issues.apache.org/jira/secure/attachment/12389124/hbase-832-v1.patch","28/Aug/08 22:18;jdcryans;hbase-832-v2.patch;https://issues.apache.org/jira/secure/attachment/12389128/hbase-832-v2.patch","30/Aug/08 21:39;jdcryans;hbase-832-v3-trunk.patch;https://issues.apache.org/jira/secure/attachment/12389238/hbase-832-v3-trunk.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25426,,,,,Sun Aug 31 05:21:46 UTC 2008,,,,,,,,,,"0|i0h9pz:",98837,,,,,,,,,,,,,,,,,,,,,"14/Aug/08 19:10;jdcryans;Won't fix for 0.2.1;;;","14/Aug/08 19:30;stack;For 0.2.1, should we add rejecting row if starts with a byte that is <= ','?

My guess is that there is little extant data that has rows with row keys that are <= ',', else we would have heard about it (table wouldn't work).

Minimally, lets add release note that this is a known issue for 0.2.1.

Good find J-D.

;;;","14/Aug/08 19:41;apurtell;Of the set of bytes < ','  '$' and '*' are commonly used for special keys in various applications. '!' is a possibility also. 

Why not use the space character instead of comma? ;;;","14/Aug/08 20:06;streamy;I think it's a very bad idea to put any kind of constraint on byte[]'s anywhere.

Moving to byte[] has given the flexibility to put anything you want, wherever you want.  I can use serialized java objects as row keys if I want to.

Having any kind of reserved characters limits your ability to blindly store binary objects wherever you'd like.  Often the encoding of a binary object is implemented in a library you don't touch, if it just so happens to be using a reserved byte to start, you're SOL or stuck doing manually munging to ensure you fit the constraints on what are valid byte[]'s.;;;","14/Aug/08 20:18;stack;Taking a quick look, inserting into memcache and out to store files, we use HStoreKey.  The comparator in HSK knows how to make sense of our row/column/ts keys.  So, problem is elsewhere in the system; somewhere we are looking at the 'row' as raw byte array and we're using dumb Bytes.BYTE_COMPARATOR when it should be a byte comparator that knows it a HSK and that parses it appropriatlely (i.e going left to right up to the DELIMITER, first compare row component, then parse out the timestamp and the remainder is the column; compare this, then ts).  I wonder if its in our MR classes where we're using ImmutableBytesWritable?  Perhaps we should be using something 'smarter';;;","14/Aug/08 20:37;stack;Ignore my comment above.  Its off the mark.  The problem is not full row key sorting.  Its the sort of the row component of a key made of row/column/timestamp.  Issue is in meta where rows are made of the tablename, delimiter, rowname, delimiter, timestamp.  If the rowname in the table has delimiter in it or bytes that are < delimiter, then sort can be off.

;;;","14/Aug/08 21:04;stack;If region is a 'meta' region, could we write row keys with a subclass of HSK named something like MetaHSK?  MetaHSK would not treat row as a byte array but instead do simple parse to pull out the tablename and timestamp components.  Remained would be startkey.  Should be possible to then do compare that is not susceptible to changed ordering just because startkey contains delimiter?

Might have to have a version for root and another for meta given that the root has rows made of the meta tables rows.;;;","27/Aug/08 17:53;jdcryans;Getting worse. Here is another symptom:

{code}
org.apache.hadoop.hbase.regionserver.WrongRegionException: org.apache.hadoop.hbase.regionserver.WrongRegionException: Requested row out of range for HRegion web_pages,http://www.altitude737.com/choix_centre.html,1219807997979, startKey='http://www.altitude737.com/choix_centre.html', getEndKey()='http://www.amoll.qc.ca/', row='http://www.amoll.qc.ca/%8elections.html'
	at org.apache.hadoop.hbase.regionserver.HRegion.checkRow(HRegion.java:1703)
	at org.apache.hadoop.hbase.regionserver.HRegion.obtainRowLock(HRegion.java:1759)
	at org.apache.hadoop.hbase.regionserver.HRegion.batchUpdate(HRegion.java:1383)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1145)
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)
{code}

Made this a blocker for 0.2.1 and 0.18.0;;;","27/Aug/08 20:38;stack;Ugh.  +1 on fixing for 0.2.1;;;","28/Aug/08 00:08;jdcryans;I will try to do it the Bigtable-way: build the meta row keys with only the table and the end-row since it is the "","" between the two keys that is problematic.;;;","28/Aug/08 00:17;stack;J-D: One thing to keep in mind is that going between 0.2.0 and 0.2.1, a migration should not be necessary.  I'm afraid that changing format of key in meta will require migration.;;;","28/Aug/08 14:45;jdcryans;Yeah, I have it in mind. For 0.2.1, in the release notes, we should specify that this bug is very important and that it will be fixed for 0.3.0;;;","28/Aug/08 19:48;jdcryans;Patch that attempts at comparing rows from META and ROOT differently. Please review.;;;","28/Aug/08 20:42;jimk;Reviewed patch. Some issues and questions:
- Missing javadoc for @param in HStoreKey, Memcache
- lines too long in HStoreKey and HStore
- I am still unclear on why the table name needs to be put in and stripped out for the ROOT and Meta regions. Can you explain?
;;;","28/Aug/08 20:45;jimk;19 javadoc warnings. Run

ant clean javadoc

;;;","28/Aug/08 22:18;jdcryans;Fixes javadoc and the timestamp I forgot to check. Would have been critical when comparing the rows of slitted regions in catalog tables. Passes unit tests. Please review.;;;","28/Aug/08 23:05;jimk;javadoc still incorrect for:

public Memcache(final long ttl, HRegionInfo regionInfo)

otherwise +1;;;","29/Aug/08 00:11;clint.morgan;Hey J-D,

When working on this, did you get a sense of how easy it would be to add an arbitrary row key comparator (per table)? I'm probably gonna need this (HBASE-661) in the next few weeks.

cheers,;;;","29/Aug/08 02:50;jdcryans;Jim, sorry for the sloppy patch, it wasn't my best one. I will correct the last javadoc issue then will try it using the 2h MR job on which it fails.

Clint, I guess further refactoring would make this relatively easy to do. We'll discuss it later.;;;","29/Aug/08 18:59;stack;Would suggest that tests do actual full HSK compares rather than just row postions.

Maybe add not to the data member 'tablename' a javadoc that its not serialized as part of HSK (Point at this issue?).

I should look closer, but does it need to be passed into HSK?  Or does HStoreKey.compareTwoRowKeys not suffice in all cases?

If you passed HRI instead of table name to HSK, you could do HRI.isMetatable and HRI.isRoottable rather than do the table name compares you're currently doing.

;;;","29/Aug/08 19:08;jdcryans;bq. Would suggest that tests do actual full HSK compares rather than just row postions.

You mean using compareTo?

bq. Maybe add not to the data member 'tablename' a javadoc that its not serialized as part of HSK (Point at this issue?).

Indeed.

bq. I should look closer, but does it need to be passed into HSK? Or does HStoreKey.compareTwoRowKeys not suffice in all cases?

I passed it when the compareTo method was used. Sometimes in the code it was a row comparison, other times it was a HSK comparison in which I had to make sure that we checked the rows correctly.

bq. If you passed HRI instead of table name to HSK, you could do HRI.isMetatable and HRI.isRoottable rather than do the table name compares you're currently doing.

Indeed.;;;","29/Aug/08 22:32;stack;Yeah, I mean doing compareTo... Add tests to the testHStoreKey method that take the messy meta keys.;;;","30/Aug/08 21:39;jdcryans;Cleaner patch, adds a test and uses compareTo. Review please.;;;","31/Aug/08 05:21;stack;Looks good J-D.  I tested it by doing a decent loading up on cluster and ran all unit tests.  Patch is ugly but we can fix later when we have luxury of a migration (HBASE-859).  Applied trunk and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
committing BatchUpdate with no row should complain,HBASE-831,12402394,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,apurtell,bien,bien,14/Aug/08 17:22,13/Sep/08 23:22,01/Jul/25 07:49,14/Aug/08 19:14,0.2.0,,,,,0.18.0,0.2.1,,,,,,0,"Running this code:

BatchUpdate update = new BatchUpdate();
update.put(key, value);
table.commit(update);

Down in getRegionServer, this triggers an NPE because the row is null (which I saw because I was running in a debugger); this NPE gets retried somewhere in the bowels of IPC.  Instead, we should either remove the zero-arg BatchUpdate constructor, or have table.commit throw a runtimeexception if the row is null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/08 17:58;stack;831-v2-shortertimeouts.patch;https://issues.apache.org/jira/secure/attachment/12388323/831-v2-shortertimeouts.patch","14/Aug/08 17:52;apurtell;831.patch;https://issues.apache.org/jira/secure/attachment/12388262/831.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25425,,,,,Thu Aug 14 19:14:54 UTC 2008,,,,,,,,,,"0|i0h9pr:",98836,,,,,,,,,,,,,,,,,,,,,"14/Aug/08 17:25;bien;Also, hbase shouldn't catch and retry NPEs.;;;","14/Aug/08 17:42;bien;Additionally, the retry logic shouldn't retry NPEs.;;;","14/Aug/08 17:52;apurtell;Patch addresses invalid BatchUpdates. Checking happens on both the client and server side.;;;","14/Aug/08 19:14;jimk;Committed to 0.2 branch and trunk. Thanks for the patch Andrew!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debugging HCM.locateRegionInMeta is painful,HBASE-830,12402385,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,jdcryans,jdcryans,14/Aug/08 15:28,13/Sep/08 23:22,01/Jul/25 07:49,15/Aug/08 18:31,0.2.0,,,,,0.18.0,0.2.1,,Client,,,,0,"I've been debugging a case where a bunch of reduces were hanging for no apparent reason and then get killed because they did not do anything for 600 seconds. I figured that it's because we are stuck in a very long waiting time due to retry backoffs. 
{code}
public static int RETRY_BACKOFF[] = { 1, 1, 1, 1, 2, 4, 8, 16, 32, 64 };
{code}

That means we wait 10 sec, 10 sec, 10, 10, ... then 640 sec. That's a long time, do we really need that much time to finally be warned that there's a bug in HBase? 

Also, the places where we get this:
{code}
LOG.debug(""reloading table servers because: "" + t.getMessage());
{code}
should be more verbose. I my logs these are caused by a table not found but the only thing I see is ""reloading table servers because: tableName"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/08 18:09;stack;830-v2-shortertimeouts.patch;https://issues.apache.org/jira/secure/attachment/12388325/830-v2-shortertimeouts.patch","15/Aug/08 16:32;jdcryans;hbase-826-v1.patch;https://issues.apache.org/jira/secure/attachment/12388318/hbase-826-v1.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25424,,,,,Fri Aug 15 18:31:17 UTC 2008,,,,,,,,,,"0|i0h9pj:",98835,,,,,,,,,,,,,,,,,,,,,"14/Aug/08 17:40;apurtell;Regarding the backoff strategy, it assumes there is no bug in hbase of course only a temporary problem on the cluster and  gives a substantial grace period for recovery by default... as you have noticed. I think this is what the ""hbase.client.retries.number"" configuration option is for, if you are debugging. Also, setting the ""hbase.client.pause"" configuration option to something smaller than 10 seconds would have a related effect. ;;;","14/Aug/08 18:56;jdcryans;{quote}
I think this is what the ""hbase.client.retries.number"" configuration option is for, if you are debugging.
{quote}

It is, but I meant this in a default way. Should it be this long for everyone? During that time, there is almost no debug information so the job really seems to be hanged on nothing. I suggest either putting client.pause to 5 or at least giving more information.;;;","14/Aug/08 19:11;apurtell;client.retries.number = 8 sound good to me. 

client.pause = 10 is an old HBase default. Maybe someone just guessed that the typical enable/disable/create/alter table operation would fall within this timeframe, I don't know. At least its configurable for those that want to try shorter periods for maybe clusters with few regionservers and not too many regions. 
;;;","15/Aug/08 15:53;stack;Make a patch lads and I'll apply it.  Yeah, 640 seconds is a long time.   In MR at least, let the task fail earlier and have it retried.;;;","15/Aug/08 16:30;jdcryans;Patch puts client pause to 8 instead of 10 and adds more useful debug information.;;;","15/Aug/08 16:32;jdcryans;Forgot to remove a tab.;;;","15/Aug/08 18:09;stack;I think that the pause should be shorter still. Rather than 8 seconds, it should be 2 seconds. Attached file changes the pause to 2 seconds and removes the 64 off the end of RETRY_BACKOFF.

At 2 seconds all is 'snappier' - creating, disablng, etc. - and after ten retries we're at about 2 minutes which is long-enough in my opinion and is < TaskTracker timeout of ten minutes so we'll see error in MR logs rather than child killed messages.

At 8 seconds, with RETRY_BACKOFF as it was, we're waiting > 17 minutes (if I did my math right).

This patch passes all unit tests. Also tried it in a big MR job. More logging if client DEBUG is enabled but at least now you have a better clue whats going on.;;;","15/Aug/08 18:11;jdcryans;+1 on this version of the patch;;;","15/Aug/08 18:19;apurtell;+1 indeed;;;","15/Aug/08 18:20;stack;Its not a patch (smile).  Just a suggested configuration change.  I'll change the suggested 'configuration' since it got two +1s.;;;","15/Aug/08 18:31;stack;Committed to branch and trunk.  Thanks Andrew and J-D.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
master logs showing byte[] in place of string on logging,HBASE-825,12402255,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,viper799,viper799,viper799,12/Aug/08 23:36,13/Sep/08 23:22,01/Jul/25 07:49,13/Aug/08 00:21,0.18.0,0.2.1,,,,0.18.0,0.2.1,,master,,,,0,"{code}
2008-08-12 17:39:48,586 INFO org.apache.hadoop.hbase.master.RegionManager: Skipping region [B@6a63d3 because it is already closing.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 23:38;viper799;825-patch.txt;https://issues.apache.org/jira/secure/attachment/12388091/825-patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25419,,,,,Wed Aug 13 00:21:34 UTC 2008,,,,,,,,,,"0|i0h9o7:",98829,,,,,,,,,,,,,,,,,,,,,"13/Aug/08 00:21;stack;Committed branch and trunk.  Thanks Billy.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Hlog we print array of byes for region name,HBASE-824,12402252,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,viper799,viper799,viper799,12/Aug/08 21:49,13/Sep/08 23:22,01/Jul/25 07:49,12/Aug/08 22:12,0.18.0,0.2.1,,,,0.18.0,0.2.1,,regionserver,,,,0,"I see lines in the debug logs like this
{code}
2008-08-12 16:13:20,638 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Found 1 logs to remove using oldest outstanding seqnum of 265156192 from region [B@18a3257
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 21:50;viper799;824-patch.txt;https://issues.apache.org/jira/secure/attachment/12388087/824-patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25418,,,,,Tue Aug 12 22:12:10 UTC 2008,,,,,,,,,,"0|i0h9nz:",98828,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 22:12;stack;Committed to TRUNK and branch.  Thanks for the patch Billy.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove DOS-style ^M carriage returns from all code where found",HBASE-819,12402167,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,11/Aug/08 23:57,13/Sep/08 23:22,01/Jul/25 07:49,12/Aug/08 01:01,0.2.0,,,,,0.18.0,0.2.1,,,,,,0,"There are a few files that contain DOS-style carriage returns.  This is leading to issues when applying patches.

The presence of these may also be causing a snowball effect as some IDEs/editors may see one and attempt to apply that LF/CR format to all lines or files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 00:03;streamy;hbase-819-v1.patch;https://issues.apache.org/jira/secure/attachment/12388009/hbase-819-v1.patch","12/Aug/08 00:07;streamy;hbase-819-v2.patch;https://issues.apache.org/jira/secure/attachment/12388010/hbase-819-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25414,,,,,Tue Aug 12 01:01:37 UTC 2008,,,,,,,,,,"0|i0h9mv:",98823,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 00:03;streamy;For *.java in the source tree, the following files contain the improper line breaks:

src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java
src/java/org/apache/hadoop/hbase/HConstants.java
src/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
src/java/org/apache/hadoop/hbase/client/HTable.java
src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java

HRegionServer was also wrong before, but was fixed by Jim so no problems in trunk.

Patch to follow;;;","12/Aug/08 00:07;streamy;v2 patch just patches CHANGES.txt

Not sure if it's necessary but for completeness... (v2 does not contain v1 changes, these are separate);;;","12/Aug/08 00:09;apurtell;I found the same set.;;;","12/Aug/08 01:01;jimk;Committed to 0.2 branch and trunk. Thanks for the patch Jonathan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock running 'flushSomeRegions',HBASE-818,12402166,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,apurtell,stack,stack,11/Aug/08 23:53,13/Sep/08 23:22,01/Jul/25 07:49,12/Aug/08 16:22,0.2.0,,,,,0.18.0,0.2.1,,,,,,0,"Playing with MR uploading no a regionserver with 60+ regions, I ran into a deadlock:

{code}
Found one Java-level deadlock:
=============================
""IPC Server handler 19 on 60020"":
  waiting to lock monitor 0x084be38c (object 0xb6f69a70, a org.apache.hadoop.hbase.regionserver.Flusher),
  which is held by ""IPC Server handler 16 on 60020""
""IPC Server handler 16 on 60020"":
  waiting to lock monitor 0x080f8dec (object 0xb73610c0, a org.apache.hadoop.hbase.regionserver.HRegion$WriteState),
  which is held by ""IPC Server handler 2 on 60020""
""IPC Server handler 2 on 60020"":
  waiting to lock monitor 0x086e8fe8 (object 0xb6f69cf0, a java.util.HashSet),
  which is held by ""IPC Server handler 16 on 60020""

Java stack information for the threads listed above:
===================================================
""IPC Server handler 19 on 60020"":
        at org.apache.hadoop.hbase.regionserver.Flusher.flushSomeRegions(Flusher.java:261)
        - waiting to lock <0xb6f69a70> (a org.apache.hadoop.hbase.regionserver.Flusher)
        at org.apache.hadoop.hbase.regionserver.Flusher.reclaimMemcacheMemory(Flusher.java:252)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1136)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:623)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)
""IPC Server handler 16 on 60020"":
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:948)
        - waiting to lock <0xb73610c0> (a org.apache.hadoop.hbase.regionserver.HRegion$WriteState)
        at org.apache.hadoop.hbase.regionserver.Flusher.flushRegion(Flusher.java:173)
        - locked <0xb6f69cf0> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.Flusher.flushSomeRegions(Flusher.java:267)
        - locked <0xb6f69a70> (a org.apache.hadoop.hbase.regionserver.Flusher)
        at org.apache.hadoop.hbase.regionserver.Flusher.reclaimMemcacheMemory(Flusher.java:252)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1136)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:623)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)
""IPC Server handler 2 on 60020"":
        at org.apache.hadoop.hbase.regionserver.Flusher.addRegion(Flusher.java:237)
        - waiting to lock <0xb6f69cf0> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.Flusher.request(Flusher.java:114)
        at org.apache.hadoop.hbase.regionserver.HRegion.requestFlush(HRegion.java:1627)
        - locked <0xb73610c0> (a org.apache.hadoop.hbase.regionserver.HRegion$WriteState)
        at org.apache.hadoop.hbase.regionserver.HRegion.update(HRegion.java:1614)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchUpdate(HRegion.java:1398)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1137)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:623)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

Found 1 deadlock.
{code}

Regionserver is hosed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 08:14;apurtell;818.patch;https://issues.apache.org/jira/secure/attachment/12388025/818.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25413,,,,,Tue Aug 12 16:22:05 UTC 2008,,,,,,,,,,"0|i0h9mn:",98822,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 05:58;stack;Suggest moving the line 'this.flushListener.request(this);' outside of the synchronize on WriteState.  Testing here, it seems to do the trick.  There's no need of the flush request being inside the WriteState synchronize block since the scheduling of flushes is async anyways.;;;","12/Aug/08 16:22;stack;Applied branch and trunk. Thanks for the patch Andrew.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction needs little better skip algo,HBASE-812,12402078,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,dleffel,viper799,viper799,10/Aug/08 15:09,13/Sep/08 23:22,01/Jul/25 07:49,12/Aug/08 04:21,0.18.0,0.2.0,,,,0.18.0,0.2.1,,regionserver,,,,0,"Looking at this section of one of my compaction's we have 3 files to compact the new algo is working great in my test but I see this below often we are skipping 2 out of the 3 files and compacting 1 file. 1 file is kind of a wast might as well just copy the file my suggestion is if there is only 1 file left after the new algo skips then just go on to the next column and skip the last file also. This will help improve compaction times a little more. 

{code}
2008-08-10 10:00:45,310 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 1339600874/size: 4.6m, skipped 2, 4851776
2008-08-10 10:00:45,438 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 1 files into /hbase/webdata/compaction.dir/1339600874/size/mapfiles/8653208152776334891
2008-08-10 10:00:46,838 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/1339600874/size/mapfiles/8653208152776334891 to /hbase/webdata/1339600874/size/mapfiles/7539342470259528578
2008-08-10 10:00:47,166 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 1339600874/size store size is 4.6m
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/08 22:38;dleffel;812.patch.txt;https://issues.apache.org/jira/secure/attachment/12388004/812.patch.txt","12/Aug/08 02:05;viper799;812v2.patch;https://issues.apache.org/jira/secure/attachment/12388013/812v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25412,,,,,Tue Aug 12 04:21:40 UTC 2008,,,,,,,,,,"0|i0h9lb:",98816,,,,,,,,,,,,,,,,,,,,,"11/Aug/08 20:06;stack;Billy: If we're compacting a single file, thats just dumb and we need to fix it (Doesn't look like the file we're compacting is a reference).  Do you have other examples?  Can you update to later hbase too -- looks like you are on older stuff -- and if the problem still exists, can you put in more examples?;;;","11/Aug/08 20:07;stack;Made this critical just because its so dumb that we're running compation of a single file.;;;","11/Aug/08 21:16;viper799;I am running the 0.2.0 release and no its not compacting a reference. after a split we do not do the skip algo when we are compacting a reference its a force compaction of all the files. 
I have more examples thats just one of the columns but we are doing this a lot when I am running a import job.

But the skip algo is working great over all I see compaction's of about 1 min vs 5-7 mins per compaction while under insert load.
There is some in the below log I just found a group of compaction's and cut this out of the log I thank its after a startup but still you can see the skip algo picks 2 on 
some of them and about half it only has 1 left we just need to skip the columns that only have 1 mapfile after the algo does the selection of the files. then on the next 
compaction for the region it will have 2 mapfiles and do something when its compacting.



{code}
2008-08-10 23:56:13,563 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/200761489/anchor/mapfiles/4695022772964499808 to /hbase/webdata/200761489/anchor/mapfiles/6542744344240857591
2008-08-10 23:56:13,631 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 200761489/anchor store size is 72.8m
2008-08-10 23:56:13,637 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 200761489/stime: 5.1m, skipped 1, 4191303
2008-08-10 23:56:13,706 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/200761489/stime/mapfiles/6863349056798671266
2008-08-10 23:56:15,559 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/200761489/stime/mapfiles/6863349056798671266 to /hbase/webdata/200761489/stime/mapfiles/3093620455129104149
2008-08-10 23:56:15,639 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 200761489/stime store size is 5.1m
2008-08-10 23:56:15,647 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 200761489/in_rank: 62.5m, skipped 1, 55688666
2008-08-10 23:56:15,724 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/200761489/in_rank/mapfiles/1559371687332050999
2008-08-10 23:56:32,318 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/200761489/in_rank/mapfiles/1559371687332050999 to /hbase/webdata/200761489/in_rank/mapfiles/5872775000657698199
2008-08-10 23:56:32,400 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 200761489/in_rank store size is 61.1m
2008-08-10 23:56:32,407 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 200761489/size: 5.5m, skipped 1, 4465439
2008-08-10 23:56:32,472 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/200761489/size/mapfiles/3296022424629716093
2008-08-10 23:56:34,710 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/200761489/size/mapfiles/3296022424629716093 to /hbase/webdata/200761489/size/mapfiles/8583717577394542471
2008-08-10 23:56:34,782 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 200761489/size store size is 5.4m
2008-08-10 23:56:34,789 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 200761489/last_seen: 61.5m, skipped 1, 54824835
2008-08-10 23:56:34,872 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/200761489/last_seen/mapfiles/3943134197356605963
2008-08-10 23:56:50,981 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/200761489/last_seen/mapfiles/3943134197356605963 to /hbase/webdata/200761489/last_seen/mapfiles/307647539041731711
2008-08-10 23:56:51,219 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 200761489/last_seen store size is 60.1m
2008-08-10 23:56:51,242 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region webdata,de.popstars-new-angels.www%2Fpopstars-biographien.html%3Ahttp,1218422328154 in 52sec
2008-08-10 23:56:51,245 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting compaction on region webdata,fr.asso.agirpourlenfant.www%2Findex.html%3Ahttp,1218422328154
2008-08-10 23:56:51,252 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 338914710/anchor: 81.6m, skipped 2, 85047362
2008-08-10 23:56:51,268 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 1 files into /hbase/webdata/compaction.dir/338914710/anchor/mapfiles/2546621217286575105
2008-08-10 23:56:52,143 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/338914710/anchor/mapfiles/2546621217286575105 to /hbase/webdata/338914710/anchor/mapfiles/3839219944106123077
2008-08-10 23:56:52,188 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 338914710/anchor store size is 80.4m
2008-08-10 23:56:52,195 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 338914710/stime: 5.6m, skipped 2, 5784755
2008-08-10 23:56:52,210 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 1 files into /hbase/webdata/compaction.dir/338914710/stime/mapfiles/1286630914618726188
2008-08-10 23:56:52,414 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/338914710/stime/mapfiles/1286630914618726188 to /hbase/webdata/338914710/stime/mapfiles/5121005992036000876
2008-08-10 23:56:52,456 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 338914710/stime store size is 5.5m
2008-08-10 23:56:52,462 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 338914710/in_rank: 71.3m, skipped 2, 74283017
2008-08-10 23:56:52,486 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 1 files into /hbase/webdata/compaction.dir/338914710/in_rank/mapfiles/1957109104676001133
2008-08-10 23:56:53,524 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/338914710/in_rank/mapfiles/1957109104676001133 to /hbase/webdata/338914710/in_rank/mapfiles/9145853810894409994
2008-08-10 23:56:53,569 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 338914710/in_rank store size is 69.8m
2008-08-10 23:56:53,576 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 338914710/size: 5.9m, skipped 2, 6167045
2008-08-10 23:56:53,589 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 1 files into /hbase/webdata/compaction.dir/338914710/size/mapfiles/5911079568309110370
2008-08-10 23:56:53,809 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/338914710/size/mapfiles/5911079568309110370 to /hbase/webdata/338914710/size/mapfiles/5518032354875458699
2008-08-10 23:56:53,853 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 338914710/size store size is 5.9m
2008-08-10 23:56:53,860 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 338914710/last_seen: 70.6m, skipped 2, 73521266
2008-08-10 23:56:53,902 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 1 files into /hbase/webdata/compaction.dir/338914710/last_seen/mapfiles/4617217208970844256
2008-08-10 23:56:54,898 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/338914710/last_seen/mapfiles/4617217208970844256 to /hbase/webdata/338914710/last_seen/mapfiles/6045607152317204195
2008-08-10 23:56:54,943 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 338914710/last_seen store size is 69.1m
2008-08-10 23:56:54,948 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region webdata,fr.asso.agirpourlenfant.www%2Findex.html%3Ahttp,1218422328154 in 3sec
2008-08-10 23:56:54,951 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting compaction on region webdata,org.jessejacksonjr.www%2Fissues%2Fi0809001569.html%3Ahttp,1218423851154
2008-08-10 23:56:54,961 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 376160388/anchor: 70.0m, skipped 2, 72330348
2008-08-10 23:56:55,258 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/376160388/anchor/mapfiles/6390472557291621209
2008-08-10 23:56:56,953 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/376160388/anchor/mapfiles/6390472557291621209 to /hbase/webdata/376160388/anchor/mapfiles/6570235857466899342
2008-08-10 23:56:57,001 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 376160388/anchor store size is 69.0m
2008-08-10 23:56:57,008 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 376160388/stime: 4.5m, skipped 1, 4373520
2008-08-10 23:56:57,041 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/376160388/stime/mapfiles/8058281260768773414
2008-08-10 23:56:57,655 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/376160388/stime/mapfiles/8058281260768773414 to /hbase/webdata/376160388/stime/mapfiles/6655752511205630946
2008-08-10 23:56:57,710 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 376160388/stime store size is 4.4m
2008-08-10 23:56:57,719 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 376160388/in_rank: 60.7m, skipped 2, 62605727
2008-08-10 23:56:58,020 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/376160388/in_rank/mapfiles/6388173210597572693
2008-08-10 23:56:59,448 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/376160388/in_rank/mapfiles/6388173210597572693 to /hbase/webdata/376160388/in_rank/mapfiles/7217625771579221464
2008-08-10 23:56:59,498 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 376160388/in_rank store size is 59.4m
2008-08-10 23:56:59,504 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 376160388/size: 4.8m, skipped 1, 4651030
2008-08-10 23:56:59,912 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/376160388/size/mapfiles/6505866612377469653
2008-08-10 23:57:00,566 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/376160388/size/mapfiles/6505866612377469653 to /hbase/webdata/376160388/size/mapfiles/3951148926146160516
2008-08-10 23:57:00,612 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 376160388/size store size is 4.7m
2008-08-10 23:57:00,621 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Compaction size of 376160388/last_seen: 59.8m, skipped 2, 61665980
2008-08-10 23:57:00,814 DEBUG org.apache.hadoop.hbase.regionserver.HStore: started compaction of 2 files into /hbase/webdata/compaction.dir/376160388/last_seen/mapfiles/6278945227615596465
2008-08-10 23:57:02,202 DEBUG org.apache.hadoop.hbase.regionserver.HStore: moving /hbase/webdata/compaction.dir/376160388/last_seen/mapfiles/6278945227615596465 to /hbase/webdata/376160388/last_seen/mapfiles/4551300457309362976
2008-08-10 23:57:02,249 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Completed compaction of 376160388/last_seen store size is 58.5m
2008-08-10 23:57:02,253 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region webdata,org.jessejacksonjr.www%2Fissues%2Fi0809001569.html%3Ahttp,1218423851154 in 7sec

{code};;;","11/Aug/08 21:35;stack;Thanks Billy.  Let me see if I can see whats broke (when you going to learn java?  So you can fix these things yourself (smile));;;","11/Aug/08 22:38;dleffel;Looks like this might fix the issue. It's my first patch though, so Billy, you might test it first.;;;","12/Aug/08 02:04;viper799;I made a new patch to add a new line on the debug logging its kind of hard to catch those 3 words on the end of the line and know whats going on and fixed a space missed on the old log line you edited. 
Fill free to change the text but I would like to see a new line saying we skipped the compaction of only 1 file.

but patch works great does just what was needed!

 ;;;","12/Aug/08 04:21;stack;Committed.  Thanks for the patch Daniel (and the edit Billy).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTD is not fully copyable,HBASE-811,12402076,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,apurtell,apurtell,apurtell,10/Aug/08 02:22,13/Sep/08 23:22,01/Jul/25 07:49,11/Aug/08 20:49,0.2.0,,,,,0.18.0,0.2.1,,Client,,,,0,Part of my HBASE-62 patch was not applied. ,,,,,,,,,,,,,HBASE-729,,,,,,,,,,,,,,,,"10/Aug/08 02:25;apurtell;811.patch;https://issues.apache.org/jira/secure/attachment/12387891/811.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25411,,,,,Mon Aug 11 20:49:42 UTC 2008,,,,,,,,,,"0|i0h9l3:",98815,,,,,,,,,,,,,,,,,,,,,"11/Aug/08 20:49;jimk;Reviewed patch. Committed to 0.2 branch and trunk. Thanks Andrew!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Prevent temporary deadlocks when, during a scan with write operations, the region splits",HBASE-810,12402046,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,jdcryans,jdcryans,09/Aug/08 01:14,13/Sep/08 23:22,01/Jul/25 07:49,21/Aug/08 21:40,0.2.0,,,,,0.18.0,0.2.1,,,,,,0,"HBASE-804 was not about the good problem, this one is. Anyone that iterates through the results of a scanner and that rewrites data back into the row at each iteration will hit a UnknownScannerException if a split occurs. See the stack in the referred jira. Timeline :

Split occurs, acquires a write lock and waits for scanners to finish
The scanner in the custom code iterates and writes data until the write is blocked by the lock
deadlock
The scanner timeouts thus the region splits but the USE will be thrown when next() is called

Inside a Map, the task will simply be retried when the first one fails. Elsewhere, it becomes more complicated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/08 16:56;jdcryans;hbase-810-v1.patch;https://issues.apache.org/jira/secure/attachment/12388680/hbase-810-v1.patch","20/Aug/08 23:16;jimk;lock-sequencing.jpg;https://issues.apache.org/jira/secure/attachment/12388643/lock-sequencing.jpg","20/Aug/08 20:56;jimk;locking-compatibility.jpg;https://issues.apache.org/jira/secure/attachment/12388630/locking-compatibility.jpg",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25410,,,,,Thu Aug 21 21:40:30 UTC 2008,,,,,,,,,,"0|i0h9kv:",98814,,,,,,,,,,,,,,,,,,,,,"11/Aug/08 14:57;srainville;What's the most problematic about this issue is that if we're in the context of a map/reduce job and that a map task fails when a split occurs then when the task is restarted it will only scan the rows in the resulting first region... simply because the input of a hbase map/reduce job is split based on the regions. Actually, is it a bigger problem that could be the consequence of other *normal* failures in a map task?
;;;","11/Aug/08 17:45;stack;I don't think so Sebastien.  A TableSplit is defined by a start and end row.  On MR job setup, these correspond to region demarcations.  If the region splits during the subsequent MR job, and the map fails and then reruns, it'll scan the two daughters since the map tasks TableSplit spans the two regions.

;;;","11/Aug/08 19:26;apurtell;Is this resolved by HBASE-816?;;;","12/Aug/08 20:29;stack;Marking as blocker.  Updating as you scan is a common operation.;;;","15/Aug/08 23:35;jimk;This is very ugly. The locking as depicted in HBASE-316 is essentially correct.

If we wanted to be more responsive during a split, we should use a tryLock in HRegion.getScanner(...)

This would allow us to scan (if we get the lock) or split (and continue from the last scanned row if we don't).

Maybe getScanner should do a synchronized(splitLock) as well.

Or, maybe splits should be more like cache flushes in that they only acquire a write lock at the end, when they are ready to move new HStores into place? No, that won't work for splits because splits require the master to reassign the children, whereas flushes and compactions continue to be served from the same HRegionServer and the row range for HRegion is the same.

It appears as if a region is splitting, any outstanding scanners either need to finish scanning the region (blocking the split) or the scanners need to be notified that a split is going to happen and they need to wait and recalibrate.
;;;","19/Aug/08 15:03;jdcryans;{quote}
It appears as if a region is splitting, any outstanding scanners either need to finish scanning the region (blocking the split) or the scanners need to be notified that a split is going to happen and they need to wait and recalibrate.
{quote}

This is also what I think we should do but like I said on IRC, we don't currently have a way to communicate from the region server to the scanner. Suppose the user code did a next() and while processing the info the split happens (and it will finally try to insert something but won't be able to since the split is blocking). What we need is an handle to the scanner to tell it to ""close"" itself and wait.;;;","20/Aug/08 02:10;jimk;Looking at the code, I have realized that a lot has changed since HBASE-316.

I am currently working on three diagrams:
- how it works today
- how it should work
- what needs to be changed.

I have already found several cases where different objects are being used for synchronization of the same operation. (Not sure how it got that way, but after looking at the code I'd be surprised if there WERE NOT deadlocks).

I have three operations left to analyze: compaction, split and log rolling. I hope to finish the analysis tomorrow.;;;","20/Aug/08 21:09;jimk;I have attached two screenshots to this issue.

locking-compatibility shows what operations are compatible and how they are protected from each other

lock-sequencing shows how locking should work. There are a couple of changes from the current state of affairs:

- StoreFileScanner constructor takes out a read lock on the HStore. This is not necessary as a read lock is already held in HStore.

- HRegion.get(byte [] row, byte [] column, long timestamp,  int numVersions) (which all the other variants of get call) and HRegion.compactStores() should take out splitsAndClosesLock.readLock().lock() at the start of the method, and release it at the end, using try - finally

;;;","20/Aug/08 22:24;jimk;Ok, now I see the problem (duh!). HRegion.close acquires the splitsAndCloses write lock preventing further updates. But then it waits until all the scanners exit. But the process doing the scanning also wants to do updates, so it is blocked on the splitsAndCloses write lock. Thus all the scanners cannot exit so close is blocked, and the process cannot do updates so it is blocked.

I will post a new lock sequence diagram that indicates the changes needed. All changes will be marked with RED.;;;","20/Aug/08 23:00;jimk;To reiterate, the following changes should be made:

- Remove HStore read lock in StoreFileScanner constructor

- HRegion.get(byte [] row, byte [] column, long timestamp, int numVersions) (which all the other variants of get call) and HRegion.compactStores() should take out splitsAndClosesLock.readLock().lock() at the start of the method, and release it at the end, using try - finally

- In HRegion.close, move the wait for active scanners outside the acquisition of splitsAndClosesLock and prevent new scanners by adding a reentrant read write lock named something like new scanner lock. New code would look something like:

{code}
  private final ReentrantReadWriteLock newScannerLock = new ReentrantReadWriteLock();
...
In close(boolean):
    newScannerLock.writeLock().lock();
    LOG.debug(""Scanners disabled for region "" + this);
    try {
        // Wait for active scanners to finish.
        synchronized (activeScannerCount) {
          while (activeScannerCount.get() != 0) {
            LOG.debug(""waiting for "" + activeScannerCount.get() +
                "" scanners to finish"");
            try {
              activeScannerCount.wait();
            } catch (InterruptedException e) {
              // continue
            }
          }
        }
        LOG.debug(""No more active scanners for region "" + this);
        splitsAndClosesLock.writeLock().lock();
        LOG.debug(""Updates disabled for region "" + this);
        try {
...
        } finally {
          splitsAndClosesLock.writeLock().unlock();
        }
    } finally {
      newScannerLock.writeLock().unlock();
    }
{code}

Then change the use of splitsAndClosesLock in getScanner to use newScannerLock instead.

New sequencing diagram forthcoming.;;;","21/Aug/08 16:56;jdcryans;Fixes what Jim told to fix. I also added a test that should fail without this patch.;;;","21/Aug/08 21:40;jimk;Committed to 0.2 branch and trunk. Thanks for the patch JD!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary getRow overloads in HRS,HBASE-805,12402021,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,streamy,streamy,08/Aug/08 17:49,13/Sep/08 23:22,01/Jul/25 07:49,11/Aug/08 18:03,0.2.0,,,,,0.18.0,0.2.1,,regionserver,,,,0,"HRS currently contains:

  public RowResult getRow(final byte [] regionName, final byte [] row, final long ts)

  public RowResult getRow(final byte [] regionName, final byte [] row, final byte [][] columns)

  public RowResult getRow(final byte [] regionName, final byte [] row, final byte [][] columns, final long ts)

The first two call the last one which calls HR.getFull.

Changes will be made to HTable to map all getRow calls to a single getRow HRS method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/08 18:13;streamy;hbase-805-v1.patch;https://issues.apache.org/jira/secure/attachment/12387840/hbase-805-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25406,,,,,Mon Aug 11 18:03:51 UTC 2008,,,,,,,,,,"0|i0h9jr:",98809,,,,,,,,,,,,,,,,,,,,,"08/Aug/08 18:29;jimk;Reviewed patch. +1.

Cannot commit until 0.2 branch is created.;;;","08/Aug/08 19:19;jimk;Committed to 0.2 branch and trunk. Thanks for the patch Jonathan!;;;","11/Aug/08 16:28;clint.morgan;-1 This patch changed all lines of HRegionServer. (apears to be whitespace change)

This makes maintaining other patches a PITA...;;;","11/Aug/08 16:56;stack;Hmm.  The patch doesn't change all lines.  Was it when you did the application Jim that all lines changed?;;;","11/Aug/08 18:03;jimk;Fixed whitespace diffs in HRegionServer;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable.getStartKeys() ignores table names when matching columns,HBASE-793,12401576,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,04/Aug/08 06:52,20/Sep/12 22:10,01/Jul/25 07:49,04/Aug/08 17:27,0.2.0,,,,,0.2.0,,,,,,,0,"Dru Jensen wrote on hbase-user@
> I found what is causing the same rows being sent to multiple map tasks.
> If you have the same column family name in other tables, the Test will
> send the same rows to multiple map reducers.

Stack wrote in response:

> Indeed, a bug in getStartKeys will make us process all tables that have
> a column family name in common.
[...]
> The above Visitor is visiting the meta table.  Its checking column
> family name.  Any region that is not offlined or split gets added to the
> list of regions.  Its not checking that the region belongs to the wanted
> table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/08 06:54;apurtell;0.2.0-793.patch;https://issues.apache.org/jira/secure/attachment/12387450/0.2.0-793.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25403,,,,,Mon Aug 04 17:27:55 UTC 2008,,,,,,,,,,"0|i0h9hb:",98798,,,,,,,,,,,,,,,,,,,,,"04/Aug/08 06:54;apurtell;Patch attached. Passes all tests:

    [mkdir] Created dir: /usr/src/Hadoop/hbase/build/test/logs
    [junit] Running org.apache.hadoop.hbase.TestClassMigration
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.077 sec
    [junit] Running org.apache.hadoop.hbase.TestCompare
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.021 sec
    [junit] Running org.apache.hadoop.hbase.TestEmptyMetaInfo
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 32.513 sec
    [junit] Running org.apache.hadoop.hbase.TestGlobalMemcacheLimit
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 49.011 sec
    [junit] Running org.apache.hadoop.hbase.TestHBaseCluster
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 41.314 sec
    [junit] Running org.apache.hadoop.hbase.TestInfoServers
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 15.607 sec
    [junit] Running org.apache.hadoop.hbase.TestMasterAdmin
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 24.418 sec
    [junit] Running org.apache.hadoop.hbase.TestMergeMeta
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 42.511 sec
    [junit] Running org.apache.hadoop.hbase.TestMergeTable
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 29.667 sec
    [junit] Running org.apache.hadoop.hbase.TestRegionRebalancing
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 58.732 sec
    [junit] Running org.apache.hadoop.hbase.TestScanMultipleVersions
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 19.332 sec
    [junit] Running org.apache.hadoop.hbase.TestScannerAPI
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 19.325 sec
    [junit] Running org.apache.hadoop.hbase.TestSerialization
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 0.311 sec
    [junit] Running org.apache.hadoop.hbase.TestTable
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 406.798 sec
    [junit] Running org.apache.hadoop.hbase.TestToString
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.046 sec
    [junit] Running org.apache.hadoop.hbase.client.TestBatchUpdate
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 385.186 sec
    [junit] Running org.apache.hadoop.hbase.client.TestHTable
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 74.031 sec
    [junit] Running org.apache.hadoop.hbase.client.TestListTables
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 28.258 sec
    [junit] Running org.apache.hadoop.hbase.client.TestScannerTimes
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 21.383 sec
    [junit] Running org.apache.hadoop.hbase.client.TestTimestamp
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 19.354 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestInclusiveStopRowFilter
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.01 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestPageRowFilter
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.02 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestRegExpRowFilter
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 0.025 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestRowFilterAfterWrite
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 24.204 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestRowFilterOnMultipleFamilies
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 19.355 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestRowFilterSet
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.092 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestStopRowFilter
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.008 sec
    [junit] Running org.apache.hadoop.hbase.filter.TestWhileMatchRowFilter
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.013 sec
    [junit] Running org.apache.hadoop.hbase.io.TestHbaseObjectWritable
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.094 sec
    [junit] Running org.apache.hadoop.hbase.mapred.TestTableIndex
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 37.092 sec
    [junit] Running org.apache.hadoop.hbase.mapred.TestTableMapReduce
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 41.634 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestBloomFilters
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 29.346 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestCompaction
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 12.23 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestDeleteAll
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 5.149 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestDeleteFamily
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 4.8 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestGet
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 11.252 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestGet2
    [junit] Tests run: 7, Failures: 0, Errors: 0, Time elapsed: 28.702 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestHLog
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 15.949 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestHMemcache
    [junit] Tests run: 8, Failures: 0, Errors: 0, Time elapsed: 0.321 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestHRegion
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 12.309 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestHRegionInfo
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.122 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestHStoreFile
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 17.201 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestLogRolling
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 58.354 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestScanner
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 5.271 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestSplit
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 30.478 sec
    [junit] Running org.apache.hadoop.hbase.regionserver.TestTimestamp
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 27.111 sec
    [junit] Running org.apache.hadoop.hbase.util.TestBase64
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.067 sec
    [junit] Running org.apache.hadoop.hbase.util.TestKeying
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.007 sec
    [junit] Running org.apache.hadoop.hbase.util.TestMergeTool
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 19.225 sec
    [junit] Running org.apache.hadoop.hbase.util.TestMigrate
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 33.421 sec
    [junit] Running org.apache.hadoop.hbase.util.TestRootPath
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.014 sec
    [junit] Running org.onelab.test.TestFilter
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.031 sec

BUILD SUCCESSFUL
Total time: 28 minutes 44 seconds
;;;","04/Aug/08 17:27;stack;Thanks Andrew for the patch (and Dru for figuring the bug).  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowCount doesn't work,HBASE-791,12401451,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,jdcryans,jdcryans,01/Aug/08 01:05,20/Sep/12 22:10,01/Jul/25 07:49,01/Aug/08 02:39,0.2.0,,,,,0.2.0,,,,,,,0,"From Yair Even-Zohar
{quote}
looked at the code in the 0.2.0 and the args[0] is used twice
   c.set(""hbase.master"", args[0]);
And

   // First arg is the output directory.

   c.setOutputPath(new Path(args[0]));

Was anybody able to use this class?
{quote}

In fact it does not work and there is also a NPE that gets thrown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/08 01:07;jdcryans;hbase-791-v1.patch;https://issues.apache.org/jira/secure/attachment/12387321/hbase-791-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25401,,,,,Fri Aug 01 02:39:59 UTC 2008,,,,,,,,,,"0|i0h9gv:",98796,,,,,,,,,,,,,,,,,,,,,"01/Aug/08 01:07;jdcryans;Fixes said problems. Please review unless I get my credentials soon.;;;","01/Aug/08 02:39;stack;Committed.  Thanks for the patch J-D.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"During import, single region blocks requests for >10 minutes, thread dumps, throws out pending requests, and continues",HBASE-790,12401450,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,apurtell,streamy,streamy,01/Aug/08 00:38,22/Aug/08 21:13,01/Jul/25 07:49,05/Aug/08 19:06,0.2.0,,,,,0.2.0,,,regionserver,,,,0,"During a batch import, I have two processes importing into a single region.

The behavior I saw was a regionserver with 2 regions of the table in question on it.  The first region split, and the new regions were reassigned to another regionserver.

Following that, inserting into the region that was left over began to block client requests.  I am attaching the regionserver log; below is the specific problem area:

2008-07-31 15:38:24,190 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cache hit in table locations for row <> and tableName .META.: location server 72.34.249.217:60020, location region name .META.,,1
2008-07-31 15:38:24,194 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: region split, META updated, and report to master all successful. Old region=REGION => {NAME => 'items,01beddd6-813b-4f2b-ac48-a0cef395cb7e,12175434512
2008-07-31 15:38:34,052 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 7 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: Memcache size 64.0m is >= than blocking
2008-07-31 15:39:00,270 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 60020, call batchUpdate([B@17b4239f, row => 02c241b4-9d32-452d-8dab-247f4af693eb, {column => content:title, value => '...', column => content:content, va
org.apache.hadoop.hbase.NotServingRegionException: items,01beddd6-813b-4f2b-ac48-a0cef395cb7e,1217543451296
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:1436)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1147)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)
2008-07-31 15:39:09,547 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 8 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: Memcache size 64.0m is >= than blocking
2008-07-31 15:39:44,079 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 9 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: Memcache size 64.0m is >= than blocking
2008-07-31 15:40:19,574 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 1 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: Memcache size 64.0m is >= than blocking
2008-07-31 15:49:09,130 INFO org.apache.hadoop.hbase.regionserver.LogRoller: Rolling hlog. Number of entries: 1
2008-07-31 15:49:09,144 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Closing current log writer /hbase/log_72.34.249.212_1217535541159_60020/hlog.dat.1217543884691
2008-07-31 15:49:09,146 INFO org.apache.hadoop.hbase.regionserver.HLog: New log writer created at /hbase/log_72.34.249.212_1217535541159_60020/hlog.dat.1217544549145
2008-07-31 16:03:09,060 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296. Current region memcache size 64.0m
2008-07-31 16:03:09,467 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296 'IPC Server handler 5 on 60020'
2008-07-31 16:03:09,478 INFO org.apache.hadoop.ipc.Server: Process Thread Dump: Discarding call batchUpdate([B@4e727e0e, row => c08408b4-b68c-433c-ba3f-d46d3ba73288, {column => content:title, value => '...', column => content:content, v


As you can see there was a 14 minute delay between updates being blocked, and the unblocking occurring.

All the pending batchUpdates were thrown out (too old) and then importing proceeded normally.

The same behavior repeated itself later on a different regionserver, and again after a while it unfroze, kicked out pending updates, and continued.","11 node cluster.  1 master w/ namenodes and hmaster.  10 slaves w/ datanodes and regionservers.  All are 2GHz quad core xeons, 4gb ram, raid 0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/08 00:10;stack;790-v2.patch;https://issues.apache.org/jira/secure/attachment/12387382/790-v2.patch","05/Aug/08 17:43;streamy;790-v3.patch;https://issues.apache.org/jira/secure/attachment/12387579/790-v3.patch","01/Aug/08 20:07;stack;790.patch;https://issues.apache.org/jira/secure/attachment/12387368/790.patch","01/Aug/08 00:39;streamy;regionserver-lockup.log;https://issues.apache.org/jira/secure/attachment/12387320/regionserver-lockup.log",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25400,,,,,Tue Aug 05 19:06:05 UTC 2008,,,,,,,,,,"0|i0h9gn:",98795,,,,,,,,,,,,,,,,,,,,,"01/Aug/08 00:39;streamy;Full regionserver log;;;","01/Aug/08 02:28;stack;Rong-en has been running a version hbase from around the same vintage as jgray's above and also sees the 20 minute blocks.;;;","01/Aug/08 20:07;stack;If we're going to block, make sure a flush has been requested.  Also adding logging of flush request.  The log has us just sitting around doing nothing; don't know if a flush request has come in.;;;","01/Aug/08 21:02;stack;Committed the 790 above to see if it helps with this issue (Trying to reproduce).  Also adds logging.;;;","02/Aug/08 00:10;stack;Patch for jon gray to test.

Usually block/unblock runs fine but then it hangs.  Logging added by earlier patch shows no flush has been queued.  Our mechanism for queuing flushes and subsequent clear of flush flag is faulty.  This patch is a bit of a hack that does check before we go into HRegion synchronized method.;;;","02/Aug/08 07:32;rafan;In my env, I have multiplier of 2 (HBASE-779) and see this blocking.
;;;","02/Aug/08 08:03;rafan;hmm.. look at the original description again. Not sure if I'm seeing the same issue as I did not see ""Discarding update in my region server logs"".

What I saw is that during the bulk (MR or not), something the client just sits idle, and all region servers are also idle. thread dump shows client is waiting in the locationRegionInMeta method. After 10~20 mins later, the client continues without any exceptions. For MR load, I have to increase the task.timeout from 10m to 30m to prevent task tracker to kill the tasks...;;;","02/Aug/08 17:43;stack;Just tried reproducing the 'hanging blocking' on cluster here.  Tried various loading combinations -- many column families, loading all of them concurrently and then just some -- but to no avail.

Jon: Want to retry with HBASE-779 enabled?  With the edit to your hbase-default.xml?   In other words, raw TRUNK.  If it still hangs, try with the above v2 patch?  It adds extra signalling flush is needed just before we go into sync block.;;;","03/Aug/08 04:15;clint.morgan;+1 I had an upload task that was timing out on trunk, and this latest patch fixed it. ;;;","04/Aug/08 20:06;jimk;HBASE-790 is the only blocker for hbase-0.2.0 at this point;;;","04/Aug/08 20:09;streamy;-1 on trunk.  Got same behavior with latest trunk, applying 790 patch v2 and testing again right now;;;","04/Aug/08 22:14;stack;Looks like patch don't work.

Looking in Jon's logs with the patch applied I see this when we hang up on items,06fcb4d2-b751-4584-8278-3f65f923e1f0,1217884873435:

{code}
2008-08-04 14:22:31,828 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region items,06fcb4d2-b751-4584-8278-3f65f923e1f0,1217884873435. Current region memcache size 64.0m
2008-08-04 14:22:31,828 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on items,06fcb4d2-b751-4584-8278-3f65f923e1f0,1217884873435
2008-08-04 14:22:34,447 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region items,06fcb4d2-b751-4584-8278-3f65f923e1f0,1217884873435 in 2619ms, sequence id=40529298, compaction requested=false
{code}

Last week we added the logging of when flushes are registered.

Above see how we have 'Flush requested on...' in the midst of a flush start/finish.

My guess is that some sequence of registering flushes and clearing flush flags is making it so we go into a block without a flush being registered.;;;","04/Aug/08 23:25;streamy;-1 on patch v2... same behavior as before;;;","04/Aug/08 23:57;clint.morgan;Disregard my previous vote. My problems were related to another issue. Sorry for the noise.;;;","05/Aug/08 17:43;streamy;After spending quite a bit of time running different load tests on v2 patch, it appears the problem did still exist.

Stack pointed me towards looking into the possibility of distributed state.  I dug in and it seemed that state was being spread across two different locations.

To fix this, I moved the flushRequested boolean into the WriteState class.  This allowed me to make it so its state can only be changed when WriteState was synchronized on.

Upon further testing it seems that this does work and I'm no longer seeing this issue.

I am +1 on 790 with this patch, and +1 on rolling 0.2.0 RC2;;;","05/Aug/08 17:55;stack;Patch looks good.  Does it really fix the issue?  Why'd you remove '-        r.setLastFlushTime(now);' in Flusher?

Let me try this patch up on our cluster.;;;","05/Aug/08 17:57;jimk;Reviewed patch. +1
;;;","05/Aug/08 17:58;apurtell;I'm also testing this right now.;;;","05/Aug/08 18:00;jimk;Wait. Stack's right. Without setting the last flush time, the optional cache flush won't work.;;;","05/Aug/08 18:09;jimk;Ah, ok. lastFlushTime is set in internalFlushCache so is not needed in Flusher. +1;;;","05/Aug/08 18:10;streamy;The patch does appear to fix the issue.  I have run an identical load test probably 15 times in the past week and last night it ran past 400 total regions without any issues.  Previously it had been breaking at either 20 or 80 total regions.

I will continue testing and also run a full test on production once we get the RC.

I removed the setLastFlushTime in Flusher because it didn't make sense to be doing it twice.;;;","05/Aug/08 18:32;apurtell;+1;;;","05/Aug/08 19:03;stack;I ran a little test on cluster.  Ran slightly faster with patch applied.  Going to commit it.
;;;","05/Aug/08 19:06;stack;Committed.  Thanks for patch Jon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Div by zero in Master.jsp,HBASE-788,12401368,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,clint.morgan,clint.morgan,30/Jul/08 23:17,22/Aug/08 21:13,01/Jul/25 07:49,12/Aug/08 20:34,0.2.0,,,,,0.2.0,,,,,,,0,"When loading the main info page, here is the stack:

java.lang.ArithmeticException: / by zero
	at org.apache.hadoop.hbase.generated.master.master_jsp._jspService(master_jsp.java:151)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:111)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:491)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:367)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:185)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:689)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:391)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:146)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)
	at org.mortbay.jetty.Server.handle(Server.java:285)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:457)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:751)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:500)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:209)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:357)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:329)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:475)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/08 23:27;clint.morgan;hbase-778.patch;https://issues.apache.org/jira/secure/attachment/12387242/hbase-778.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25399,,,,,Tue Aug 12 20:34:36 UTC 2008,,,,,,,,,,"0|i0d447:",74443,,,,,,,,,,,,,,,,,,,,,"30/Jul/08 23:21;clint.morgan;I had a low msginterval (100) , and it looks like this is what caused it (interval is converted to secs, and so got rounded to zero);;;","30/Jul/08 23:27;clint.morgan;This (naive approach) fixed it for me.;;;","31/Jul/08 22:33;jimk;Reviewed patch. +1;;;","12/Aug/08 20:34;stack;Closing.  Was committed to 0.2.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The DELETE key in the hbase shell deletes the wrong character,HBASE-782,12401178,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,tim_s,srainville,srainville,28/Jul/08 18:23,28/Dec/09 19:24,01/Jul/25 07:49,22/Dec/08 17:50,0.19.0,,,,,0.19.0,,,Client,,,,0,"Within the hbase shell, if you type ""monkey"", then place the cursor on the letter 'k' and hit the key 'DELETE' several times it will do this:
monkey
mokey
mkey
key
key
key
...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/08 17:45;tim_s;782.patch;https://issues.apache.org/jira/secure/attachment/12396612/782.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25398,,,,,Mon Dec 22 17:50:20 UTC 2008,,,,,,,,,,"0|i0h9f3:",98788,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 20:37;stack;Whats it supposed to do Sebastien?  Delete k, then e, then y? (Seems to work properly to me).;;;","12/Aug/08 22:40;srainville;yes it's supposed to delete k, then e, then y... and no it's not working for me.

OS: Ubuntu 8.04

It's the only place I see that behaviour... never seen it before in any program...
;;;","22/Dec/08 17:43;tim_s;This happens to me too and finally annoyed me enough to look at it.
It's due to the suppressing 'nil' output incorrectly (or too optimistically?)

Attached a wonderfully simple patch. Should also work for all versions since hirb.rb was included.;;;","22/Dec/08 17:50;stack;Thanks for the patch Tim.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master not reassigning .META. from failed/failing regionserver,HBASE-776,12401055,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,apurtell,apurtell,25/Jul/08 21:02,22/Aug/08 21:13,01/Jul/25 07:49,05/Aug/08 19:07,0.2.0,,,,,0.2.0,,,master,,,,0,"In our environment sometimes the regionserver carrying META is also assigned to the 'content' table, into which objects retrieved from Internet crawling is stored. For unclear reason the regionserver occasionally goes ""deaf"" (seperate issue) and when this happens META is no longer available. The master then never reassigns META, so the whole cluster is down from this point and does not recover. Logs attached.
","CentOS x86_64, JDK 1.6, Hadoop 0.17.1, HBase 0.2.0, r679585, Fri Jul 25 16:47:26 UTC 2008",,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/08 21:02;apurtell;hbase-hadoop-master-sjdc-atr-dc-1.log;https://issues.apache.org/jira/secure/attachment/12386922/hbase-hadoop-master-sjdc-atr-dc-1.log","25/Jul/08 21:03;apurtell;hbase-hadoop-regionserver-sjdc-atr-dc-13.log;https://issues.apache.org/jira/secure/attachment/12386923/hbase-hadoop-regionserver-sjdc-atr-dc-13.log","25/Jul/08 21:03;apurtell;master_gui.png;https://issues.apache.org/jira/secure/attachment/12386924/master_gui.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25394,,,,,Tue Aug 05 19:07:04 UTC 2008,,,,,,,,,,"0|i0h9dr:",98782,,,,,,,,,,,,,,,,,,,,,"25/Jul/08 21:07;apurtell;There is no indication in the regionserver log why RPC becomes unresponsive. 
;;;","25/Jul/08 21:09;apurtell;During the duration of this incident the regionserver in question was being pounded while the rest of the cluster was idle.  Wouldn't it make sense to migrate META from a heavily loaded regionserver if another is substantially less loaded in general? ;;;","26/Jul/08 02:30;apurtell;Is 200-500 requests for META per second is normal? The client code is reusing the same HTable object and is just using table.commit(BatchUpdate) to insert data imported from a Postgres DB. ;;;","26/Jul/08 02:35;apurtell;Just saw a period where a new steady state was reached with 1800-2000 requests for META per second. ;;;","26/Jul/08 05:13;stack;Andrew, can you run with DEBUG level logging -- we might get a clue why it goes 'deaf' -- and can you up your ulimit for file descriptors from 1024 (because out-of-FDs can manifest in weird ways)?  See FAQ for how.

A socket timeout when scanning meta should provoke relocation of meta I'd say.  Need to dig more.  From your logs, it happens over and over and the master just sits there dumb.

Regards pounding META, you'd think the META info cached.  Let me do loading here and log the META region accesses.  How many clients you running?;;;","26/Jul/08 06:12;apurtell;Ok, I'll up ulimit to 10K for the hadoop account and see what happens. Was already planning to sit down and try to produce a simple program that reproduces the problem reliably. If I can get something consistent I'll run in DEBUG and if it still triggers I'll upload the logs. Maybe I'll find the client is doing something dumb. However as I can see it is just one client with two threads doing nothing particularly odd or special. 
The cluster configuration is 15 nodes:
    1 master/regionserver/namenode/datanode/jobtracker
    14 regionserver/datanode/tasktracker
Each node is dual processor 4 core Xeons with 8GB RAM. 
The master node does not load up. 
Client is external to the cluster. ;;;","26/Jul/08 19:44;stack;This exception of yours Andrew seems pretty easy to manufacture.  I see it here in a little test I'm running.  Kept getting the exception over and over for 30 mins now.

{code}
2008-07-26 19:09:22,111 WARN org.apache.hadoop.hbase.master.BaseScanner: Scan one META region: {regionname: .META.,,1, startKey: <>, server: XX.XX.XX.XX:60020}
java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:559)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:230)
        at $Proxy2.openScanner(Unknown Source)
        at org.apache.hadoop.hbase.master.BaseScanner.scanRegion(BaseScanner.java:159)
        at org.apache.hadoop.hbase.master.MetaScanner.scanOneMetaRegion(MetaScanner.java:69)
        at org.apache.hadoop.hbase.master.MetaScanner.maintenanceScan(MetaScanner.java:124)
        at org.apache.hadoop.hbase.master.BaseScanner.chore(BaseScanner.java:139)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:63)
{code};;;","26/Jul/08 20:58;stack;Its looking like this is symptom of HBASE-751;;;","28/Jul/08 07:20;apurtell;The excessive queries to META was a client issue. Sorry for the noise. There was an existence test buried in some old code which created HBaseAdmin objects. It did apparently serve as a torture test, though. ;;;","05/Aug/08 19:07;apurtell;HBASE-751 is supposed to contain fix. Will reopen if seen again. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Migration] This message 'java.io.IOException: Install 0.1.x of hbase and run its migration first' is useless,HBASE-768,12400862,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,irubin,stack,stack,23/Jul/08 18:47,13/Sep/08 23:22,01/Jul/25 07:49,28/Aug/08 22:10,0.2.0,,,,,0.18.0,0.2.1,,,,,,0,You'll see above message after you've committed to a new version of hadoop.  You won't be able to go back.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/08 18:32;jdcryans;hbase-768.patch;https://issues.apache.org/jira/secure/attachment/12389111/hbase-768.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25389,,,,,Thu Aug 28 22:10:27 UTC 2008,,,,,,,,,,"0|i0h9bz:",98774,,,,,,,,,,,,,,,,,,,,,"28/Aug/08 17:40;jimk;Not a blocker. Moving to next releases;;;","28/Aug/08 18:32;jdcryans;Please review. Thanks to Izaak for discussion.;;;","28/Aug/08 22:10;jimk;Committed to branch and trunk. Thanks for the patch JD!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The name of column request has padding zero using REST interface,HBASE-764,12400794,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,sishen,sishen,23/Jul/08 06:13,22/Aug/08 21:13,01/Jul/25 07:49,23/Jul/08 17:52,0.2.0,,,,,0.2.0,,,REST,,,,0,"Today when i play with the REST interface and found the column POST/PUT/GET has a problem.
When i use the hbase shell to check the data, i found the row name has the padding zero.

The cause is that TableHandler use Text class to encode the string to the UTF-8. But CharSetEncoder
will pre-allocate more spaces then the length of String for performance. So we get the padding zero
when inserting the value to the table.  The fix is to get the String instead of the byte[] for the BatchUpdate.

Below is the patch.  Also, the patch includes fixing the wrong use of (bytes[]).toString() using Bytes.toString(byte[])


Index: src/java/org/apache/hadoop/hbase/rest/TableHandler.java
===================================================================
--- src/java/org/apache/hadoop/hbase/rest/TableHandler.java    (revision 678664)
+++ src/java/org/apache/hadoop/hbase/rest/TableHandler.java    (working copy)
@@ -174,7 +174,7 @@
 
         // copy over those cells with requested column names
         for(byte [] current_column: columns_retrieved) {
-          if(requested_columns_set.contains(current_column.toString())){
+          if(requested_columns_set.contains(Bytes.toString(current_column))){
             m.put(current_column, prefiltered_result.get(current_column));           
           }
         }
@@ -295,7 +295,7 @@
    
     try{
       // start an update
-      Text key = new Text(row);
+      String key = new Text(row).toString();
       batchUpdate = timestamp == null ?
         new BatchUpdate(key) : new BatchUpdate(key, Long.parseLong(timestamp));
 
@@ -308,7 +308,7 @@
 
         // extract the name and value children
         Node name_node = column.getElementsByTagName(""name"").item(0);
-        Text name = new Text(name_node.getFirstChild().getNodeValue());
+        String name = new Text(name_node.getFirstChild().getNodeValue()).toString();
 
         Node value_node = column.getElementsByTagName(""value"").item(0);
 
@@ -356,7 +356,7 @@
           XMLOutputter outputter = getXMLOutputter(response.getWriter());
           outputter.startTag(""regions"");
           for (int i = 0; i < startKeys.length; i++) {
-            doElement(outputter, ""region"", startKeys[i].toString());
+            doElement(outputter, ""region"", Bytes.toString(startKeys[i]));
           }
           outputter.endTag();
           outputter.endDocument();
@@ -368,7 +368,7 @@
           PrintWriter out = response.getWriter();
           for (int i = 0; i < startKeys.length; i++) {
             // TODO: Add in the server location.  Is it needed?
-            out.print(startKeys[i].toString());
+            out.print(Bytes.toString(startKeys[i]));
           }
           out.close();
         break;
@@ -454,7 +454,7 @@
     // pull the row key out of the path
     String row = URLDecoder.decode(pathSegments[2], HConstants.UTF8_ENCODING);
    
-    Text key = new Text(row);
+    String key = new Text(row).toString();
 
     String[] columns = request.getParameterValues(COLUMN);
        
@@ -472,7 +472,7 @@
       } else{
         // delete each column in turn     
         for(int i = 0; i < columns.length; i++){
-          table.deleteAll(key, new Text(columns[i]));
+          table.deleteAll(key, new Text(columns[i]).toString());
         }
       }
       response.setStatus(202);","Debian GNU/Linux,  Java5",1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,"23/Jul/08 17:22;stack;rest_column-v2.patch;https://issues.apache.org/jira/secure/attachment/12386736/rest_column-v2.patch","23/Jul/08 06:16;sishen;rest_column.patch;https://issues.apache.org/jira/secure/attachment/12386687/rest_column.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25386,,,,,Wed Jul 23 17:52:29 UTC 2008,,,,,,,,,,"0|i0h9b3:",98770,,,,,,,,,,,,,,,,,,,,,"23/Jul/08 17:22;stack;Sishen:

I edited your patch to remove going String -> Text -> toString.  Please review/try it.  If its OK with you, +1 it and I'll commit it.

I'd like to get these REST fixes into the 0.2.0 second release candidate if possible.   Thanks.;;;","23/Jul/08 17:40;sishen;Hi, stack.

It works for me. Actually it's was my first version of patch. 
I just don't want why the author using the Text instead of String directly. Maybe his reason is to use UTF8 string in the hbase. So i use String -> Text -> toString.  Is UTF8 the default charset?  

I'd like to see the REST interface work with HBase. It's better for the integration with other languages. Thanks for all your hard work.;;;","23/Jul/08 17:52;stack;Committed.  Thanks for the patch Sishen.

I think the Text stuff is just left-over from the days when hbase was all Text (now its all byte arrays).  Regards encoding, when you pass a String into BatchUpdate, it'll do getBytes(""UTF-8"") on the passed String so yes, default is UTF-8 everything.  If you want to do another encoding, you could manage the String.getBytes yourself passing an alternate encoding.

Keep banging on the REST.  There may be more lurkers in there yet, stuff that hasn't been updated to match the new all-byte-arrays-all-the-time regime.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException from RowResult.get(String),HBASE-763,12400785,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,apurtell,apurtell,23/Jul/08 01:33,22/Aug/08 21:13,01/Jul/25 07:49,23/Jul/08 15:06,0.2.0,,,,,0.2.0,,,,,,,0,"[hadoop@sjdc-atr-dns column-test]$ hbase net.iridiant.simpletest.Main --master=10.30.94.1:60000
Exception in thread ""main"" java.lang.ClassCastException: java.lang.String cannot be cast to [B
        at org.apache.hadoop.hbase.util.Bytes$1.compare(Bytes.java:32)
        at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351)
        at java.util.TreeMap.getEntry(TreeMap.java:322)
        at java.util.TreeMap.get(TreeMap.java:255)
        at org.apache.hadoop.hbase.io.HbaseMapWritable.get(HbaseMapWritable.java:112)
        at org.apache.hadoop.hbase.io.RowResult.get(RowResult.java:79)
        at net.iridiant.simpletest.Main.main(Unknown Source)

Please see attached testcase.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/08 02:00;apurtell;RowResult.patch;https://issues.apache.org/jira/secure/attachment/12386680/RowResult.patch","23/Jul/08 01:35;apurtell;simpletest.java;https://issues.apache.org/jira/secure/attachment/12386678/simpletest.java",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25385,,,,,Wed Jul 23 15:06:03 UTC 2008,,,,,,,,,,"0|i0h9av:",98769,,,,,,,,,,,,,,,,,,,,,"23/Jul/08 02:00;apurtell;I know the get(Object) method is required by the Map interface, but a HBaseMapWritable is not a generic map. I think at least a convenience method for RowResult.get(String) should go into 0.2.0 because it represents a pretty big WTF moment for a user trying to do something basic with the client API, since there are other convenience methods that take String keys. I know I had one. 

;;;","23/Jul/08 15:06;stack;Committed.  Thanks for the patch Andrew.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"deleteFamily takes timestamp, should only take row and family.  Javadoc describes both cases but only implements the timestamp case.",HBASE-762,12400775,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,irubin,streamy,streamy,22/Jul/08 21:52,13/Sep/08 23:22,01/Jul/25 07:49,28/Aug/08 21:57,0.2.0,0.2.1,,,,0.18.0,0.2.1,,Client,IPC/RPC,regionserver,,0,"The three version of deleteFamily in client.HTable (Text, String, byte[]) have varying descriptions about whether they take timestamps or not.

public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family, long timestamp) throws IOException

    Delete all cells for a row with matching column family at all timestamps. 

public void deleteFamily(String row, String family, long timestamp) throws IOException

    Delete all cells for a row with matching column family at all timestamps. 

public void deleteFamily(byte[] row, byte[] family, long timestamp) throws IOException

    Delete all cells for a row with matching column family with timestamps less than or equal to timestamp. 


These will become:

public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family) throws IOException

    Delete all cells for a row with matching column family at all timestamps. 

public void deleteFamily(String row, String family) throws IOException

    Delete all cells for a row with matching column family at all timestamps. 

public void deleteFamily(byte[] row, byte[] family) throws IOException

    Delete all cells for a row with matching column family at all timestamps.


Per Jean-Daniel's comment, deleteAll should then not permit families.  I'm unsure whether this is currently allowed or not, but the documentation must be updated either way.

Will post patch after more thorough testing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/08 18:34;jdcryans;hbase-762.patch;https://issues.apache.org/jira/secure/attachment/12389113/hbase-762.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25384,,,,,Thu Aug 28 21:57:06 UTC 2008,,,,,,,,,,"0|i0h9an:",98768,,,,,,,,,,,,,,,,,,,,,"28/Aug/08 17:43;jimk;Not a blocker for 0.2.1. Moving to 0.2.2 and 0.19.0;;;","28/Aug/08 18:34;jdcryans;Please review. Thanks to Izaak for discussing.;;;","28/Aug/08 21:57;jimk;Committed to branch and trunk. Thanks for the patch JD!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throwing IOE read-only when should be throwing NSRE,HBASE-758,12400696,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,21/Jul/08 21:30,22/Aug/08 21:13,01/Jul/25 07:49,21/Jul/08 21:35,0.2.0,,,,,0.2.0,,,,,,,0,"Am seeing exceptions like the following during 'normal' operation though the region has not been explicitly set to be read-only (new feature added with commit of HBASE-62).

{code}
2008-07-21 20:50:25,071 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 60020, call batchUpdate([B@63443c, row => 0000791906, {column => info:data, value => '...'}) from XX.XX.XX.139:59778: error: java.io.IOException: region is read only
java.io.IOException: region is read only
        at org.apache.hadoop.hbase.regionserver.HRegion.batchUpdate(HRegion.java:1322)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdate(HRegionServer.java:1151)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/08 21:31;stack;readonly.patch;https://issues.apache.org/jira/secure/attachment/12386574/readonly.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25380,,,,,Mon Jul 21 21:35:08 UTC 2008,,,,,,,,,,"0|i0h99r:",98764,,,,,,,,,,,,,,,,,,,,,"21/Jul/08 21:35;stack;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In HBase shell, the put command doesn't process the timestamp",HBASE-756,12400664,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,jdcryans,jdcryans,21/Jul/08 15:24,22/Aug/08 21:13,01/Jul/25 07:49,21/Jul/08 15:49,0.2.0,,,,,0.2.0,,,scripts,,,,0,"{code}
      if timestamp
        bu = BatchUpdate.new(row)
      else
        bu = BatchUpdate.new(row)
{code}

Something is wrong here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/08 15:26;jdcryans;hbase-756.patch;https://issues.apache.org/jira/secure/attachment/12386533/hbase-756.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25378,,,,,Mon Jul 21 15:49:01 UTC 2008,,,,,,,,,,"0|i0h99b:",98762,,,,,,,,,,,,,,,,,,,,,"21/Jul/08 15:26;jdcryans;Review please.;;;","21/Jul/08 15:49;stack;Committed.  Thanks for the patch J-D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The JRuby shell documentation is wrong in ""get"" and ""put""",HBASE-754,12400579,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,jdcryans,jdcryans,18/Jul/08 19:23,22/Aug/08 21:13,01/Jul/25 07:49,19/Jul/08 22:26,0.2.0,,,,,0.2.0,,,scripts,,,,0,"In the shell documentation we can read: 
{code}
hbase> get 't1', 'r1', {TIMESTAMP => ts1, VERSIONS => 4}
{code}
when in fact there are no facility for this. It will work only because it uses getRow(row, ts).

Also
{code}
hbase> put 't1', 'r1', 'c1', ts1
{code}
does not work because the 'value' is missing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/08 19:24;jdcryans;hbase-754.patch;https://issues.apache.org/jira/secure/attachment/12386427/hbase-754.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25377,,,,,Sat Jul 19 22:26:01 UTC 2008,,,,,,,,,,"0|i0h98v:",98760,,,,,,,,,,,,,,,,,,,,,"18/Jul/08 19:24;jdcryans;Review please.;;;","19/Jul/08 22:26;stack;Committed.  Thanks for the patch J-D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dfs exception and regionserver stuck during heavy write load,HBASE-751,12400518,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,rafan,rafan,18/Jul/08 02:07,22/Aug/08 21:13,01/Jul/25 07:49,01/Aug/08 06:40,0.2.0,,,,,0.2.0,,,regionserver,,,,0,"It's a 3 node setup, each runs datanode and regionserver. One runs as hbase master and hadoop namenode.

After some heavy write load via java client, the client is stuck. Stack trace on the regionserver shows:

""IPC Server handler 46 on 60020"" daemon prio=10 tid=0x4dd3f000 nid=0x4eb3 waiting for monitor entry [0x4cc82000..0x4cc83130]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 43 on 60020"" daemon prio=10 tid=0x4dd3bc00 nid=0x4eb0 waiting for monitor entry [0x4cd75000..0x4cd75fb0]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 40 on 60020"" daemon prio=10 tid=0x4dd38400 nid=0x4ead runnable [0x4ce68000..0x4ce68e30]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked <0x6a557580> (a sun.nio.ch.Util$1)
    - locked <0x6a557570> (a java.util.Collections$UnmodifiableSet)
    - locked <0x5cdcec18> (a sun.nio.ch.EPollSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:237)
    at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:155)
    at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:149)
    at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:122)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
    - locked <0x552ffb60> (a java.io.BufferedInputStream)
    at java.io.DataInputStream.readInt(DataInputStream.java:370)
    at org.apache.hadoop.dfs.DFSClient$BlockReader.readChunk(DFSClient.java:928)
    - locked <0x55300f78> (a org.apache.hadoop.dfs.DFSClient$BlockReader)
    at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:236)
    at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:178)
    at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:195)
    at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)
    - locked <0x55300f78> (a org.apache.hadoop.dfs.DFSClient$BlockReader)
    at org.apache.hadoop.dfs.DFSClient$BlockReader.read(DFSClient.java:823)
    - locked <0x55300f78> (a org.apache.hadoop.dfs.DFSClient$BlockReader)
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1352)
    - locked <0x59a70e40> (a org.apache.hadoop.dfs.DFSClient$DFSInputStream)
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1388)
    - locked <0x59a70e40> (a org.apache.hadoop.dfs.DFSClient$DFSInputStream)
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1337)
    - locked <0x59a70e40> (a org.apache.hadoop.dfs.DFSClient$DFSInputStream)
    at java.io.DataInputStream.readInt(DataInputStream.java:370)
    at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1847)
    - locked <0x651f77b0> (a org.apache.hadoop.io.SequenceFile$Reader)
    at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1877)
    - locked <0x651f77b0> (a org.apache.hadoop.io.SequenceFile$Reader)
    at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1782)
    - locked <0x651f77b0> (a org.apache.hadoop.io.SequenceFile$Reader)
    at org.apache.hadoop.io.MapFile$Reader.seekInternal(MapFile.java:476)
    - locked <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.io.MapFile$Reader.getClosest(MapFile.java:558)
    - locked <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.rowKeyFromMapFileEmptyKeys(HStore.java:1463)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1434)
    - locked <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 38 on 60020"" daemon prio=10 tid=0x4dd36000 nid=0x4eab waiting for monitor entry [0x4cf0a000..0x4cf0b130]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 37 on 60020"" daemon prio=10 tid=0x4dd35000 nid=0x4eaa waiting for monitor entry [0x4cf5b000..0x4cf5c0b0]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 35 on 60020"" daemon prio=10 tid=0x4dd32c00 nid=0x4ea8 waiting for monitor entry [0x4cffd000..0x4cffdfb0]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 30 on 60020"" daemon prio=10 tid=0x4dd2d400 nid=0x4ea3 waiting for monitor entry [0x4d192000..0x4d193130]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 29 on 60020"" daemon prio=10 tid=0x4dd2c000 nid=0x4ea2 waiting for monitor entry [0x4d1e3000..0x4d1e40b0]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 26 on 60020"" daemon prio=10 tid=0x4dd29800 nid=0x4e9f waiting for monitor entry [0x4d2d6000..0x4d2d6f30]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 17 on 60020"" daemon prio=10 tid=0x4dd1f800 nid=0x4e96 waiting for monitor entry [0x4d5af000..0x4d5afeb0]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 14 on 60020"" daemon prio=10 tid=0x4dd1c400 nid=0x4e93 waiting for monitor entry [0x4d6a2000..0x4d6a3130]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 10 on 60020"" daemon prio=10 tid=0x4dd17c00 nid=0x4e8f waiting for monitor entry [0x4d7e6000..0x4d7e6f30]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 7 on 60020"" daemon prio=10 tid=0x4dd14800 nid=0x4e8c waiting for monitor entry [0x4d8d9000..0x4d8da1b0]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

""IPC Server handler 0 on 60020"" daemon prio=10 tid=0x4e2c0c00 nid=0x4e85 waiting for monitor entry [0x4db10000..0x4db10e30]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1424)
    - waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1399)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1210)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:438)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

in regionserver log, I see the following right before the client stuck (there are few other similar logs, but the client keeps going at those time points):

2008-07-17 22:31:49,404 INFO org.apache.hadoop.hbase.regionserver.HRegion: region aaa,bbb,1216304670433/1145836031 available
2008-07-17 22:31:49,404 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting compaction on region aaa,bbb,1216304670433
2008-07-17 22:32:07,653 WARN org.apache.hadoop.hbase.regionserver.HStore: Exception closing reader for 1145836031/ccc
java.io.IOException: Stream closed
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.close(DFSClient.java:1319)
    at java.io.FilterInputStream.close(FilterInputStream.java:155)
    at org.apache.hadoop.io.SequenceFile$Reader.close(SequenceFile.java:1581)
    at org.apache.hadoop.io.MapFile$Reader.close(MapFile.java:577)
    at org.apache.hadoop.hbase.regionserver.HStore.closeCompactionReaders(HStore.java:917)
    at org.apache.hadoop.hbase.regionserver.HStore.compactHStoreFiles(HStore.java:910)
    at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:787)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:887)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:847)
    at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:84)
(and two of the same exception, since I have 3 HStoreFIle to compact)
2008-07-17 22:32:07,912 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region aaa,bbb,1216304670433 in 18sec
[after this point, I only see regionserver rotates HLog, no other activities)

At 22:32, no suspicious log in datanode, but 8mins later, I see this

2008-07-17 22:40:07,928 WARN org.apache.hadoop.dfs.DataNode: 192.168.1.5650010:Got exception while serving blk_-38731635936101350 to /192.168.1.56
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/192.168.1.56:50010 remote=/192.168.1.56:40691]
    at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:170)
    at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:144)
    at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:105)
    at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
    at java.io.DataOutputStream.write(DataOutputStream.java:90)
    at org.apache.hadoop.dfs.DataNode$BlockSender.sendChunks(DataNode.java:1784)
    at org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1840)
    at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1055)
    at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:984)
    at java.lang.Thread.run(Thread.java:619)

for this particular block in question, I found around the region available time:

2008-07-17 22:31:49,642 INFO org.apache.hadoop.dfs.DataNode: Receiving block blk_-38731635936101350 src: /192.168.1.56:37878 dest: /192.168.1.56:50010
2008-07-17 22:31:56,856 INFO org.apache.hadoop.dfs.DataNode: Received block blk_-38731635936101350 of size 67108864 from /192.168.1.56
2008-07-17 22:31:56,857 INFO org.apache.hadoop.dfs.DataNode: PacketResponder 1 for block blk_-38731635936101350 terminating

And after the hbase client stuck, I found one datanode keeps sending the *same* block to the regionserver, which is blocked as shown above.

=====

For the record, I did not see this ""Stream closed"" error on another small 4-node cluster with trunk r675659 (same hadoop version with the 3-node cluster above).

For hbase trunk r677011, I got 

java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.ServerCallable.getServerName(ServerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:886
        at org.apache.hadoop.hbase.client.HTable.commit(HTable.java:1161)

then, the region server stucks

08/07/18 05:29:29 INFO ipc.RPC: Problem connecting to server: /192.168.1.56:60020

stack dump shows similar as the above one, and I'm also seeing the dfs exception.","jdk 1.6, hadoop 0.17.2-dev (hadoop-0.17, r677779), hbase trunk r677517",,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/08 09:15;rafan;751-test.diff;https://issues.apache.org/jira/secure/attachment/12386971/751-test.diff","01/Aug/08 06:30;stack;751-v17.patch;https://issues.apache.org/jira/secure/attachment/12387327/751-v17.patch","29/Jul/08 07:57;stack;751-v2.patch;https://issues.apache.org/jira/secure/attachment/12387086/751-v2.patch","29/Jul/08 19:20;stack;751-v3.patch;https://issues.apache.org/jira/secure/attachment/12387124/751-v3.patch","30/Jul/08 05:03;stack;751-v4.txt;https://issues.apache.org/jira/secure/attachment/12387162/751-v4.txt","30/Jul/08 06:17;stack;751-v5.txt;https://issues.apache.org/jira/secure/attachment/12387165/751-v5.txt","30/Jul/08 06:25;stack;751-v6.txt;https://issues.apache.org/jira/secure/attachment/12387166/751-v6.txt","31/Jul/08 06:17;stack;751-v7.patch;https://issues.apache.org/jira/secure/attachment/12387258/751-v7.patch","31/Jul/08 19:12;stack;751-v9.patch;https://issues.apache.org/jira/secure/attachment/12387306/751-v9.patch","28/Jul/08 00:28;stack;751.patch;https://issues.apache.org/jira/secure/attachment/12387000/751.patch",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25376,,,,,Sat Aug 02 05:58:45 UTC 2008,,,,,,,,,,"0|i0h987:",98757,,,,,,,,,,,,,,,,,,,,,"18/Jul/08 04:42;rafan;For the record, I see similar backtrace in Hudson HBase-Patch #224 build:

regionserver.HStore(919): Exception closing reader for 1028785192/info
   [junit] java.io.IOException: Stream closed
   [junit]     at org.apache.hadoop.dfs.DFSClient$DFSInputStream.close(DFSClient.java:1319)
   [junit]     at java.io.FilterInputStream.close(FilterInputStream.java:155)
   [junit]     at org.apache.hadoop.io.SequenceFile$Reader.close(SequenceFile.java:1581)
   [junit]     at org.apache.hadoop.io.MapFile$Reader.close(MapFile.java:577)
   [junit]     at org.apache.hadoop.hbase.regionserver.HStore.closeCompactionReaders(HStore.java:917)
   [junit]     at org.apache.hadoop.hbase.regionserver.HStore.compactHStoreFiles(HStore.java:910)
   [junit]     at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:787)
   [junit]     at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:887)
   [junit]     at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:847)
   [junit]     at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:84)
   [junit] 2008-07-17 16:52:38,638 WARN  [RegionServer:0.compactor] 
;;;","23/Jul/08 08:33;rafan;The IOE in regionserver log is fixed by stack.

And I still see regionserver hang against latest trunk.
After ~1hr of the hang, the client shows

java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.ServerCallable.getServerName(ServerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:885)
        at org.apache.hadoop.hbase.client.HTable.commit(HTable.java:1167)

Some non-default settings:

  <property>
    <name>hbase.master.lease.period</name>
    <value>600000</value>
  </property>

  <property>
    <name>hbase.client.retries.number</name>
    <value>10</value>
  </property>





;;;","23/Jul/08 13:59;jdcryans;Can you confirm that you don't have the same kind of problem as seen in Renaud Delbru's setup? His main issue was that the nodes were swapping like there is no tomorrow.;;;","23/Jul/08 16:18;rafan;No. it's not swapping.;;;","23/Jul/08 19:10;stack;This is ugly, particularly when its in 0.17.2 hadoop.   I've seen variations on this, e.g. HBASE-634, but not this exact hang not having run big stuff on 0.17.x hadoop yet.

We're stuck doing a readInt down in DFSClient.  Looks like an HDFS issue.  Do you want to try asking over in hadoop-user Rong-en to see if you get any pointers? (I started to put together an email myself but its a bit awkward me telling your story; I think you would do it better yourself).  Pertinent I think are the block serving timeout exception that you note above.   Do you think you could reproduce with HDFS logging all on DEBUG?  That'd give us more clues as to whats going on.  Thanks Rong-en;;;","23/Jul/08 19:12;stack;Out of interest, whats your OS info?  And which particular 1.6 JDK?;;;","24/Jul/08 01:30;rafan;Ya, I started to think this a HDFS issue. I will write a mail
to core-user later.

JDK is 1.6u4, OS is RHEL 4 update 4.
;;;","24/Jul/08 17:21;stack;Made this critical.

And, looking again, the thread doing the readInt is not stuck, rather, its RUNNABLE.  But I'd guess we probably are not returning up out of this read.  Rong-en, you might try doing a couple of thread dumps in a row to see if this is so.;;;","25/Jul/08 03:30;rafan;with trunk 679236, I do couple thread dumps in a row, and it suggests that

    at org.apache.hadoop.hbase.regionserver.HStore.rowKeyFromMapFileEmptyKeys(HStore.java:1510)

is in a infinite loop as seen in successive dumps:

    at org.apache.hadoop.io.MapFile$Reader.seekInternal(MapFile.java:463)
    - locked <0x5cd28ec8> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.io.MapFile$Reader.getClosest(MapFile.java:558)
    - locked <0x5cd28ec8> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.rowKeyFromMapFileEmptyKeys(HStore.java:1510)
    at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1481)
    - locked <0x5cd28ec8> (a org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader)
    at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1446)
    at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1227)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1099)

The only difference is sometimes it is in

    at org.apache.hadoop.io.MapFile$Reader.seekInternal(MapFile.java:476)

The block that dfs keeps sending is

blk_8253555549802756519

which is 

/hbase/.META./1028785192/historian/mapfiles/6501633157510870075/data

A close look at rowKeyFromMapFileEmptyKeys() and the historian file give me some 
hints. The while loop may never exit if we can not find a candidate key (foundCandidate=false)
*and* a deleted or expired row exists (deletedOrExpiredRow != null). In this case,
we will put the searchKey as the deletedOrExpiredRow. Then do the search again.
Since the searchKey is the one that is delete or expired and this searchKey
*does* exist in the map file.So next time, 

readkey = (HStoreKey)map.getClosest(searchKey, readval, true);

gives the same readKey as before which is the row that is deleted or expired.

This scenario happens if the first searchKey  is an deleted/expired row and 
exist in the mapfile or this does no exist, but the one immediate before this
key in mapfile is delete/expired.

I examine the historian file, the first entry's value is HBASE::DELETEVAL. So if 
we are looking at the row whose HStoreKey is this entry's key. we end up
in an infinite loop. Since 

#1 searchKey is the first entry
#2 we get the readKey which is deleted
#3 the next key exceeds the row we want
#4 we set searchKey the the readKey (that are the same!)
#5 loops begins...

my 0.02 cents.;;;","26/Jul/08 21:15;stack;This is an awkward one to fix.

Chatting with Todd Lipcon, he made two suggestions:

{code}
[14:01]	<Toad>	here's an ugly idea until we can get a patch in hadoop:
[14:02]	<Toad>	class NeverEqualWritableComparable implements WritableComparable {
[14:02]	<Toad>	public NeverEqualWritableComparable(WritableComparable wrapped) {
[14:02]	<Toad>	_wrapped = wrapped;
[14:02]	<Toad>	}
[14:02]	<Toad>	public int compare(WritableComparable other) {
[14:02]	<Toad>	int res = _wrapped.compare(other);
[14:03]	<Toad>	return (res == 0) ? 1 : res;
[14:03]	<Toad>	}
[14:03]	<Toad>	}
[14:03]	<st^ack>	Where would I insert it?
[14:03]	<Toad>	then use getClosest(new NeverEqualWritableComparable(searchkey), value, true)
[14:04]	<Toad>	that way you ensure that you won't get the same one again
[14:04]	* st^ack	looking
[14:04]	<Toad>	err, it's compareTo(...) I guess, not .compare. and you need to implement write and readFields to wrap the wrapped one
[14:05]	<Toad>	hmm, should it return 1 or -1 for equality...
[14:05]	<Toad>	I think -1 actually
[14:06]	<Toad>	you want the ""just before"" search key to report that it's smaller than itself
[14:06]	<st^ack>	We've gone too far when we hit the not-wanted hsk
[14:06]	<Toad>	and I guess you should override .equals also to always return false
[14:06]	<Toad>	just to maintain that compareTo/equals consistency contract
{code}

... or just copy MapFile local and add getBefore to get the row just before the delete record.;;;","26/Jul/08 21:15;stack;Made blocker for 0.2.0. release.;;;","27/Jul/08 09:15;rafan;The bug is more serious than I thought.

It also exists in Memcache. In internalGetRowKeyAtOrBefore(), we search all keys *after* the searchKey and if the searchKey is deleted, then we only look into tailMap while we should also check headMap for this case.
;;;","27/Jul/08 09:15;rafan;Attach a unit test that test both memcache and mapfile. The data inserted is simple:

10
20 <== deleted
30 <== deleted

then we look for 30.

For Memcache, we get NPE. For HStore, we have the infinite loop.

The loop in HStore happens like the following. The first searchKey (30 without column name, latest timestamp) gives (20, 30). But since 20 and 30 are all deleted and no candidates found. We set searchKey to 30 (with columnname, latest timestamp. Next search gives us (20, 30). Now the loops begin. If you add some debug messages in HStore, you can see something like this:

{code}
enter rowKeyFromMapFileEmptyKeys to search row 030 now 1217149629818
// 1st searchKey
searchKey 030//9223372036854775807
foundCandidate false searchKey 030//9223372036854775807
readKey 020/colfamily1:/1217149629664 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
close match, but deleted
readKey 030/colfamily1:/1217149629665 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
exact match, but deleted
no candidates, but a deleted/expired row exists
// 2nd searchKey
set searchKey to 030/colfamily1:/9223372036854775807
foundCandidate false searchKey 030/colfamily1:/9223372036854775807
readKey 020/colfamily1:/1217149629664 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
close match, but deleted
readKey 030/colfamily1:/1217149629665 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
exact match, but deleted
no candidates, but a deleted/expired row exists
// 3rd searchKey (same as 2nd!!)
set searchKey to 030/colfamily1:/9223372036854775807
foundCandidate false searchKey 030/colfamily1:/9223372036854775807
readKey 020/colfamily1:/1217149629664 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
close match, but deleted
readKey 030/colfamily1:/1217149629665 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
exact match, but deleted
no candidates, but a deleted/expired row exists
set searchKey to 030/colfamily1:/9223372036854775807
foundCandidate false searchKey 030/colfamily1:/9223372036854775807
readKey 020/colfamily1:/1217149629664 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
close match, but deleted
readKey 030/colfamily1:/1217149629665 readVal 48 42 41 53 45 3a 3a 44 45 4c 45 54 45 56 41 4c
exact match, but deleted
// I break out here so that unit test can really output debug messages
same searchKey!
{code};;;","27/Jul/08 16:27;stack;Thanks Rong-en.  I'm working on a fix based on Todd Lipcon's first suggestion; seems least disruptive (though may be issue if multiple deletes in a row -- checking).;;;","28/Jul/08 00:28;stack;Patch passes all unit tests but still not right.  If I make optional flushes happen every 3 minutes, then on cluster run big loading, things go awry soon after optional flush runs.  Need to spend more time on this.;;;","29/Jul/08 07:57;stack;Still not right.  Throwing WRE.;;;","29/Jul/08 19:20;stack;v2 seems to work in my testing.  Here is v3.  Some cleanup.  Running more tests.  Comment on patch to follow....;;;","30/Jul/08 05:03;stack;Address corner case.  Ensure the first candidate key is not before the first key in a mapfile if first key in mapfile is of same row as candidate.;;;","30/Jul/08 06:17;stack;Fix error found by Rong-en review.;;;","30/Jul/08 06:25;stack;Update v5 so applies to trunk.;;;","30/Jul/08 16:26;rafan;For the record, with v6 applied, I got table not found exception within
6 mins which suggests the error is in memcache.
;;;","31/Jul/08 06:17;stack;Adds fix for the WrongRegionException I'd been seeing.  Rong-en, don't know if it fixes the TableNotFound that you were seeing.;;;","31/Jul/08 18:02;stack;Comment on IRC from Rong-en:

{code}
[08:10]	<rafan>	st^ack: my job looks fine with v7 after 6 hours of running
[08:12]	<rafan>	st^ack: but it won't be finished until 1.5 days later so.. let's what happens
{code};;;","31/Jul/08 19:12;stack;Latest version of patch.  v8/v9 adds fix for case where we found candidate row-before out in the mapfiles but its been deleted up in the memcache; we weren't moving past the delete.

There is still one more issue outstanding; getting NULL HRegionInfo up on test cluster about 70% into loading job with 8 clients.

Rong-en, can we change your loading job into an MR upload once this bug is fixed so it completes faster?;;;","31/Jul/08 21:36;jimk;Reviewed patch V9. Makes my head hurt as the code becomes even more complicated. However the patch  looks as if it covers the corner cases. A lot of the corner cases are exposed by this patch which is good. Certainly better than before. My only fear is that we still might have missed some, but in view of the complexity of this, I can't imagine what is being overlooked. Some good additions to test suite. 

+1
;;;","01/Aug/08 06:30;stack;Fix silly mistake in memcache where tailmap would be empty because of test comparing rows; was using the search_key.getRow instead of passed in row.  This is error I introduced.;;;","01/Aug/08 06:40;stack;Committed.;;;","02/Aug/08 05:58;rafan;Thanks for the fix! With r681612 (v17 fix in place), I can
finish my load. But it's slower due to HBASE-790.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
NPE caused by StoreFileScanner.updateReaders,HBASE-750,12400502,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,apurtell,apurtell,17/Jul/08 18:21,22/Aug/08 21:13,01/Jul/25 07:49,23/Jul/08 19:33,0.2.0,,,,,0.2.0,,,regionserver,,,,0,"Running a test to determine performance during inserts of many 100,000s of cells into a single column family in a single row, the region server involved went down after taking a NPE:

2008-07-17 18:12:18,051 FATAL org.apache.hadoop.hbase.regionserver.Flusher: Replay of hlog required. Forcing server restart
org.apache.hadoop.hbase.DroppedSnapshotException
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1040)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:942)
        at org.apache.hadoop.hbase.regionserver.Flusher.flushRegion(Flusher.java:174)
        at org.apache.hadoop.hbase.regionserver.Flusher.run(Flusher.java:93)
Caused by: java.lang.NullPointerException
        at java.lang.String.<init>(String.java:516)
        at org.apache.hadoop.hbase.util.Bytes.toString(Bytes.java:71)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.updateReaders(StoreFileScanner.java:374)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:797)
        at org.apache.hadoop.hbase.regionserver.HStore.updateReaders(HStore.java:784)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:755)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:682)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1030)
        ... 3 more

Any ideas about this one?
",CentOS 5.1 x86_64 / 64-bit JDK build 1.6.0_03-b05,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/08 18:25;apurtell;HRegionServer-sjdc-atr-test-4.log;https://issues.apache.org/jira/secure/attachment/12386331/HRegionServer-sjdc-atr-test-4.log","17/Jul/08 19:14;apurtell;chart.png;https://issues.apache.org/jira/secure/attachment/12386337/chart.png","17/Jul/08 18:24;apurtell;columntest.java;https://issues.apache.org/jira/secure/attachment/12386330/columntest.java","17/Jul/08 18:28;apurtell;output.txt;https://issues.apache.org/jira/secure/attachment/12386332/output.txt",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25375,,,,,Wed Jul 23 19:33:23 UTC 2008,,,,,,,,,,"0|i0h97z:",98756,,,,,,,,,,,,,,,,,,,,,"17/Jul/08 19:14;apurtell;Regarding the performance test itself, these were my findings:

I created a test table with a single column family and a single row. The test added 1000 cells at a time, using unique qualified column family addresses, into the single column family in that single row. The test attempted to add eventually 1M cells to the family/row and hence 1M unique qualified column family members. During the test both the time required for insert of 1000 cells in batch, the time required to read back one cell from the row, and the time required to read back all cells from the row were measured.

I executed the test using a test cluster running Hadoop 0.17.1 and HBase trunk (0.2.0-dev) revision 675659. The test client and regionserver ran on different hosts linked by a Gigabit Ethernet network. 

The time required to insert 1000 cells remained within a narrow range over the duration of the test, ranging from 6 to 85 milliseconds.

The time required to retrieve a single cell from the column/row remained within a narrower range over the duration of the test, ranging from 1 to 42 milliseconds.

The time required to read back all values in the column/row increased in essentially a linear manner over the duration of the test, rising from 18 milliseconds to 6,064 milliseconds from 1,000 cells to 865,000 cells. See attached chart. I don't find this surprising and I do not think this behavior indicates a bug. Maybe the slope and possibly also some of the variability can be decreased through profile driven changes, but there will always be this kind of linear increase as cells are added to a column family/row in this manner. ;;;","18/Jul/08 00:14;jimk;WRT the NPE in StoreFileScanner, please update your source. It has been fixed (I believe) since the revision you are running.;;;","18/Jul/08 01:09;apurtell;I can reproduce this also with 677784, updated maybe 60 minutes ago.;;;","18/Jul/08 01:18;jimk;Can you please attach a current stack trace? The line numbers of your previous stack trace do  not line up with the current source. Thanks!
;;;","18/Jul/08 02:43;apurtell;2008-07-18 02:38:09,912 FATAL org.apache.hadoop.hbase.regionserver.Flusher: Replay of hlog required. Forcing server restart
org.apache.hadoop.hbase.DroppedSnapshotException: region: t1,,1216346719330
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1050)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:949)
	at org.apache.hadoop.hbase.regionserver.Flusher.flushRegion(Flusher.java:173)
	at org.apache.hadoop.hbase.regionserver.Flusher.run(Flusher.java:91)
Caused by: java.lang.NullPointerException
	at java.lang.String.<init>(String.java:516)
	at org.apache.hadoop.hbase.util.Bytes.toString(Bytes.java:71)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.updateReadersStoreFileScanner.java:379)
	at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:677)
	at org.apache.hadoop.hbase.regionserver.HStore.updateReaders(HStore.java:664)
	at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:634)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:565)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1037)
	... 3 more
2008-07-18 02:38:09,913 INFO org.apache.hadoop.hbase.regionserver.Flusher: regionserver/0:0:0:0:0:0:0:0:60020.cacheFlusher exiting
2008-07-18 02:38:10,594 INFO org.apache.hadoop.hbase.regionserver.LogRoller: LogRoller exiting.
;;;","22/Jul/08 20:59;stack;I committed this:

{code}
Index: src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java (revision 678878)
+++ src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java (working copy)
@@ -377,7 +377,8 @@
       ViableRow viableRow = getNextViableRow();
       openReaders(viableRow.getRow());
       LOG.debug(""Replaced Scanner Readers at row "" +
-        Bytes.toString(viableRow.getRow()));
+        (viableRow == null || viableRow.getRow() == null? ""null"":
+          Bytes.toString(viableRow.getRow())));
     } finally {
       this.lock.writeLock().unlock();
     }
{code}

Lets see if we see the exception again.;;;","22/Jul/08 22:51;apurtell;Did not fix.

Version: 0.2.0-dev, r678937

2008-07-22 22:48:19,800 FATAL org.apache.hadoop.hbase.regionserver.Flusher: Replay of hlog required. Forcing server restart
org.apache.hadoop.hbase.DroppedSnapshotException: region: t1,,1216764706358
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1067)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:965)
        at org.apache.hadoop.hbase.regionserver.Flusher.flushRegion(Flusher.java:173)
        at org.apache.hadoop.hbase.regionserver.Flusher.run(Flusher.java:91)
Caused by: java.lang.NullPointerException
        at java.lang.String.<init>(String.java:516)
        at org.apache.hadoop.hbase.util.Bytes.toString(Bytes.java:71)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.updateReaders(StoreFileScanner.java:379)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:678)
        at org.apache.hadoop.hbase.regionserver.HStore.updateReaders(HStore.java:665)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:635)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:566)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1054)
        ... 3 more;;;","22/Jul/08 22:56;apurtell;Let me check again. Just noticed that the '-dev' has been dropped from the version (and jar file).;;;","23/Jul/08 00:39;apurtell;Looks good. The always repeatable test case I have for this now runs to completion, and it never did prior to today's commit to StoreFileScanner. ;;;","23/Jul/08 19:33;stack;OK.  Resolving.  Looking more at code, seems like a ViableRow with a null row is a distinct possibility. All code but silly debug statement had explicit handling of null row.;;;","23/Jul/08 19:33;stack;Oh, thanks for testing Andrew.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column length limit is not enforced,HBASE-742,12400126,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jimk,jimk,11/Jul/08 17:15,22/Aug/08 21:13,01/Jul/25 07:49,14/Jul/08 21:40,0.1.3,0.2.0,,,,0.1.4,0.2.0,,Client,,,,0,"HColumnDescriptor provides for a limit on column value length but it is not enforced in 0.1.3 or 0.2.0 other than in the REST and Thrift APIs. (I thought it was enforced in some earlier revision but cannot find it).

Enforcement on the client side would be less complicated than doing it on the server side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/08 19:03;jdcryans;hbase-742-v1.patch;https://issues.apache.org/jira/secure/attachment/12385893/hbase-742-v1.patch","14/Jul/08 18:02;jdcryans;hbase-742-v2.patch;https://issues.apache.org/jira/secure/attachment/12385997/hbase-742-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25372,,,,,Mon Jul 14 21:40:39 UTC 2008,,,,,,,,,,"0|i0h967:",98748,,,,,,,,,,,,,,,,,,,,,"11/Jul/08 18:08;jdcryans;Correction, REST and Thrift APIs only refer to it, nothing is enforced.;;;","11/Jul/08 19:03;jdcryans;First try at validating values length.;;;","14/Jul/08 17:21;jdcryans;The first patch broke the build in TestTable, had to put the validation code at the HRS level. On the good side, in the first patch the getTableDescription did an RPC at each validation which was somewhat bad so having it in HRS is more direct.

Review please.;;;","14/Jul/08 21:40;stack;Committed.  Thanks for the patch J-D.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThriftServer getting table names incorrectly,HBASE-740,12400116,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,,tim_s,tim_s,11/Jul/08 13:21,22/Aug/08 21:13,01/Jul/25 07:49,11/Jul/08 16:44,0.2.0,,,,,0.2.0,,,Thrift,,,,0,"Slight bug.
TableDescriptor name is stored internally as byte[] now, but the thrift server wasn't updated to reflect that.
It is returning the table name incorrectly in getTableNames. This is also the case, for getTableRegions",,,,,,,,,,,,,,,,,,HBASE-697,,,,,,,,,,,"11/Jul/08 13:32;tim_s;thrifttablename.patch;https://issues.apache.org/jira/secure/attachment/12385871/thrifttablename.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25371,,,,,Fri Jul 11 16:44:15 UTC 2008,,,,,,,,,,"0|i0h95r:",98746,,,,,,,,,,,,,,,,,,,,,"11/Jul/08 16:44;stack;Committed.  Thanks for figuring this one Tim and for the patch.  I'd guess that there are a bunch more like this lurking; HBASE-697 was supposed to be the place where we nailed them all.   If you keep submitting patches, I'll keep applying them.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseAdmin.createTable() using old HTableDescription doesn't work,HBASE-739,12400062,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,irubin,srainville,srainville,10/Jul/08 21:26,22/Aug/08 21:13,01/Jul/25 07:49,14/Jul/08 21:40,0.2.0,,,,,0.2.0,,,Client,,,,0,"The following test case (see below) illustrate what used to work in branch 0.1 and that doesn't anymore. testTruncateInTrunk() shows how I got it to work again. I get this error now when trying the old code but using trunk:

java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.openplaces.test.fixture.FixtureLoader.truncateHbaseTable(FixtureLoader.java:105)
	at com.openplaces.test.fixture.FixtureLoader.loadHbaseFixtures(FixtureLoader.java:63)
	at com.openplaces.test.fixture.TestCaseWithFixtures.hbaseFixtures(TestCaseWithFixtures.java:34)
	at com.openplaces.test.isolated.TestSearchSRFIEF.setUp(TestSearchSRFIEF.java:37)
	at junit.framework.TestCase.runBare(TestCase.java:125)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: java.net.SocketTimeoutException: timed out waiting for rpc response
	at org.apache.hadoop.ipc.Client.call(Client.java:559)
	at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:211)
	at $Proxy5.createTable(Unknown Source)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTableAsync(HBaseAdmin.java:184)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:144)
	at com.openplaces.util.hbaserecord.connectionadapters.HbaseAdapter.truncateTable(HbaseAdapter.java:502)
	at com.openplaces.util.hbaserecord.Base$Singleton.truncate(Base.java:609)
	... 21 more




import java.io.IOException;
import java.util.Collection;

import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;

import junit.framework.TestCase;

@SuppressWarnings(""deprecation"")
public class TestTruncate extends TestCase {

	public void testTruncateInBranch_0_1() throws IOException{
		HTable table = new HTable(""mytable"");
		HBaseAdmin admin = new HBaseAdmin(new HBaseConfiguration());
		HTableDescriptor tableDesc = table.getMetadata();
		admin.deleteTable(table.getTableName());
		admin.createTable(tableDesc);
	}

	public void testTruncateInTrunk() throws IOException{
		HTable table = new HTable(""mytable"");
		HBaseAdmin admin = new HBaseAdmin(new HBaseConfiguration());
		Collection<HColumnDescriptor> families = table.getMetadata().getFamilies();
		
		HTableDescriptor tableDesc = new HTableDescriptor(table.getTableName());
		for(HColumnDescriptor family : families){
			tableDesc.addFamily(family);
		}

		admin.deleteTable(table.getTableName());
		admin.createTable(tableDesc);
	}
}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/08 13:54;srainville;TEST-org.apache.hadoop.hbase.TestTruncateIsaak.txt;https://issues.apache.org/jira/secure/attachment/12385981/TEST-org.apache.hadoop.hbase.TestTruncateIsaak.txt","11/Jul/08 22:21;irubin;TestHbase739.java;https://issues.apache.org/jira/secure/attachment/12385908/TestHbase739.java","14/Jul/08 13:54;srainville;TestTruncate2.java;https://issues.apache.org/jira/secure/attachment/12385982/TestTruncate2.java","11/Jul/08 22:14;irubin;hbase-739.patch;https://issues.apache.org/jira/secure/attachment/12385907/hbase-739.patch","14/Jul/08 20:21;irubin;hbase-739_v2.patch;https://issues.apache.org/jira/secure/attachment/12386010/hbase-739_v2.patch","14/Jul/08 13:54;srainville;master.log;https://issues.apache.org/jira/secure/attachment/12385983/master.log",,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25370,,,,,Mon Jul 14 21:40:34 UTC 2008,,,,,,,,,,"0|i0h95j:",98745,,,,,,,,,,,,,,,,,,,,,"11/Jul/08 15:56;jimk;The API for 0.2.0 is not backward compatible with 0.1.x 

This was a deliberate decision since the new features of 0.2.0 required big API changes, and some of the 0.1.x behavior cannot be supported with a compatibility layer.;;;","11/Jul/08 22:14;irubin;I've investigated this issue and have several things to report.  Sebastien, I wasn't able to replicate the exact error you gave in the issue description.  I did, however, get an error under the same circumstances (for testTruncateInBranch_0_1, although my error was a SocketTimeoutException).  Also, I should point out that the testing code you posted doesn't disable the table before dropping it, which causes other problems.

The issue, or at least the one that I experienced with SocketTimeoutException, was that UnmodifyableHTableDescriptor did not serialize.  The server could not see the UnmodifyableHTableDescriptor class because it was package-protected and resides in hbase.client.  The solution, and the attached patch, merely involves adding the word ""public"" to the UnmodifyableHTableDescriptor class declaration.  The server can now see the class and serialize it.  ;;;","11/Jul/08 22:19;stack;I applied Izaak's patch (You should be able to use an UnmodifableHTD in the manner Sebastien is trying to use it above).

Sebastien, if Izaak's patch fixes your issue, please close this issue; otherwise, provide more info.  Thanks.;;;","11/Jul/08 22:21;irubin;I'm attaching the JUnit test I used for testing the issue.  The code is almost identical to that posted by Sebastien, except I've added lines that disable the table before dropping.;;;","14/Jul/08 13:54;srainville;I'm still getting the same error. I attached the logs of the test case and the master in debug mode. I modified the test case so it creates the test table automatically. I'm not so worried about this issue but it might uncover something worse underneath. And about the missing disableTable()... it was a copy paste error.;;;","14/Jul/08 14:01;srainville;Actually, I'm not getting the exact same error as before. I'm getting java.net.SocketTimeoutException directly... and before I was getting a java.lang.reflect.InvocationTargetException CAUSED BY java.net.SocketTimeoutException.

It sounds like I'm getting the same error as you were seeing Isaac but somehow you got rid of it and not me. (and yes I updated my code and I see that you patch has been applied) So, your patch resolved the java.lang.reflect.InvocationTargetException but not the java.net.SocketTimeoutException.
;;;","14/Jul/08 16:33;irubin;Yeah, I'm also getting the same error again.  Its weird, the code hasn't changed since Friday!  I'm looking back into it again.;;;","14/Jul/08 20:21;irubin;I think I've really fixed it this time.  I added a default constructor to UnmodifyableHTableDescriptor, which I should have done in the last patch (this all goes in the same vein of making UnmodifyableHTableDescriptor serializable).  

I found that Eclipse really confused the testing process - I definitely ran the test with and without the default constructor, sometimes succeeding and sometimes failing.  Running the test on the command-line was much more useful and straightforward, and I'd recommend doing that instead (at least for this test).  Here's how you can run just that test on the command-line:

Add the following to your build.xml:

{code}
<target name=""ensure-test-name"" unless=""test"">
  <fail message=""You must run this target with -Dtest=TestName""/>
</target>

<target name=""runtest"" description=""Runs the test you specify on the command line with -Dtest="" depends=""compile, ensure-test-name"">
  <junit printsummary=""withOutAndErr"" fork=""yes""> 
    <classpath refid=""test.classpath"" />
    <formatter type=""plain"" usefile=""false""/>
    <batchtest>
      <fileset dir=""${src.test}"">
        <include name=""**/${test}.java""/>
      </fileset>
    </batchtest>
  </junit>
</target>
{code}

Start HBase, make sure TestHbase739.java is added, ""mytable"" exists, etc, and then enter:

{code}
ant clean jar compile-test runtest -Dtest=TestHbase739
{code}

Without the attached patch, hbase-739_v2.patch, this should fail (it might say ""Build Successful"", but check the actual junit output).  After adding the attached patch, this should succeed (at least it did for me, when doing this process on a fresh copy of HBase).  

Let me know if this is still generating errors.;;;","14/Jul/08 21:15;stack;Applied Izaaks second patch (Thanks Izaak).;;;","14/Jul/08 21:40;srainville;Unit test ran successfuly on my box too using the code in trunk, which has your patch applied.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase shell doesn't trap CTRL-C signal,HBASE-735,12399972,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,srainville,srainville,09/Jul/08 23:58,22/Aug/08 21:13,01/Jul/25 07:49,10/Jul/08 17:40,0.2.0,,,,,0.2.0,,,util,,,,0,"From withing the hbase shell, when there's a IO problem, the hbase client code tries to recover automatically but sometimes we know what's going on and all we want is to cancel the operation by pressing CTRL-C but the shell doesn't catch it and we need to either wait for the operation to timeout or close the terminal and open another one.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/08 17:23;jdcryans;hbase-735-v1.patch;https://issues.apache.org/jira/secure/attachment/12385786/hbase-735-v1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25368,,,,,Thu Jul 10 17:40:52 UTC 2008,,,,,,,,,,"0|i0h94n:",98741,,,,,,,,,,,,,,,,,,,,,"10/Jul/08 17:10;jdcryans;+1. An easy way of getting an IO error is shutting down HBase and trying almost anything in the shell.;;;","10/Jul/08 17:23;jdcryans;Removed the sigint trap in hirb;;;","10/Jul/08 17:40;stack;Committed.  Thanks for the patch J-D. 

This is improves over what was there previous but kills the shell altogether.  Best would be our killing the outstanding process if we could leaving the shell up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"scan '.META.', {LIMIT => 10} crashes",HBASE-734,12399958,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,irubin,srainville,srainville,09/Jul/08 20:52,22/Aug/08 21:13,01/Jul/25 07:49,09/Jul/08 22:55,0.2.0,,,,,0.2.0,,,util,,,,0,"this command in the jruby doesn't work: scan '.META.', {LIMIT => 10}

got this:
hbase(main):007:0> scan '.META.', {LIMIT => 10}
08/07/09 16:49:08 DEBUG client.HConnectionManager$TableServers: Cache hit in table locations for row <> and tableName .META.: location server 127.0.0.1:51708, location region name .META.,,1
NoMethodError: undefined method `to_java' for {""LIMIT""=>10}:Hash
	from /home/sebastien/projets/java/hbase-trunk/bin/../bin/HBase.rb:225:in `scan'
	from /home/sebastien/projets/java/hbase-trunk/bin/../bin/hirb.rb:265:in `scan'
	from (hbase):8:in `binding'
hbase(main):008:0> 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/08 22:06;irubin;hbase-734.patch;https://issues.apache.org/jira/secure/attachment/12385680/hbase-734.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25367,,,,,Wed Jul 09 22:55:08 UTC 2008,,,,,,,,,,"0|i0h94f:",98740,,,,,,,,,,,,,,,,,,,,,"09/Jul/08 20:58;stack;Izaak is the man for this one;;;","09/Jul/08 22:06;irubin;The problem was in the documentation of the scan command, not the actual command itself.  Scan must take in the name of the table, and may optionally take an array of columns *or* an array of columns and scanner specifications.  Scan will not accept only a table name and scanner specifications.  The attached patch makes this more clear in the documentation, and gives the user a warning if they use scan with only a table name and scanner specifications.

The tests in bin/HBase.rb and bin/Formatter.rb all passed.;;;","09/Jul/08 22:55;stack;Committed.  Thanks for the patch Izaak.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
client region/metadata cache should have a public method for invalidating entries,HBASE-729,12399795,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,apurtell,apurtell,apurtell,08/Jul/08 09:36,13/Sep/08 23:22,01/Jul/25 07:49,11/Aug/08 21:58,0.2.0,,,,,0.18.0,0.2.1,,Client,,,,0,"While writing a testcase for HBASE-62, I observed that table metadata is cached as part of the region information cached  client side. This cached region information (and therefore table metadata) is not directly invalidated by disable/enable table, so to get up to date metadata the client may have to use a scanner over .META. directly using the meta visitor. Ideally other client code -- for example the support for HBASE-62 -- should be able to invalidate entries as necessary, so then the next HTable.getTableDescriptor() would go to meta to return up to date information instead of incorrectly reusing outdated information from the cache.",Linux CentOS 5.1 x86_64 / JDK 1.6,,,,,,,,,HBASE-811,,,HBASE-800,,,,,,,,,,HBASE-62,,,,,,"10/Aug/08 03:06;apurtell;729.patch;https://issues.apache.org/jira/secure/attachment/12387893/729.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25365,,,,,Mon Aug 11 21:58:39 UTC 2008,,,,,,,,,,"0|i0h93b:",98735,,,,,,,,,,,,,,,,,,,,,"09/Aug/08 04:54;apurtell;Any interface that references HTD or HCD metadata relies on the cache containing up to date structures, so this blocks work on any client interface for this (e.g. shell, Thrift, or REST).;;;","10/Aug/08 02:29;apurtell;Actually just always using a MetaScanner in getHTableDescriptor is enough.;;;","10/Aug/08 03:06;apurtell;Previous patch had linefeed issues that would cause patch to barf.;;;","11/Aug/08 21:51;stack;Andrew, does TestTable unit test pass for you?  Fails for me when I apply this patch.;;;","11/Aug/08 21:55;stack;Ignore my comment in above. 

Cleaning and retrying all passed (just takes a while).;;;","11/Aug/08 21:58;stack;Committed to branch and trunk.  Thanks for the patch Andrew.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High-load import of data into single table/family never triggers split,HBASE-707,12399016,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,streamy,streamy,25/Jun/08 17:46,22/Aug/08 21:14,01/Jul/25 07:49,26/Jun/08 03:40,0.1.3,,,,,0.1.3,,,,,,,0,"Importing a heavy amount of data into a single table and family.

One column in that family (the same fam:col for every row) contains a frequently large amount of UTF-8 data.  This column grows and grows but never causes a region split.

Currently there is a single mapfile containing nearly 10GB.

Eventually this has caused regions to crash with OOME, as described in HBASE-706


Table in question:

hql > describe items;
+-----------------------------------------------------------------------------+
| Column Family Descriptor                                                    |
+-----------------------------------------------------------------------------+
| name: cfrecs, max versions: 2, compression: NONE, in memory: false, max leng|
| th: 2147483647, bloom filter: none                                          |
+-----------------------------------------------------------------------------+
| name: clusters, max versions: 2, compression: NONE, in memory: false, max le|
| ngth: 2147483647, bloom filter: none                                        |
+-----------------------------------------------------------------------------+
| name: content, max versions: 2, compression: NONE, in memory: false, max len|
| gth: 2147483647, bloom filter: none                                         |
+-----------------------------------------------------------------------------+
| name: readby, max versions: 2, compression: NONE, in memory: false, max leng|
| th: 2147483647, bloom filter: none                                          |
+-----------------------------------------------------------------------------+
| name: receivedby, max versions: 2, compression: NONE, in memory: false, max |
| length: 2147483647, bloom filter: none                                      |
+-----------------------------------------------------------------------------+
| name: savedby, max versions: 2, compression: NONE, in memory: false, max len|
| gth: 2147483647, bloom filter: none                                         |
+-----------------------------------------------------------------------------+
| name: sentby, max versions: 2, compression: NONE, in memory: false, max leng|
| th: 2147483647, bloom filter: none                                          |
+-----------------------------------------------------------------------------+
7 columnfamily(s) in set. (0.34 sec)
","Linux 2.6.25-14.fc9.x86_64, Fedora Core 9",,,,,,,,,,,,,,,,,,,,,,,,HBASE-706,,,,"25/Jun/08 19:49;stack;707.patch;https://issues.apache.org/jira/secure/attachment/12384702/707.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25350,,,,,Thu Jun 26 03:40:48 UTC 2008,,,,,,,,,,"0|i0h8yf:",98713,,,,,,,,,,,,,,,,,,,,,"25/Jun/08 17:47;streamy;The lack of splitting eventually lead to the OOME when attempting compaction;;;","25/Jun/08 17:52;streamy;Added table description;;;","25/Jun/08 19:49;stack;Have been working with John on his cluster on this issue.  This patch seems to fix the issue (more testing to do).

Splits are triggered if the compaction run returns true.  The return up out of compaction was coming up from the depths of store file and on the way could be mangled if multiple families in a region; one might compact but the subsequent one might not.  Because of the latter, we'd not run split check.;;;","25/Jun/08 23:28;streamy;Recreated entire use case scenario and the issue is gone.  We are now seeing normal region splits.

However, we have experienced a new behavior during those splits.  We are writing and the client receives an IllegalStateException:

Trying to commit: Exception in thread ""main"" java.lang.RuntimeException: java.lang.IllegalStateException: region offline: items,823ce1e3-d414-474f-ac70-c4081cecef0f,1214434560891
 at org.apache.hadoop.hbase.HTable.getRegionServerWithRetries(HTable.java:1062)
 at org.apache.hadoop.hbase.HTable.commit(HTable.java:763)
 at org.apache.hadoop.hbase.HTable.commit(HTable.java:744)
 at HBase.AddAttributes(HBase.java:220)
 at PoJaMigratorItems.Add(PoJaMigratorItems.java:143)
 at PoJaMigrator.AddItems(PoJaMigrator.java:123)
 at PoJaMigrator.AddAllData(PoJaMigrator.java:57)
 at PoJaMigrator.<init>(PoJaMigrator.java:27)
 at PoJaMigrator.main(PoJaMigrator.java:35)
Caused by: java.lang.IllegalStateException: region offline: items,823ce1e3-d414-474f-ac70-c4081cecef0f,1214434560891
 at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:438)
 at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:350)
 at org.apache.hadoop.hbase.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:318)
 at org.apache.hadoop.hbase.HTable.getRegionLocation(HTable.java:114)
 at org.apache.hadoop.hbase.HTable$ServerCallable.instantiateServer(HTable.java:1021)
 at org.apache.hadoop.hbase.HTable.getRegionServerWithRetries(HTable.java:1036)
 ... 8 more
;;;","26/Jun/08 00:48;stack;Thanks for confirming patch Jon. The ISE is because your clocks are way skewed.   Will fix that over in HBASE-710  I'll commit this patch later tonight.;;;","26/Jun/08 03:40;stack;Committed to branch.  Trunk doesn't have this issue.  It has another: ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update new shell docs and commands on help menu,HBASE-704,12398968,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,viper799,viper799,25/Jun/08 07:47,22/Aug/08 21:13,01/Jul/25 07:49,25/Jun/08 21:04,0.2.0,,,,,0.2.0,,,,,,,0,"From the help screen on the new shell

{code}
[root@s2 hbase]# hbase shell
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Version: 0.2.0-dev, r670701, Wed Jun 25 02:27:19 CDT 2008

hbase(main):001:0> create 't1' {NAME => 'f1', VERSIONS => 5}
SyntaxError: (hbase):2: , unexpected tLCURLY
{code}

The help menu gives the above example on creating table in hbase but it does not work!

If we release this for the new shell examples need to be more clear. I have not been able to create a table using the new shell yet,

Also might be worth adding the old shell back in and remove it after we release 2.0 or at lease until we work out the bugs in new client. If it would not require much updating to keep it current with the new api.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/08 17:18;stack;704.patch;https://issues.apache.org/jira/secure/attachment/12384689/704.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25347,,,,,Wed Jun 25 21:04:25 UTC 2008,,,,,,,,,,"0|i0h8xr:",98710,,,,,,,,,,,,,,,,,,,,,"25/Jun/08 17:18;stack;Hey Billy:

Sorry, there was a typo in the new shell help.  There was a missing comma after the 't1'.  The attached patch which I've applied fixes the typo and adds some more create table examples that hopefully will help.

If the new help text is sufficient, please close this issue.  Open new issues for other places that help is lacking and I'll address them.;;;","25/Jun/08 21:04;viper799;Looks good now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid regions listed by regionserver.jsp,HBASE-703,12398967,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,irubin,mbu,mbu,25/Jun/08 06:40,22/Aug/08 21:13,01/Jul/25 07:49,03/Jul/08 19:27,0.2.0,,,,,0.2.0,,,,,,,0,"The region list displayed by regionserver.jsp contains regions that have ceased existence due to splits.

Example:
Region Name	Encoded Name	Start Key	End Key
...
maxentriestest,acacdk,1214292085212	732557990 	acacdk	
maxentriestest,acacdk,1214297936860	1583424516 	acacdk	acqtzk
maxentriestest,acacdk,1214293855954	1509492302 	acacdk	adhlxw
maxentriestest,acqtzk,1214297936862	1120286366 	acqtzk	adhlxw
maxentriestest,adhlxw,1214293855955	400707061 	adhlxw	
maxentriestest,adhlxw,1214299372674	2060549477 	adhlxw	aelrxo
maxentriestest,adhlxw,1214297324386	336026175 	adhlxw	afpxzs
maxentriestest,aelrxo,1214299372674	1352588233 	aelrxo	afpxzs
maxentriestest,afpxzs,1214297324387	1235754353 	afpxzs	
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/08 17:54;irubin;HBASE-703.patch;https://issues.apache.org/jira/secure/attachment/12385221/HBASE-703.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25346,,,,,Thu Jul 03 19:27:25 UTC 2008,,,,,,,,,,"0|i0h8xj:",98709,,,,,,,,,,,,,,,,,,,,,"25/Jun/08 18:26;stack;Fix for 0.2;;;","29/Jun/08 06:25;viper799;I am also seeing this, It also effects the master rebalance Total Load numbers so region server are not balanced correctly.

;;;","02/Jul/08 19:54;stack;Giving this one to Izaak.;;;","03/Jul/08 17:53;irubin;I've attached a patch (HBASE-703.patch) that fixes the problem.

The problem originated in the HRegion splitting code. Each HRegionServer maintains a collection of its daughter HRegions that are currently online. When an HRegion was split, the parent HRegion would be correctly closed and taken offline; however, the responsible HRegionServer was never informed of the change. regionserver.jsp would display the HRegionServer's improperly updated collection of online HRegions. The result was that all regions, even those that were split and offline, were displayed in the UI.

The patch makes changes to HRegionServer.java and CompactSplitThread.java. There is a new method in HRegionServer, removeFromOnlineRegions(region), which removes the HRegion from the collection of online regions. This method is called in CompactSplitThread.split().

What changed that created this bug? In branch/0.1, there was a notion of both online regions and retiring regions, and there were separate collections for each. There was also a listener structure (RegionUnavailableListener) built into the splitting process that correctly handled the region transitions from online to retiring to their removal. However, since this branch, the splitting code has been simplified such that the listener and ""retiring"" regions no longer exist. Since the listener was responsible for updating the HRegionServer, its removal created the bug.


**A note on the patch: all HBase tests passed.  However, in running all tests together, a few tests (including TestMetaUtils) timed out.  I re-ran these tests individually and they passed.;;;","03/Jul/08 19:27;stack;Committed.  Thanks for the patch Izaak.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestMigrate up on Hudson,HBASE-699,12398558,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,19/Jun/08 05:49,22/Aug/08 21:13,01/Jul/25 07:49,19/Jun/08 07:14,0.2.0,,,,,0.2.0,,,,,,,0,"Its hanging on hudson again.  Caught a threaddump.  Its that old waiting on a vanished unix process... no hbase threads hanging out.

I tried adding relocateRegion just before taking out scan in verify.  That was good for fixing the first region in the table.  We hung when we tried to get second region.  It was trying to go to old address.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/08 05:54;stack;699.patch;https://issues.apache.org/jira/secure/attachment/12384259/699.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25343,,,,,Thu Jun 19 07:14:53 UTC 2008,,,,,,,,,,"0|i0h8wn:",98705,,,,,,,,,,,,,,,,,,,,,"19/Jun/08 05:54;stack;This patch makes the test pass if I run it up on hudson.

{code}
HBASE-699  Fix TestMigrate up on Hudson
M  src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java
M src/java/org/apache/hadoop/hbase/HMerge.java
  HCM.deleteConnection renamed as deleteConnectionInfo
M  src/test/org/apache/hadoop/hbase/util/TestMigrate.java
  Use new deleteConnectionInfo that takes no arguments.
M  src/java/org/apache/hadoop/hbase/client/HTable.java
  Javadoc for getConnection.
M  src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  Added override of deleteConnection that deletes all connection info.
  Also renamed deleteConnection as deleteConnectionInfo.
{code};;;","19/Jun/08 07:14;stack;Build #184 and #185 didn't hang, just failed in the usual rebalancing and regionserverexit test.  Fix them next.  Resolving.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HLog recovery is not performed after master failure,HBASE-698,12398550,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,clint.morgan,clint.morgan,19/Jun/08 00:36,21/Jan/11 05:38,01/Jul/25 07:49,14/Jul/09 16:49,0.1.2,0.19.3,0.2.1,,,0.20.0,,,master,regionserver,,,0,"I have a local cluster running, and its logging to
<hbase>/log_X.X.X.X_1213228101021_60020/

Then I kill both master and regionserver, and restart. Looking through
the logs I don't see anything about trying to recover from this hlog,
it just creates a new hlog alongside the existing one (with a new
startcode).  The older hlog seems to be ignored, and the tables
created in the inital session are all gone.",,,,,,,,,,HBASE-728,HBASE-546,HBASE-1302,,,,,,,,,,,,,,,,,"14/Jul/09 14:16;jdcryans;hbase-698-0.19.patch;https://issues.apache.org/jira/secure/attachment/12413430/hbase-698-0.19.patch","07/Jul/09 18:57;jdcryans;hbase-698.patch;https://issues.apache.org/jira/secure/attachment/12412775/hbase-698.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25342,Reviewed,,,,Tue Jul 14 23:22:42 UTC 2009,,,,,,,,,,"0|i0h8wf:",98704,Old setups running 0.19 should first delete everything that's log_* in HBase root HDFS folder before updating. Make sure HBase is down before doing that. Also this will be done by migration in 0.20.0,,,,,,,,,,,,,,,,,,,,"19/Jun/08 03:26;stack;Moving into 0.2 for now.  Lets take a look at least at whats going on.;;;","01/Jul/08 02:20;jimk;Moving to 0.3.0 because it really depends on hbase ""safe-mode"" and hadoop appends which aren't supported until hadoop 0.18;;;","01/Aug/08 17:41;jimk;Currently the master will only recover logs during startup if it finds information in in the ROOT or META that indicate that a region was being served. Without appends in hadoop, the edits to the ROOT or META could be lost and so the master would not look for the log of the server that was killed.

This is not a good way to do this. Once appends make log files persistent, the master should start up in safe mode, and first recover all the log files it finds. Region servers will not create their logs until the master has finished recovering all the existing logs and tells the region servers it has finished.;;;","10/Nov/08 20:07;jimk;There is a very simple fix if the master comes back up and knows a region server is dead.

However, if the master dies, region servers hang around until the master comes back up. Thus the master cannot know which HLogs to recover and which belong to running region servers. (""recovering"" a HLog from a running region server would produce unpredictable results, most likely leading to data corruption).

Relying on hdfs lease timeouts on the log files is also not an option as the lease timeout interval is too long for this purpose.

The master can therefore not recover any region server logs unless it knows that region server is dead.  This cannot be accomplished without Zookeeper integration, which will monitor the region servers (and the regions they serve) using ephemeral files. At that point, if the master dies and is restarted, it will know which region servers are alive, which ones have died and all the regions that are currently being served. Then it will know which region server logs to recover and which ones can be ignored (because the region server writing it is still alive).
;;;","10/Nov/08 21:51;jimk;Removed Zookeeper integration as a blocker. 

While Zookeeper will make this much easier, we need an interim solution.;;;","10/Nov/08 22:27;jimk;Making this issue require Zookeeper. We could do something hokey in the mean time, but it would still have holes in it.;;;","10/Nov/08 22:29;jimk;Moving this issue to 0.20.0 as it requires Zookeeper to close all the holes.;;;","30/Apr/09 22:28;apurtell;The scope of this problem is narrowed by new developments. With multiple masters on standby and fail over via ZK, the possibility of encountering this situation is reduced. ;;;","04/May/09 18:22;jimk;> Andrew Purtell added a comment - 30/Apr/09 03:28 PM
> The scope of this problem is narrowed by new developments. With multiple masters on
> standby and fail over via ZK, the possibility of encountering this situation is reduced.

Agreed that the window for missing a lease loss narrows the window greatly.

Do the standby masters watch the region server leases?

The gist of the patch I was working on was to move the code from BaseScanner.checkAssigned
(lines 367 - 392) to a new method in HMaster that is called from HMaster.run() *before*
HMaster.startServiceThreads is called.;;;","21/May/09 17:38;stack;HBASE-1439 is talking about ownership of commit logs being orchestrated by zk; perhaps on startup (or on assumption of master role) master can check zk if commit logs to replay.;;;","09/Jun/09 22:36;jdcryans;When checking the cluster start in a master failover situation, we should check for HLogs to process.;;;","07/Jul/09 14:07;jdcryans;In HBASE-1143 we set a very low flush size on .META. to make it flush after every ~10 updates which lessens the impact of a lost hlog. So the original situation can still happen in the way it is described or if a table split was done and that the region server holding META died pretty much right after that (because either a flush will happen or after 1 hour we will roll the commit log ). The hole is there but it's very small. The only thing missing is appends now. We could punt this to 0.21.0, unless someone votes for a hlog ZK orchestration for 0.20.0. ;;;","07/Jul/09 16:15;stack;Can we not do something basic here?  Even if its master looking into all regions on filesystem for logs to recover on startup.;;;","07/Jul/09 18:03;jimk;Yes, we need to do something. In particular, the master needs to know what region servers are running and what their start code is, so it does not try to recover a log out from under a running region server.;;;","07/Jul/09 18:07;jdcryans;Ok, then I'll simply do a check of the .logs folder. It's going to be part of the verifyClusterState() method so that I have knowledge of existing RSs.;;;","07/Jul/09 18:57;jdcryans;This patch fixes the original issue (or as much we can do without appends). This happens on a master failover:

{code}
2009-07-07 14:45:28,097 INFO org.apache.hadoop.hbase.master.HMaster: Found log folder : jdcryans.local,60020,1246991630883
2009-07-07 14:45:28,097 INFO org.apache.hadoop.hbase.master.HMaster: Log folder belongs to an existing region server
{code}

Whn know about the alive RS since we just scanned the rs folder in ZK. Then if I kill -9 both Master and RS then I restart the cluster I see:
{code}
2009-07-07 14:47:10,034 DEBUG org.apache.hadoop.hbase.master.HMaster: This is a fresh start, proceeding with normal startup
2009-07-07 14:47:10,038 INFO org.apache.hadoop.hbase.master.HMaster: Found log folder : jdcryans.local,60020,1246991630883
2009-07-07 14:47:10,038 INFO org.apache.hadoop.hbase.master.HMaster: Log folder doesn't belong to a known region server, splitting
2009-07-07 14:47:10,043 INFO org.apache.hadoop.hbase.regionserver.HLog: Splitting 1 hlog(s) in hdfs://localhost:9000/hbase/.logs/jdcryans.local,60020,1246991630883
...
{code}

I also removed the log splitting stuff in BaseScanner.;;;","07/Jul/09 19:24;stack;I wonder why the code in BaseScanner was insufficient?;;;","07/Jul/09 19:41;jimk;I don't think it had any idea of region servers that were running, so it would recover *all* logs, including
those from running region servers, which really messed things up.;;;","07/Jul/09 20:05;stack;Ok.

So, j-d, why not mod BaseScanner to check zk rather than do the filesystem check?  Filesystem check is more comprehensive?;;;","07/Jul/09 20:16;jdcryans;bq. I wonder why the code in BaseScanner was insufficient? 

It was reacting only when seeing stale data in META or ROOT so if you lost an edit in META, you won't see it back in BaseScanner. I think BaseScanner never recovered anything ever because we don't have appends. Now we will have the issue of recovering regions without a META entry tho...

bq. So, j-d, why not mod BaseScanner to check zk rather than do the filesystem check? Filesystem check is more comprehensive?

ZK won't tell you about dead region servers, only the live ones.;;;","07/Jul/09 21:28;jimk;If you scan the file system for region server logs, you can ignore the ones for region servers that ZK says
are alive.;;;","07/Jul/09 21:54;jdcryans;bq. If you scan the file system for region server logs, you can ignore the ones for region servers that ZK says are alive. 

That's what my patch does.;;;","07/Jul/09 23:32;stack;Ok.  Thanks lads.

+1 on patch.;;;","08/Jul/09 00:30;jdcryans;Jim, since you worked on this issue, would you like to try it before I commit? (thanks stack);;;","10/Jul/09 15:00;jdcryans;I committed this to trunk. Should I make a patch for 0.19?;;;","10/Jul/09 15:53;stack;I took another look at the patch.  A backport don't look like it'd be that hard and it'd be a nice fix to have.  Good on you J-D;;;","10/Jul/09 16:08;jimk;+1 on backport if it is not too complicated.;;;","14/Jul/09 14:16;jdcryans;Patch against 0.19 branch. It will recover any log directory found at the root HBase folder. I tried it on my machine by creating a table, inserting 500 rows into it and then killing --9 both Master and RS. After restart everything was there. This patch doesn't cover the situation where the logs weren't flush to disk (inserting 10 rows didn't trigger it) and it doesn't take into account existing RS like the patch for trunk does.;;;","14/Jul/09 15:43;davelatham;I have a cluster where I can see several old log directories in the root hbase folder.  What would be the affect of deploying this patch?  Would it be best to manually delete those directories first?;;;","14/Jul/09 15:55;jdcryans;@Dave

I have the same situation here and I'm doing some tests. I saw that edits going to since deleted regions will still create a directory and put the oldlogfile.log there. This is not so bad but not so good either, it eats up disk space for no reason... Edits going to META are more problematic, you could see old regions coming back and completely mess up your data. I'm currently trying to recreate such a setup to confirm.

So, if you want to apply this patch, I currently suggest that you delete those folders.;;;","14/Jul/09 16:04;davelatham;@JD

Thanks for the helpful info.  It might be worth putting out a warning then with a 0.19.4 release to do that - possibly for migration to 0.20.0 also?;;;","14/Jul/09 16:08;jdcryans;Ah well I tested it and it's all OK because of the sequence ID. I used current 0.19, created a table, inserted 500 rows, then killed -9. I redid the same thing in the second run but with a different column family name and shut down clean. I then applied the patch, restarted and I saw:

{code}
2009-07-14 12:06:05,783 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Opening region .META.,,1/1028785192
2009-07-14 12:06:05,789 DEBUG org.apache.hadoop.hbase.regionserver.HStore: loaded /tmp/hbase-jdcryans/hbase/.META./1028785192/info/info/7874001023615296020, isReference=false, sequence id=512, length=559, majorCompaction=false
2009-07-14 12:06:05,789 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Loaded 1 file(s) in hstore 1028785192/info, max sequence id 512
2009-07-14 12:06:05,791 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Applied 0, skipped 7 because sequence id <= 512
{code}

So basically it's ok.;;;","14/Jul/09 16:24;stack;Yes.  Sequenceid should prevent our taking on old edits.  I like the idea of release noting cleanup of dirs going 0.19.3 to 0.19.4 and I'll add cleanup to migration to 0.20.0.  +1 on patch.

;;;","14/Jul/09 16:49;jdcryans;Committed to branch 0.19.;;;","14/Jul/09 23:22;stack;Regards 0.20.x, we don't need to flag folks.  Logs are now in a subdirectory named .logs.  Old log files in old location will be ignored.;;;",,,,,,,,,,,,,,,,,
thrift idl needs update/edit to match new 0.2 API (and to fix bugs),HBASE-697,12398549,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tim_s,stack,stack,18/Jun/08 23:58,13/Sep/08 23:22,01/Jul/25 07:49,16/Aug/08 22:14,0.2.0,,,,,0.18.0,0.2.1,,,,,,0,"Talking w/ Bryan, moving this out of the way of the 0.2.0 release.",,,,,,,,,,,,,HBASE-585,,,,,,,HBASE-740,,,,,HBASE-657,HBASE-822,HBASE-800,,"14/Aug/08 13:23;tim_s;HBASE-697.patch;https://issues.apache.org/jira/secure/attachment/12388235/HBASE-697.patch","16/Aug/08 15:06;tim_s;HBASE-697v2.patch;https://issues.apache.org/jira/secure/attachment/12388363/HBASE-697v2.patch","13/Aug/08 16:42;tow21;hbase-timestamps.patch;https://issues.apache.org/jira/secure/attachment/12388164/hbase-timestamps.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25341,,,,,Sun Aug 17 20:30:39 UTC 2008,,,,,,,,,,"0|i0h8w7:",98703,,,,,,,,,,,,,,,,,,,,,"19/Jun/08 06:06;stack;Try fixing HBASE-657 when we do this one too.;;;","17/Jul/08 07:19;stack;Will we just punt this to 0.2.1.;;;","11/Aug/08 18:13;stack;Here's description of API changes: http://wiki.apache.org/hadoop/Hbase/Plan-0.2/APIChanges.  MIght help figuring what needs fixing.;;;","11/Aug/08 19:14;apurtell;Relates to HBASE-800. ;;;","12/Aug/08 17:58;stack;See HBASE-822 for updates to HBase.thrift and to README to go against newer thrift.;;;","13/Aug/08 16:40;tow21;This extends the thrift API to allow retrieval of timestamp information.

A new thrift object is created: HCell, matching HBase's Cell, which is just a (timestamp, value) pair.

Two new methods are added to the thrift Client object: getCells and getCellsTs, which mirror the existing getVer & getVerTs.

The patch is against 0.2.0.

There are also some changes to ThriftServer.java required in order to bring it up to date with latest thrift - these mostly consist of changing ArrayList to List wherever appropriate, all of which is included in the patch. All of the generated files are also changed as a result of using latest thrift, also inside the patch.
;;;","13/Aug/08 16:42;tow21;This is the patch referred to in the previous comment.;;;","13/Aug/08 17:37;tim_s;I've been working on this.
There's a lot of things to update.

Exposing timestamps for cells.
  Change all the get functions to return TCell instead of Bytes and list<Bytes>.
  A TCell would be the thrift version of a Cell. Just as ColumnDescriptor is to HColumnDescriptor.
  with a value and a timestamp field.

Updating to new thrift versions
  Requires the ArrayList's and AbstractMap's to be List and Map. 
  Also the newest libthrift.jar would need to be replaced.

isEnabled, disable enable functions
  Add isTableEnabled function.
  Add enable and disableTable functions. So that a table can actually be deleted.

Fix the examples/thrift/DemoClient.java to match all the changes.

Expose more stuff with RegionDescriptor.
  Add endKey, id, name and version.
  I'm inclined that this should be changed to be called TRegionInfo (to match the Hbase class RegionInfo)

What about the put functions?
  The HTable as per 0.2 does not have put, they must be done through a BatchUpdate.
  The thrift equivalent is mutationRows(table, row, mutations) 
  Do we still want the put functions? Or remove them to match the api?
  They are less cumbersome, they create a BatchUpdate anyway with just a single put.

I wanted to have a patch to pick at today, but I ran out of time.
So comments on just the above?

;;;","13/Aug/08 17:39;tim_s;oops I was writing while you were posting. It's pretty much identical to what I did too. Except I was going to replace the existing get functions.
I think H as a prefix should be reserved for stuff part of HBase, rather then thrift stuff.;;;","13/Aug/08 17:46;stack;+1 on just replacing the methods that are there currently.
+1 on 'H' as prefix in hbase that doesn't have to show in thrift data structures
+1 on updating thrift (HBASE-822 has gone at least some of the ways).
+1 on TRegionInfo instad of RegionDescriptor
+1 on removing the put methods and using mutateionRow with commit

Thanks Toby and Tim for taking this on.;;;","13/Aug/08 18:22;tim_s;Cool, I'll add a patch for the above tomorrow, as soon as I can.;;;","13/Aug/08 18:40;stack;Excellent (Maybe Toby can verify it works?);;;","14/Aug/08 13:23;tim_s;Attached Patch.
Needs review.
If Toby and anyone could look this over and confirm it gives us what we need and conforms to the 0.2 api that would be grand :)

The examples/thrift/DemoClient.java has been updated.

Note, this includes the regenerated thrift files. Importantly I regenerated them using thrift release 20080411p1
which you can get http://developers.facebook.com/thrift/
Thrift is under apache incubation, but the latest stable release is still the last facebook one.

We will need to replace the libthrift jar file with this version. I will attach it for convenience.

There are still some things which could be improved. 
- The new RowLock stuff should be available via thrift. 
- All the non java examples need updating.
- We should really have a junit test covering all of ThriftServer.HBaseHandler.
;;;","14/Aug/08 18:40;tow21;From a brief review, this looks fine to me - but I'm not familiar with most of the HBase API, so don't take my word for it.

Certainly it serves my purposes well enough - I can now retrieve timestamps & delete tables.

A minor thing - the new thrift jar attached requires a newer version of java than I have on this computer (1.5.0_13), but I've run into no problems using the old thrift jar.

As a side issue, I've attached another patch for ThriftServer.java (apply on top of Tim's patch) to HBASE-657, which should probably be rolled into this.;;;","15/Aug/08 17:03;stack;Toby: Is it because the jar up on facebook was compiled w/ a 1.6JVM?  I tried downloading and recompiling w/ 1.5 JVM.  Generated jar is suspiciously small.  The content of this package, com/facebook/thrift/processor/, is missing from my built jar.  Agreed that the thing should run w/ a 1.5JVM if that is the issue since rest of hbase is based on 1.5 but I wouldn't put this in way of a commit.

Should I go with Tim's patch and just leave the old thrift jar for now?  That works for you Toby and Tim?

Should  Imake issue to add unit tests for HBaseHandler? IIRC, you don't have to put up thrift servers to exercise this class so should be easy to add.

Tim: Looking at patch, ignoring the generated code (yuck), DemoClient contains a class named 'CopyOfDemoClient'.  Thats a little odd (I can fix on commit if you like).  And you don't have RowResult or a RowResult-like thing in your IDL.  Not needed?  The thrift package.html hasn't been updated (I can do that on commit too).  Otherwise, patch looks great.

Good stuff lads.

;;;","15/Aug/08 17:58;tim_s;Hmm, oops the attached libthrift jar might have been compiled on my machine using java 1.6. sorry about that. I'll remove that file.
We should use the jar from facebook.

Stack: Oops, sorry about the CopyOfDemoClient, I guess I lost track of things somewhere. So yes that needs fixing. 
About RowResult, looking over the code again, I realise the RowResult is the analogue of ScanEntry, and this should also be returned for getRow functions (even though we know what row we are getting). ScanEntry should really be called TRowResult I guess. Want me to fix these and stick a new patch up tomorrow?;;;","15/Aug/08 18:37;stack;Toby: Any chance of your compiling jar using the facebook 20080411p1 tarball?  It looks like things compile fine but I'm suspicious because a whole package is missing (maybe I need to do more than just do 'an' in lib/java').

Tim: I think a TRowResult will make for less confusion so if you don't mind (scanentry was created before rowresult).  That'd be a nice fixup.  I can do the others no problem.

Thanks;;;","16/Aug/08 15:06;tim_s;- Added TRowResult.
- Added to ThriftUtilities function for converting RowResult to TRowResult.
- Fixed silly, CopyOfDemoClient from my other patch.
- Updated thrift/package.html
;;;","16/Aug/08 22:14;stack;Thanks for the patch Tim.  I applied to branch and trunk.   Also added in the new jar built by me using JVM 1.5 against a check out of 20080411p1 (under the lib/java dir).  If it don't work, lets file new issues.;;;","16/Aug/08 22:19;stack;Tim: I made issues to cover your suggestions above made at the end of your 'Tim Sell - 14/Aug/08 06:23 AM' comment.
;;;","17/Aug/08 14:22;tim_s;Just want to confirm that with the new libthrift jar it works fine for me.;;;","17/Aug/08 20:30;stack;Assigning Tim.  He fixed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HStore.rowAtOrBeforeFromMapFile() fails to locate the row if # of mapfiles >= 2,HBASE-694,12398398,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,bryanduxbury,rafan,rafan,17/Jun/08 06:30,22/Aug/08 21:13,01/Jul/25 07:49,18/Jun/08 16:28,0.2.0,,,,,0.2.0,,,regionserver,,,,0,"After HBASE-528 committed, a misplaced return statement and } cause 
rowAtOrBeforeFromMapFile() never look into 2nd (and latter) MapFile
if candidateKeys.firstKey() <= map.finalKey().",Hadoop 0.17.0 release + HBase trunk ,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/08 14:45;bryanduxbury;694-v2.patch;https://issues.apache.org/jira/secure/attachment/12384217/694-v2.patch","17/Jun/08 06:32;rafan;694.diff;https://issues.apache.org/jira/secure/attachment/12384107/694.diff",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25340,,,,,Wed Jun 18 16:28:47 UTC 2008,,,,,,,,,,"0|i0h8vj:",98700,,,,,,,,,,,,,,,,,,,,,"17/Jun/08 06:32;rafan;- add a unit test case for this problem
- propose a fix for HStore.java;;;","17/Jun/08 06:34;rafan;patch available;;;","17/Jun/08 15:23;rafan;This bug will cause client see WrongRegionException if META has 
more than one MapFile. And this can only be solved after the META
is compacted (so we have only one file).;;;","18/Jun/08 01:41;bryanduxbury;This is a pretty big issue. If META is flushing often enough, then you'll get really mixed up results at times. 

I think part of the problem here is that the rowAtOrBeforeFromMapfile method is far too large and unruly. Maybe it should be factored into a few smaller methods?

I'm going to run the unit tests later tonight and possibly take a crack at the refactoring.;;;","18/Jun/08 14:45;bryanduxbury;Here's the patch with a little refactoring in it.

In general, I like the patch. I think it's fixing an issue we have. However, when I run the full test suite, I get a few weird errors. TestRegionRebalancing and TestMetaUtils both fail. Are other people seeing this problem when running the test suite?;;;","18/Jun/08 15:31;stack;Welcome back Bryan.

TRB is failing pretty consistently.  It needs looking at.  TMU usually passes IIRC.  If you run tests w/o this patch do you have same failures?;;;","18/Jun/08 16:28;bryanduxbury;I just committed the v2 patch to TRUNK. Thanks Rong-En!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Method expecting HBaseConfiguration throw NPE when given Configuration,HBASE-688,12381885,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,stack,stuhood,stuhood,06/Nov/07 05:45,14/Jun/08 22:37,01/Jul/25 07:49,06/Nov/07 20:47,0.16.0,,,,,0.1.0,,,,,,,0,"The HBaseAdmin constructor takes a Configuration object, but if passed a Hadoop Configuration, will throw a very unhelpful NPE. Instead, it requires a HBaseConfiguration, and should explicitly say so.","Ubuntu Gutsy, Java 6u3",,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/07 08:33;stack;2156.patch;https://issues.apache.org/jira/secure/attachment/12369011/2156.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25335,,,,,Wed Nov 07 12:27:53 UTC 2007,,,,,,,,,,"0|i0h8u7:",98694,,,,,,,,,,,,,,,,,,,,,"06/Nov/07 08:33;stack;Changed Configuration everywhere so we're explicit that its an HBaseConfiguration thats needed rather than being lazy taking Configuration.;;;","06/Nov/07 08:45;stack;Passes tests locally;;;","06/Nov/07 19:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12369011/2156.patch
against trunk revision r592324.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests -1.  The patch failed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1069/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1069/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1069/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1069/console

This message is automatically generated.;;;","06/Nov/07 20:47;stack;Committed.;;;","07/Nov/07 12:27;hudson;Integrated in Hadoop-Nightly #296 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/296/]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MemcacheScanner didn't return the first row(if it exists), cause HScannerInterface's output incorrect",HBASE-686,12398270,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,ln@webcate.net,ln@webcate.net,14/Jun/08 08:12,22/Aug/08 21:13,01/Jul/25 07:49,17/Jun/08 20:52,0.1.2,0.2.0,,,,0.1.3,0.2.0,,,,,,3,"HTable.obtainScanner methods should return the start row if it exists, although HTable's javadoc didn't clearly desc. but i found the result of htable scanners sometimes contain the start row, sometimes not.

after more testing and code review, i found it should be a bug in HStore.Memcache.MemcacheScanner. in the constructor it set this.currentRow = firstRow, but when doing next(), there's a this.currentRow = getNextRow(this.currentRow) before fetch result.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/08 05:33;ln@webcate.net;HBASE-686.patch;https://issues.apache.org/jira/secure/attachment/12384105/HBASE-686.patch","17/Jun/08 01:14;jimk;HBASE_686.java;https://issues.apache.org/jira/secure/attachment/12384097/HBASE_686.java","17/Jun/08 01:09;jimk;HBASE_686.java;https://issues.apache.org/jira/secure/attachment/12384096/HBASE_686.java","17/Jun/08 05:33;ln@webcate.net;TestHMemcache.java;https://issues.apache.org/jira/secure/attachment/12384104/TestHMemcache.java","17/Jun/08 12:37;acure;scanner-test-for.ver.0.2.0.dev.jar.jar;https://issues.apache.org/jira/secure/attachment/12384123/scanner-test-for.ver.0.2.0.dev.jar.jar",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25334,,,,,Tue Jun 17 20:52:04 UTC 2008,,,,,,,,,,"0|i0h8tr:",98692,,,,,,,,,,,,,,,,,,,,,"14/Jun/08 08:17;ln@webcate.net;a testcase for this issue, i add this to TestHMemcache.java for reproduce and validating my patch.
    /**
     * Test memcache scanner scanning cached rows, see HBASE-686
     * @throws IOException
     */
    public void testScanner_686() throws IOException
    {
        addRows(this.hmemcache);
        long timestamp = System.currentTimeMillis();
        Text[] cols = new Text[COLUMNS_COUNT * ROW_COUNT];
        for (int i = 0; i < ROW_COUNT; i++)
        {
            for (int ii = 0; ii < COLUMNS_COUNT; ii++)
            {
                cols[(ii + (i * COLUMNS_COUNT))] = getColumnName(i, ii);
            }
        }
        //starting from each row, validate results should contain the starting row
        for (int startRowId = 0; startRowId < ROW_COUNT; startRowId++)
        {
            HInternalScannerInterface scanner =
                    this.hmemcache.getScanner(timestamp, cols
                            , new Text(getRowName(startRowId)));
            HStoreKey key = new HStoreKey();
            TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();
            for (int i = 0; scanner.next(key, results); i++)
            {
                int rowId = startRowId + i;
                assertTrue(""Row name"",
                        key.toString().startsWith(getRowName(rowId).toString()));
                assertEquals(""Count of columns"", COLUMNS_COUNT,
                        results.size());
                TreeMap<Text, byte[]> row = new TreeMap<Text, byte[]>();
                for (Map.Entry<Text, byte[]> e : results.entrySet())
                {
                    row.put(e.getKey(), e.getValue());
                }
                isExpectedRow(rowId, row);
                // Clear out set.  Otherwise row results accumulate.
                results.clear();
            }
        }
    }
;;;","14/Jun/08 08:19;ln@webcate.net;my patch, due to my local source changed for HBASE-684, the line number is incorrect i think.

Index: src/java/org/apache/hadoop/hbase/HStore.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HStore.java	Sat Jun 14 15:46:26 CST 2008
+++ src/java/org/apache/hadoop/hbase/HStore.java	Sat Jun 14 15:46:26 CST 2008
@@ -631,8 +631,7 @@
        if (results.size() > 0) {
          results.clear();
        }
-       while (results.size() <= 0 &&
-           (this.currentRow = getNextRow(this.currentRow)) != null) {
+       while (results.size() <= 0 && this.currentRow != null) {
          if (deletes.size() > 0) {
            deletes.clear();
          }
@@ -661,6 +660,7 @@
            }
            results.put(column, c);
          }
+         this.currentRow = getNextRow(this.currentRow);
        }
        return results.size() > 0;
      }
;;;","16/Jun/08 21:09;jimk;LN,

Could you please post your testcase and patch as attachments to this Jira. Jira has mangled your input to the extent that it is difficult to make a working test program or patch out of either of your posts.

Thanks.
;;;","16/Jun/08 21:23;acure;    My test case :

        you have to create table 'aaa' with a column family 'xxx'


		HTable htable = new HTable(""aaa"");
		  BatchUpdate batch = new BatchUpdate(""123456"".getBytes());
		  batch.put(""xxx"", ""some value"".getBytes());
		  htable.commit(batch);
		
		  byte[][] cols = new byte[][]{ ""xxx"".getBytes() };		  		   
		  RowResult row = htable.getRow(""123456"".getBytes(), cols, new Date().getTime());
		  System.out.println(""there is a row = "" + row.get(""xxx"".getBytes()).toString());
		
		  cols = new byte[][]{ ""xxx:"".getBytes() };		  
		  Scanner scanner = htable.getScanner(cols);
		  
		  if (scanner.iterator().hasNext()) {
			  System.out.println("" GOOD !!!! - scanner returns some rows "");
		  } else {
			  System.out.println("" WRONG !!!! - scanner didn't found any rows"");
		  }		  

;;;","16/Jun/08 21:34;jimk;This patch is listed as affecting hbase-0.1.2, but the APIs you are using only exist in trunk (0.2.0) is this issue for 0.1.2 or for 0.2.0?;;;","17/Jun/08 01:09;jimk;From the comments in the issue, this is the best test case I could come up with. It passes tests on hbase-0.1 branch;;;","17/Jun/08 01:14;jimk;This is a updated version of the test to work with trunk. Again, it passes with no issues.;;;","17/Jun/08 01:22;jimk;This issue cannot be replicated on either the 0.1 branch or trunk. If there are explicit test cases attached which can reproduce the issue, please re-open with both the versions affected and please attach a test case that demonstrates that the test case fails for that release.;;;","17/Jun/08 05:33;ln@webcate.net;here is a modified memcache test cases(0.1.2 release) and my patch, i have add a new case 'testScanner_686'.  i also have a small ant task definition to run this case seprately if u need.
;;;","17/Jun/08 05:56;ln@webcate.net;i can't understand why this issue resolved as CR in 9 hours(before i wake up:-). i HAVE post my testcase when opening this issue, with clearly piont out it is part of TestHMemcache.java, which is the 0.1.2 test source about memcache. 

and, the mistake in the source should be wrong iteration control, as i commented in the issue description.

junit result here:
Testsuite: org.apache.hadoop.hbase.TestHMemcache
Tests run: 7, Failures: 1, Errors: 0, Time elapsed: 0.22 sec

Testcase: testSnapshotting took 0.16 sec
Testcase: testGetFull took 0.01 sec
Testcase: testGetNextRow took 0.01 sec
Testcase: testGetClosest took 0 sec
Testcase: testScanner_686 took 0.01 sec
	FAILED
Row name
junit.framework.AssertionFailedError: Row name
	at org.apache.hadoop.hbase.TestHMemcache.testScanner_686(TestHMemcache.java:195)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

Testcase: testScanner took 0.01 sec
Testcase: testGetRowKeyAtOrBefore took 0.02 sec
;;;","17/Jun/08 12:37;acure;it's test case for ver 0.2.0 dev  from trunk. - sources inside the jar package

  this is class with main method which do : 
  1) creates table - if it doesn't exist
  2) create BatchUpdate with a ""rowKey"" and commit into a table
  3) make - getRow with ""rowKey"" - and shows on console data in this row
  4) make - getScanner on whole table  and shows all rows - and counts how many rows scanner returns

it returns :

table : testTable exists. 
 data inserted 
-------------------------------------
 getRow : 
  isEmpty = false
  row key = 0000
  col val = testvalue
-------------------------------------
 getScanner : 
scanner find : 0 rows in this table.
 done. 

 what does it mean ?
getRow can gets data by rowKey - this data exists in table, but the scanner doesn't see this row - and return 0 rows.

   Antoni;;;","17/Jun/08 17:14;jimk;Mea culpa on closing this issue too early.

The patch and test case provided do indeed demonstrate and fix the problem. All other regression tests passed as well with the patch applied. Committed to 0.1 branch. Thanks for the contribution LN!;;;","17/Jun/08 18:33;krzysiek;It also affects 0.2 from trunk as test case provided by Antonii (in scanner-test-for.ver.0.2.0.dev.jar) shows;;;","17/Jun/08 20:52;jimk;Committed to both 0.1 branch and trunk. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can not get svn revision # at build time if locale is not english,HBASE-683,12398208,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,rafan,rafan,rafan,13/Jun/08 04:12,22/Aug/08 21:13,01/Jul/25 07:49,19/Jun/08 03:51,0.2.0,,,,,0.2.0,,,build,,,,0,"My locale is zh_TW.UTF-8, so 'svn info' shows messages in chinese. But
src/saveVersion.sh expects english from output.

I suggest that we add clear LANG, LC_* in saveVersion.sh before calling svn.
",,,,,,,,,,,,,,,,,,,,,,,HADOOP-3819,,,,,,"19/Jun/08 02:45;rafan;683.diff;https://issues.apache.org/jira/secure/attachment/12384247/683.diff",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25333,,,,,Thu Jun 19 03:51:55 UTC 2008,,,,,,,,,,"0|i0h8t3:",98689,,,,,,,,,,,,,,,,,,,,,"18/Jun/08 20:56;stack;Can you make a patch Rong-En to do the above please so I can test and commit if alls well?;;;","19/Jun/08 02:45;rafan;here is the patch. I believe this also applies to Hadoop.;;;","19/Jun/08 03:48;stack;Tested.  Doesn't break anything.  Committed.  Thanks for the patch Rong-En.

Regards hadoop, open a new issue against hadoop (if the same issue exists there).  Thanks.;;;","19/Jun/08 03:51;stack;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in Memcache,HBASE-681,12398195,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,clint.morgan,clint.morgan,12/Jun/08 21:06,22/Aug/08 21:13,01/Jul/25 07:49,16/Jun/08 19:00,0.1.2,0.1.3,0.2.0,,,0.1.3,0.2.0,,regionserver,,,,0,"java.io.IOException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.Memcache.internalGetKeys(Memcache.java:585)
	at org.apache.hadoop.hbase.regionserver.Memcache.getKeys(Memcache.java:551)
	at org.apache.hadoop.hbase.regionserver.HStore.getKeys(HStore.java:1437)
	at org.apache.hadoop.hbase.regionserver.HRegion.getKeys(HRegion.java:1243)
	at org.apache.hadoop.hbase.regionserver.HRegion.deleteMultiple(HRegion.java:1498)
	at org.apache.hadoop.hbase.regionserver.HRegion.deleteAll(HRegion.java:1424)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteAll(HRegionServer.java:1266)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:424)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:867)

",,,,,,,,,,,,,,,,,,HBASE-613,,,,,,,,,,,"12/Jun/08 21:08;clint.morgan;hbase-681.patch;https://issues.apache.org/jira/secure/attachment/12383932/hbase-681.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25332,,,,,Mon Jun 16 19:00:01 UTC 2008,,,,,,,,,,"0|i0h8sn:",98687,,,,,,,,,,,,,,,,,,,,,"12/Jun/08 21:08;clint.morgan;this fixed it for me;;;","15/Jun/08 20:49;jimk;This will be incorporated for the change for HBASE-613 for release 0.2.0;;;","16/Jun/08 18:56;jimk;Committed to 0.1 branch for inclusion in new 0.1.3 release candidate. Thanks for the patch Clint!;;;","16/Jun/08 19:00;jimk;Committed for 0.1.3.

Resolving issue because this change for 0.2.0 will be incorporated in HBASE-613;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memcache size unreliable,HBASE-674,12397840,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,stack,stack,09/Jun/08 20:54,22/Aug/08 21:13,01/Jul/25 07:49,04/Jul/08 19:16,0.1.2,,,,,0.2.0,,,,,,,0,"Multiple updates against same row/column/ts will be seen as increments to cache size on insert but when we then play the memcache at flush time, we'll only see the most recent entry and decrement the memcache size by whatever its size; memcache will be off.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/08 22:49;stack;674-v2.patch;https://issues.apache.org/jira/secure/attachment/12383720/674-v2.patch","09/Jun/08 22:09;stack;674.patch;https://issues.apache.org/jira/secure/attachment/12383716/674.patch","04/Jul/08 03:13;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12385266/patch.txt",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25328,,,,,Fri Jul 04 19:16:56 UTC 2008,,,,,,,,,,"0|i0h8r3:",98680,,,,,,,,,,,,,,,,,,,,,"09/Jun/08 21:18;stack;For example:

{code}
  public void testSizeCount() throws Exception {
    HStoreKey hsk = new HStoreKey(new Text(getName()),
      new Text(getName()), System.currentTimeMillis());
    for (int i = 0; i < 3; i++) {
      this.hmemcache.add(hsk, HStoreKey.getBytes(hsk));
    }
    this.hmemcache.snapshot();
    System.out.println(this.hmemcache.getSnapshot().size());
  }
{code}

The out.println in above says only one entry in the memcache though adding we added 3 items to the memcache size.

Other issues here are that exceptions adding/deleting, etc., items can cause count to be off.  We add to stuff to the memcache size before successful add of item.;;;","09/Jun/08 21:30;stack;Issue is a regionserver that is stuck with the block gate down.  I can see it flushing over time but the memcache size continues to crawl until its at maximum and then never goes down in spite of fact we've been regularly flushing.  Our math is obviously off... See here:

{code}
2008-06-05 23:29:54,872 DEBUG org.apache.hadoop.hbase.HRegion: Started memcache flush for region enwiki_meta,4xm9tOa_JLlpDI7EFNz4OF==,1212282770124. Current region memcache size 10.7m
2008-06-05 23:29:56,246 DEBUG org.apache.hadoop.hbase.HStore: Added /hbase/aa0-005-2.u.powerset.com/enwiki_meta/1966274647/alternate_title/mapfiles/7141575363599707225 with 4 entries, sequence id 1109246475, data size 290.0
2008-06-05 23:29:56,444 DEBUG org.apache.hadoop.hbase.HStore: Added /hbase/aa0-005-2.u.powerset.com/enwiki_meta/1966274647/misc/mapfiles/8160270909949078904 with 39 entries, sequence id 1109246475, data size 2.3k
2008-06-05 23:29:56,661 DEBUG org.apache.hadoop.hbase.HStore: Added /hbase/aa0-005-2.u.powerset.com/enwiki_meta/1966274647/alternate_url/mapfiles/3974349317844214611 with 4 entries, sequence id 1109246475, data size 398.0
2008-06-05 23:29:56,889 DEBUG org.apache.hadoop.hbase.HStore: Added /hbase/aa0-005-2.u.powerset.com/enwiki_meta/1966274647/page/mapfiles/5684117091377129929 with 117 entries, sequence id 1109246475, data size 87.0k
2008-06-05 23:29:56,890 DEBUG org.apache.hadoop.hbase.HRegion: Finished memcache flush for region enwiki_meta,4xm9tOa_JLlpDI7EFNz4OF==,1212282770124 in 2019ms, sequence id=1109246475
{code}

See how we are at 10.7M when flush starts but how if you count up all that was flushed, we flushed 90k odd.;;;","09/Jun/08 21:36;jimk;Good catch! 

We should probably update the memcache size with each update, and if we are overwriting a previous update we need to take that into account.;;;","09/Jun/08 21:42;ningli;Would it be better if HStore's mem cache computes/maintains its own memory size/usage? When a region needs its memory size, it sums from all its stores instead increasing a count during update and subtracting after flush.;;;","09/Jun/08 21:52;stack;Ning: Agree.

I think for 0.1 branch, I'll just set memcache size to zero after successful flush. Will do better fix for 0.2.;;;","09/Jun/08 22:09;stack;Here's a crude patch for 0.1; just zeros memcache size on successful flush (needed internally).  Lets do something better in TRUNK.;;;","09/Jun/08 22:33;stack;Committed patch to branch.;;;","09/Jun/08 22:49;stack;I applied to branch something that had fewer changes after tesitng on cluster to make sure flushing worked as it used to.;;;","30/Jun/08 19:51;jimk;While this is an important issue, it is not a blocker for 0.2.0;;;","01/Jul/08 20:54;viper799;I thank this should be a blocker for 2.0 as it causes problems on long running jobs and region server that have been online for a while.

What I am seeing is the last line of the flush like the below log lines has the memcache size on the end after the flush. this grows with every flush that has data in it. 

The problem comes up with the region server thanks the memcache is > then hbase.hregion.memcache.flush.size then it starts flushing back to back with little or nothing 
to flush when it starts doing this on my servers the region server hangs and is needed to be restarted after killing it with a kill -9 pid.

Also a side note I do see flushes back to back on region server that only have one or two regions I see a memcache flush then 1-3 more of the same region back to back then its ok but this seams to go away after the region server host more then 4 region.

Logs from a few days ago:
{code}
2008-06-25 21:02:20,632 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 10604ms, sequence id=1973821, 17.0m
2008-06-25 21:02:20,633 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 1ms, sequence id=1973822, 17.0m
2008-06-25 21:02:21,478 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 842ms, sequence id=1973832, 17.4m
2008-06-25 21:02:22,896 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 1408ms, sequence id=1976711, 17.8m
2008-06-25 21:11:20,979 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 16988ms, sequence id=3827578, 52.0m
hbase restarted
2008-06-25 21:16:18,004 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 10336ms, sequence id=4817365, 17.0m
2008-06-25 21:16:18,408 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 404ms, sequence id=4817378, 17.0m
2008-06-25 21:16:19,838 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 1421ms, sequence id=4817952, 17.7m
2008-06-25 21:19:01,512 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 9692ms, sequence id=5661153, 32.8m
2008-06-25 21:19:01,821 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 308ms, sequence id=5661169, 32.8m
2008-06-25 21:19:04,261 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 2439ms, sequence id=5661523, 33.4m
2008-06-25 21:19:05,317 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 1052ms, sequence id=5666391, 33.5m
2008-06-25 21:49:21,616 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 14214ms, sequence id=7415697, 64.7m
2008-06-25 21:49:22,369 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 738ms, sequence id=7415798, 64.7m
2008-06-25 21:49:22,373 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214427652459 in 4ms, sequence id=7415800, 64.7m
hbase restarted
2008-06-25 22:54:33,665 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 11168ms, sequence id=9238125, 19.5m
2008-06-25 22:54:34,248 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 582ms, sequence id=9241589, 19.7m
2008-06-25 22:54:34,989 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 740ms, sequence id=9242882, 19.8m
2008-06-25 22:54:35,749 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 759ms, sequence id=9244773, 19.8m
2008-06-25 22:54:36,619 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 869ms, sequence id=9247214, 19.8m
2008-06-25 22:54:37,657 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 1031ms, sequence id=9249946, 20.0m
2008-06-25 23:14:29,857 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 13549ms, sequence id=12776517, 53.0m
hbase restart
2008-06-25 23:21:25,013 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 13706ms, sequence id=14457037, 20.2m
2008-06-25 23:21:25,508 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 494ms, sequence id=14474826, 20.2m
2008-06-25 23:21:26,353 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 844ms, sequence id=14475821, 20.3m
2008-06-25 23:21:27,208 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 854ms, sequence id=14478027, 20.3m
2008-06-25 23:21:27,816 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 607ms, sequence id=14481138, 20.3m
2008-06-25 23:21:28,609 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 791ms, sequence id=14483523, 20.3m
2008-06-25 23:21:29,234 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 623ms, sequence id=14485701, 20.3m
2008-06-25 23:21:31,459 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 2223ms, sequence id=14487619, 20.6m
2008-06-25 23:21:32,546 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 1079ms, sequence id=14496978, 20.8m
2008-06-25 23:24:36,701 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 3432ms, sequence id=15103902, 27.4m
hbase restarted
2008-06-25 23:28:18,288 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214448561806 in 1ms, sequence id=15103936, 0.0
2008-06-25 23:28:19,151 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214454498285 in 0ms, sequence id=15103937, 0.0
2008-06-25 23:45:43,021 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214454498285 in 14039ms, sequence id=19210706, 23.6m
2008-06-25 23:45:43,285 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214454498285 in 264ms, sequence id=19226471, 23.6m
2008-06-25 23:45:44,016 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214454498285 in 730ms, sequence id=19227141, 23.6m
2008-06-25 23:45:45,481 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214454498285 in 1464ms, sequence id=19229633, 23.7m
2008-06-25 23:45:46,474 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214454498285 in 990ms, sequence id=19234660, 23.8m
2008-06-25 23:47:56,208 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,com.wallbuilders.www%2FLIBissuesArticles.asp%3Fid%3D45%3Ahttp467123 in 759ms, sequence id=19961148, 21.7m
2008-06-25 23:55:07,552 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214454498285 in 5795ms, sequence id=22333011, 41.4m
hbase restarted
{code};;;","01/Jul/08 20:56;viper799;I currently run jobs in smaller batches and restart hbase after abotu 20 small jobs inserting about 500K cells per job on one region server.

If we want to get 2.0 out then this should be a blocker in 2.1;;;","02/Jul/08 06:03;stack;Bringing this back into 0.2 because of the Billy comments.  Should be easy enough to reproduce.;;;","02/Jul/08 23:46;jimk;There are a number of issues here:
- multiple inserts or deletes for the same row/colum/timestamp are counted and can inflate the memcache size some. This may not be a big issue because it is unlikely that someone is using the same row/column/timestamp especially if they do not specify a timestamp for puts or deletes.
- because of the inaccuracies of the above, subtracting the actual number of flushed bytes from the memcache size leads to the potential of the memcache size growing over time if fewer bytes are flushed than what HRegion thinks is is the memcache. What we really need to do is keep track of both updates and memcache size, so that during a flush, we accumulate the size of updates that are taken after the snapshot. When the flush is completed, we can set the size of the memcache to the number of bytes submitted as updates during the flush.
- why the memcache size seems to be going negative more frequently recently is somewhat of a mystery. It is pretty easy to understand why we might flush less than what we think is in the cache, but how would we flush more than what we think is in the cache.
- Finally I don't particularly like the finished memcache flush message in HRegion. It reports what it thinks is the current memcache size after the flush, but doesn't say that. It would lead the casual observer to think that the size reported by HRegion after the flush is the number of bytes flushed from the cache.;;;","04/Jul/08 03:13;jimk;Memcache.add now computes the delta size of the memcache (so if multiple updates are made to the same row/column/timestamp, they are correctly accounted for)

HStore.add returns the value from Memcache.add

HRegion.internalFlushCache now zeros the memcache size while it has updates locked out. Because of this, the memcache size will reflect the size of the updates that happened since the flush started. Additionally, at the end of a cache flush it reports the number of bytes flushed and not the number of bytes currently in the memcache.

memcache size is now updated based on the value returned from HStore.add (which is computed by Memcache.add

;;;","04/Jul/08 03:14;jimk;patch available. Please review.;;;","04/Jul/08 08:27;viper799;I changed my flush size to 16MB from my default 128MB and run a large job here is some lines from the logs
also I added back StringUtils.humanReadableInt(this.memcacheSize.get()) on the end so I could see if the size was growing after every flush

{code}
2008-07-04 03:09:46,684 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214968800601 in 5595ms, sequence id=237700684, 12.9m, 351.2k
2008-07-04 03:15:02,169 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214968800601 in 5741ms, sequence id=239128833, 13.4m, 188.1k
2008-07-04 03:15:04,145 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214968800601 in 1975ms, sequence id=239155758, 167.6k, 222.8k
{code}

so the above looks good now the last number is the memcacheSize.get() and its moveing down and up so thats good to see I thank this patch solved my problem of the flushes.
I run the job for quite a while and flushes seams to happen normal in place of less time between flushes.

The only other thing I see on this issue is the size before flush of the memcache is still off like the logs stack posted above

{code}
2008-07-04 03:09:41,089 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region webdata,,1214968800601. Current region memcache size 16.0m
2008-07-04 03:09:42,587 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/webdata/1748955538/anchor/mapfiles/5595070426400799233 with 29142 entries, sequence id 237700684, data size 3.7m, file size 507.2k
2008-07-04 03:09:43,719 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/webdata/1748955538/stime/mapfiles/4943615301545296809 with 2872 entries, sequence id 237700684, data size 154.9k, file size 24.3k
2008-07-04 03:09:45,197 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/webdata/1748955538/in_rank/mapfiles/367761225010760821 with 36409 entries, sequence id 237700684, data size 4.5m, file size 415.8k
2008-07-04 03:09:45,470 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/webdata/1748955538/size/mapfiles/6451240725630689572 with 2872 entries, sequence id 237700684, data size 135.9k, file size 28.4k
2008-07-04 03:09:46,683 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/webdata/1748955538/last_seen/mapfiles/3472123077784461203 with 36410 entries, sequence id 237700684, data size 4.4m, file size 409.3k
2008-07-04 03:09:46,684 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region webdata,,1214968800601 in 5595ms, sequence id=237700684, 12.9m, 351.2k
{code}

16m is whats reported as the size before the flush and the total data flushed was 12.9m;;;","04/Jul/08 19:12;jimk;Computing the memcache size is an approximation at best. If the memcache size is not growing over time, and causing OutOfMemoryExceptions, then I think this the best we can do for 0.2.0. If you think the accounting should be more accurate, please open an issue for 0.3.0.;;;","04/Jul/08 19:16;jimk;Committed patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HLog#cacheFlushLock not cleared; hangs a region",HBASE-659,12397203,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,30/May/08 20:35,22/Aug/08 21:13,01/Jul/25 07:49,01/Jun/08 05:21,0.1.2,,,,,0.1.3,0.2.0,,,,,,0,"I have a region that is stuck in a close that was ordained by a split.  Here is what I have from the log pertaining to the stuck region:

{code}
4    6416 2008-05-29 22:29:03,433 INFO org.apache.hadoop.hbase.HRegion: checking compaction completed on region enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061 in 12sec
5    6417 2008-05-29 22:29:03,439 INFO org.apache.hadoop.hbase.HRegion: Splitting enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061 because largest aggregate size is 288.3m and desired size is 256.0m                                                                    
6    6418 2008-05-29 22:29:03,443 DEBUG org.apache.hadoop.hbase.HRegion: compactions and cache flushes disabled for region enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061
7    6419 2008-05-29 22:29:03,443 DEBUG org.apache.hadoop.hbase.HRegion: new updates and scanners for region enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061 disabled                                                                                                    
8    6420 2008-05-29 22:29:03,443 DEBUG org.apache.hadoop.hbase.HRegion: no more active scanners for region enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061
9    6421 2008-05-29 22:29:03,443 DEBUG org.apache.hadoop.hbase.HRegion: no more row locks outstanding on region enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061                                                                                                        
10   6422 2008-05-29 22:29:03,443 DEBUG org.apache.hadoop.hbase.HRegionServer: enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061 closing (Adding to retiringRegions)
11   6423 2008-05-29 22:29:03,443 DEBUG org.apache.hadoop.hbase.HRegion: Started memcache flush for region enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061. Current region memcache size 2.1m                                                                           
12    6424 2008-05-29 22:29:03,561 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 60020, call batchUpdate(enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061, 1171081390000, org.apache.hadoop.hbase.io.BatchUpdate@2eeb0275) from 208.76.44.139:49358: err        or: org.        apache.hadoop.hbase.NotServingRegionException: enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061                                                                                                                                                        
13   6425 org.apache.hadoop.hbase.NotServingRegionException: enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061
14   6434 2008-05-29 22:29:03,982 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 60020, call batchUpdate(enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061, 1202595259000, org.apache.hadoop.hbase.io.BatchUpdate@46ee6763) from 208.76.44.139:49358: err        or: org.        apache.hadoop.hbase.NotServingRegionException: enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061
15   6435 org.apache.hadoop.hbase.NotServingRegionException: enwiki,IK9sWdHJe6ffGZgFPsqIvk==,1212092907061
{code}

Then in thread dump, I have two threads blocked on the HLog#cacheFlushLock but looking in code, there is no obvious code path that would get a situation where a lock is held and then not released.

{code}
""regionserver/0:0:0:0:0:0:0:0:60020.compactor"" daemon prio=1 tid=0x00002aab381e5fd0 nid=0x6195 waiting on condition [0x0000000041c6c000..0x0000000041c6ce00]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock.lock(Unknown Source)
        at org.apache.hadoop.hbase.HLog.startCacheFlush(HLog.java:459)
        at org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:1089)
        at org.apache.hadoop.hbase.HRegion.close(HRegion.java:594)
        - locked <0x00002aaab70bf3a0> (a java.lang.Integer)
        at org.apache.hadoop.hbase.HRegion.splitRegion(HRegion.java:759)
        - locked <0x00002aaab70bf3a0> (a java.lang.Integer)
        at org.apache.hadoop.hbase.HRegionServer$CompactSplitThread.split(HRegionServer.java:248)
        at org.apache.hadoop.hbase.HRegionServer$CompactSplitThread.run(HRegionServer.java:204)

...


""regionserver/0:0:0:0:0:0:0:0:60020.logRoller"" daemon prio=1 tid=0x00002aab38181d70 nid=0x6193 waiting on condition [0x0000000041a6a000..0x0000000041a6ab00]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock.lock(Unknown Source)
        at org.apache.hadoop.hbase.HLog.rollWriter(HLog.java:219)
        at org.apache.hadoop.hbase.HRegionServer$LogRoller.run(HRegionServer.java:615)
        - locked <0x00002aaab69ccf00> (a java.lang.Integer)
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/08 23:23;stack;659-0.1.patch;https://issues.apache.org/jira/secure/attachment/12383177/659-0.1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25319,,,,,Sun Jun 01 05:21:36 UTC 2008,,,,,,,,,,"0|i0h8nr:",98665,,,,,,,,,,,,,,,,,,,,,"30/May/08 20:57;stack;Here's the thread dump on exit.  Gives a clean view of the deadlock.  Regionserver will not go down when we have this condition, not without a kill -9:

{code}
Full thread dump Java HotSpot(TM) 64-Bit Server VM (1.5.0_13-b05 mixed mode):

""Thread-8"" prio=1 tid=0x00002aab3c696e60 nid=0x38db in Object.wait() [0x0000000042f7f000..0x0000000042f7fc80]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab69a2e90> (a org.apache.hadoop.hbase.HRegionServer$CompactSplitThread)
        at java.lang.Thread.join(Unknown Source)
        - locked <0x00002aaab69a2e90> (a org.apache.hadoop.hbase.HRegionServer$CompactSplitThread)
        at java.lang.Thread.join(Unknown Source)
        at org.apache.hadoop.hbase.HRegionServer.join(HRegionServer.java:1081)
        at org.apache.hadoop.hbase.HRegionServer.join(HRegionServer.java:1074)
        at org.apache.hadoop.hbase.HRegionServer$ShutdownThread.run(HRegionServer.java:158)
        
""SIGTERM handler"" daemon prio=1 tid=0x00002aab37d61eb0 nid=0x38d8 in Object.wait() [0x0000000043282000..0x0000000043282e00]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab693ad28> (a org.apache.hadoop.hbase.HRegionServer$ShutdownThread)
        at java.lang.Thread.join(Unknown Source)
        - locked <0x00002aaab693ad28> (a org.apache.hadoop.hbase.HRegionServer$ShutdownThread)
        at java.lang.Thread.join(Unknown Source)
        at java.lang.Shutdown.runHooks(Unknown Source)
        at java.lang.Shutdown.sequence(Unknown Source)
        at java.lang.Shutdown.exit(Unknown Source)
        - locked <0x00002aaab1bdced8> (a java.lang.Class)
        at java.lang.Terminator$1.handle(Unknown Source)
        at sun.misc.Signal$1.run(Unknown Source) 
        at java.lang.Thread.run(Unknown Source)
        
""IPC Client connection to coral-dfs.cluster.powerset.com/208.76.44.135:10000"" daemon prio=1 tid=0x00002aab3c5157d0 nid=0x7db5 runnable [0x0000000043585000..0x0000000043585c80]
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(Unknown Source)
        at java.io.FilterInputStream.read(Unknown Source)
        at org.apache.hadoop.ipc.Client$Connection$1.read(Client.java:190)
        at java.io.BufferedInputStream.fill(Unknown Source)
        at java.io.BufferedInputStream.read(Unknown Source)
        - locked <0x00002aaab6ef97c8> (a java.io.BufferedInputStream)
        at java.io.DataInputStream.readInt(Unknown Source)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:276)

""regionserver/0:0:0:0:0:0:0:0:60020.compactor"" daemon prio=1 tid=0x00002aab381e5fd0 nid=0x6195 waiting on condition [0x0000000041c6c000..0x0000000041c6ce00]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock.lock(Unknown Source)
        at org.apache.hadoop.hbase.HLog.startCacheFlush(HLog.java:459)
        at org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:1089)
        at org.apache.hadoop.hbase.HRegion.close(HRegion.java:594)
        - locked <0x00002aaab6fa4e60> (a java.lang.Integer)
        at org.apache.hadoop.hbase.HRegion.splitRegion(HRegion.java:759)
        - locked <0x00002aaab6fa4e60> (a java.lang.Integer)
        at org.apache.hadoop.hbase.HRegionServer$CompactSplitThread.split(HRegionServer.java:248)
        at org.apache.hadoop.hbase.HRegionServer$CompactSplitThread.run(HRegionServer.java:204)

""regionserver/0:0:0:0:0:0:0:0:60020.logRoller"" daemon prio=1 tid=0x00002aab38181d70 nid=0x6193 waiting on condition [0x0000000041a6a000..0x0000000041a6ab00]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(Unknown Source)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(Unknown Source)
        at java.util.concurrent.locks.ReentrantLock.lock(Unknown Source)
        at org.apache.hadoop.hbase.HLog.rollWriter(HLog.java:219)
        at org.apache.hadoop.hbase.HRegionServer$LogRoller.run(HRegionServer.java:615)
        - locked <0x00002aaab69a3790> (a java.lang.Integer)

""org.apache.hadoop.io.ObjectWritable Connection Culler"" daemon prio=1 tid=0x00002aab38001280 nid=0x6185 waiting on condition [0x0000000041666000..0x0000000041666d00]
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ipc.Client$ConnectionCuller.run(Client.java:423)

""org.apache.hadoop.hbase.io.HbaseObjectWritable Connection Culler"" daemon prio=1 tid=0x00002aab3771f2a0 nid=0x5ea2 waiting on condition [0x0000000041464000..0x0000000041464d00]
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ipc.Client$ConnectionCuller.run(Client.java:423)

""DestroyJavaVM"" prio=1 tid=0x0000000040116a20 nid=0x5e70 waiting on condition [0x0000000000000000..0x00007fffb7bdab80]

""regionserver/0:0:0:0:0:0:0:0:60020"" prio=1 tid=0x00002aab37b76d60 nid=0x5ea1 waiting for monitor entry [0x0000000041363000..0x0000000041363d80]
        at org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:862)
        - waiting to lock <0x00002aaab69a3790> (a java.lang.Integer)
        at java.lang.Thread.run(Unknown Source)

""Low Memory Detector"" daemon prio=1 tid=0x00002aab37148810 nid=0x5e9f runnable [0x0000000000000000..0x0000000000000000]

""CompilerThread1"" daemon prio=1 tid=0x00002aab37146be0 nid=0x5e9e waiting on condition [0x0000000000000000..0x000000004105f3e0]

""CompilerThread0"" daemon prio=1 tid=0x00002aab37145630 nid=0x5e9d waiting on condition [0x0000000000000000..0x0000000040f5e4a0]

""AdapterThread"" daemon prio=1 tid=0x00002aab37143ed0 nid=0x5e9c waiting on condition [0x0000000000000000..0x0000000000000000]

""Signal Dispatcher"" daemon prio=1 tid=0x00002aab37136fd0 nid=0x5e9b waiting on condition [0x0000000000000000..0x0000000040d5dc50]

""Finalizer"" daemon prio=1 tid=0x00002aab37130020 nid=0x5e9a in Object.wait() [0x0000000040c5c000..0x0000000040c5cd00]
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        - locked <0x00002aaab69b91e0> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)

""Reference Handler"" daemon prio=1 tid=0x00002aab3712f740 nid=0x5e99 in Object.wait() [0x0000000040b5b000..0x0000000040b5bd80]
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Unknown Source)
        at java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)
        - locked <0x00002aaab69b6fa0> (a java.lang.ref.Reference$Lock)

""VM Thread"" prio=1 tid=0x00002aab371283d0 nid=0x5e98 runnable

""GC task thread#0 (ParallelGC)"" prio=1 tid=0x0000000040133610 nid=0x5e90 runnable

""GC task thread#1 (ParallelGC)"" prio=1 tid=0x0000000040134490 nid=0x5e91 runnable

""GC task thread#2 (ParallelGC)"" prio=1 tid=0x0000000040135310 nid=0x5e92 runnable

""GC task thread#3 (ParallelGC)"" prio=1 tid=0x0000000040136190 nid=0x5e93 runnable

""GC task thread#4 (ParallelGC)"" prio=1 tid=0x0000000040137010 nid=0x5e94 runnable

""GC task thread#5 (ParallelGC)"" prio=1 tid=0x0000000040137e90 nid=0x5e95 runnable

""GC task thread#6 (ParallelGC)"" prio=1 tid=0x0000000040138d10 nid=0x5e96 runnable

""GC task thread#7 (ParallelGC)"" prio=1 tid=0x0000000040139b90 nid=0x5e97 runnable

""VM Periodic Task Thread"" prio=1 tid=0x000000004012a520 nid=0x5ea0 waiting on condition
{code};;;","31/May/08 21:42;stack;It happened again on a new upload.  Here's the why:

{code}
2008-05-31 03:47:05,806 ERROR org.apache.hadoop.hbase.HRegionServer: Cache flush failed for region .META.,,1
java.lang.NullPointerException
        at org.apache.hadoop.hbase.HStore$StoreFileScanner.openReaders(HStore.java:2491)
        at org.apache.hadoop.hbase.HStore$StoreFileScanner.updateReaders(HStore.java:2654)
        at org.apache.hadoop.hbase.HStore.notifyChangedReadersObservers(HStore.java:1371)
        at org.apache.hadoop.hbase.HStore.updateReaders(HStore.java:1358)
        at org.apache.hadoop.hbase.HStore.internalFlushCache(HStore.java:1330)
        at org.apache.hadoop.hbase.HStore.flushCache(HStore.java:1267)
        at org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:1100)
        at org.apache.hadoop.hbase.HRegion.flushcache(HRegion.java:1020)
        at org.apache.hadoop.hbase.HRegionServer$Flusher.flushRegion(HRegionServer.java:446)
        at org.apache.hadoop.hbase.HRegionServer$Flusher.run(HRegionServer.java:390)
{code}

We'll clear the lock if its an IOE, but not if its NPE, etc.;;;","31/May/08 23:23;stack;Changed code so we'll ALWAYS clear flush lock, even if non-IOE exception.  Also fix code so we don't NPE.

{code}
M src/java/org/apache/hadoop/hbase/HStore.java
    Formatting.
    Renamed the readers data member in StoreFileScanner to be sfsReaders
    In openReaders, check for readers that have already been nulled/closed
    before closing myself (Avoids NPE).
    Made findFirstRow, closeSubScanner, getNext private.
M src/java/org/apache/hadoop/hbase/HRegion.java
    Always clear the cache flush lock.  Added exception stacktrace
    to DroppedSnapshotException so we have more info when these things
    happen.
{code};;;","01/Jun/08 05:21;stack;Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split reports incorrect elapsed time,HBASE-629,12396208,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,jimk,jimk,16/May/08 17:27,22/Aug/08 21:13,01/Jul/25 07:49,16/May/08 21:46,0.2.0,,,,,0.2.0,,,regionserver,,,,0,"Split reports incorrect elapsed time. That is because the start time for the split is never set. (It used to be set in closing()).

Additionally, since CompactSplitThread doesn't do anything in closing or closed anymore, why keep them around?

We can just pass null for the RegionUnavailableListener and can then remove closing and closed from CompactSplitThread.

In fact, it turns out that RegionUnavailableListener is not used anywhere anymore so it should just be removed altogether.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/08 20:56;jimk;629.patch;https://issues.apache.org/jira/secure/attachment/12382215/629.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25303,,,,,Fri May 16 21:46:05 UTC 2008,,,,,,,,,,"0|i0h8hb:",98636,,,,,,,,,,,,,,,,,,,,,"16/May/08 18:17;stack;+1 on removing unused methods and exceptions.;;;","16/May/08 21:10;stack;+1 if commented out code is removed
;;;","16/May/08 21:46;jimk;I just committed this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable table doesn't work reliably,HBASE-627,12396088,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jimk,mbu,mbu,15/May/08 09:58,29/Nov/11 19:50,01/Jul/25 07:49,28/Jun/08 02:45,0.2.0,,,,,0.2.0,,,,,,,0,"When creating a couple of tables like this:
1) create an empty table
2) disable table, add new column family, enable table
3) put 100 small documents into newly created column
around once in 10 tries the disable doesn't happen.

I have no clue as to why the table isn't disabled in the first place, but if this occurs, two things in HBaseAdmin.disableTable() strike me as odd:
- after numRetries tries to wait for disabling we exit the loop; there is no exception or error message:
...
2008-05-14 16:19:47,903 INFO org.apache.hadoop.hbase.client.HBaseAdmin: Disabled table table31
2008-05-14 16:19:47,910 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 60000, call addColumn(table31, {name: document, max versions: 3, compression: NONE, in memory: false, block cache enabled: false, max length: 2147483647, time to live: FOREVER, bloom filter: none}) from XXX.XX.40.36:47116: error: org.apache.hadoop.hbase.TableNotDisabledException: table31
...

- the scanner iterates over HRegionInfos of several tables. If any one of those is disabled, we also leave the loop as if the requested table had been disabled.

I've had this disabling problem occur quite reliably over the last days - today I couldn't reproduce it, though HBase version hasn't changed. ???",Hadoop/HBase on two nodes,,,,,,,,,HBASE-615,,,,,,,,,,,,,,,HBASE-713,HBASE-478,,,"15/May/08 10:42;mbu;disableTable31.log;https://issues.apache.org/jira/secure/attachment/12382099/disableTable31.log","15/May/08 13:19;mbu;disableTable5.log;https://issues.apache.org/jira/secure/attachment/12382110/disableTable5.log","27/Jun/08 18:07;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12384852/patch.txt","26/Jun/08 08:01;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12384747/patch.txt",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25302,,,,,Tue Nov 29 19:50:10 UTC 2011,,,,,,,,,,"0|i0h8gv:",98634,,,,,,,,,,,,,,,,,,,,,"15/May/08 10:42;mbu;This is the log output for table31, one of the tables for which disable didn't work.
I've noticed that this table's region was added to the kill list of node2, but had previously been reassigned to node1 and was opened there shortly afterwards. This does look like a timing problem with load balancing.;;;","15/May/08 13:19;mbu;I've attached another case. Here table5 is being reassigned to the same server (see disableTable5.log). In the log I find:
- RegionManager: Going to close region table5,,1210232134304
- ServerManager: Received MSG_REPORT_CLOSE : table5
- TableOperation: adding region table5,,1210232134304 to kill list
- ServerManager: Received MSG_REPORT_OPEN : table5

From now on the RegionManager says: ""Skipping region table5,,1210232134304 because it is already closing."" when selecting regions for reassignment.
And in HBaseAdmin.disableTable, we are ""Waiting for first region to be disabled from table5"".
;;;","15/May/08 17:16;jimk;Are you sure you have updated trunk past the patch for this issue? The same patch was applied to both trunk and branch, so it should work *provided* region assignment is not oscillating. If region assignment is oscillating, enable/disable do not work reliably, and probably won't until we fix that problem.;;;","19/May/08 10:07;mbu;Yes, I am sure I've updated. I can still observe the behaviour described above.

I did read in HBASE-615 that there might be some oscillation due to balancing but the fact that this oscillation affects enabling/disabling of tables has not been mentioned to my knowledge.
In my case, I wouldn't speak of oscillation though. Often it is just one region being assigned to the other one of my two nodes, which is certainly not unusual after having created a table. After all this is what the balancing is about. Still this reassignment and the more or less simultaneous disabling don't seem to go together well.;;;","19/May/08 15:49;mbu;Some debugging later I now have a clearer idea of what's happening after ServerManager, in his load balancing activities, receives MSG_REPORT_CLOSE for region xy from node1:

- RegionManager assigns region xy to node2 and sets it to ""unassigned"" (towards the end of assignRegionsToMultipleServers())
- At this point, HBaseAdmin.disableTable is called; in .META., region xy still has info:server node1, region xy is marked as beingServed by node1 in ProcessTableOperation.call() and thus added to the local kill list of node1
- Now we wait for region xy to go offline. But in the meantime, region xy opens on node2. .META. changes, region xy now has info:server node2

So there is a short period during which the information in .META. is not consistent with the actual state of regions. But disableTable() relies on the information found in .META. How could this best be solved? It looks like quite a fundamental problem to me.;;;","19/May/08 16:54;stack;Thanks for digging in Michaela.

Yeah, its a fundamental issue somewhat recognized in HBASE-543.  I think HBASE-543 needs to do more than make state changes atomic; it should also add logic that would prevent or intercept nonsensical transitions (e.g., from above, allowing an offlined table go online).;;;","20/Jun/08 18:03;jimk;Now that HBASE-615 has been fixed and the master stabilizes on region assignments, disable table and enable table appear to work reliably.

Remember that both enable and disable table run asychronously in the master so if you immediately follow either operation with with another, you may get table is not disabled or table is not enabled exceptions. 

In the shell, if you do enable table and immediately follow that with a disable table, some regions may still be on-line because they were in transition during the disable operation. Issuing a second disable table should disable the remaining on-line regions.

From the API you can use HTable.isTableOnline or HTable.isTableOffline to determine a table's state. You can then wait in a loop for the operation to finish.

We believe that these operations are now reliable.  Can you confirm?
;;;","23/Jun/08 14:34;mbu;In my setting, the disabling-problem described above doesn't seem to happen now as there is hardly any reassignment of regions anymore. So this measure certainly decreases the probability that the error will arise. I'm not sure however that it really solves the issue, since the underlying problem was outdated information in .META., while HBASE-615 affects only the conditions under which the problem occurrs. There might be other line-ups leading to this situation.

Using HTable.isTableOnline/Offline in a loop is probably a good way to avoid the problem. A few remarks on that:
- The ""// Wait until first region is disabled"" - section in HBaseAdmin.disableTable() looks very similar. Wouldn't it make sense to use isTableOffline() here instead?
- In both disableTable() and isTableOn-/Offline(), regions of other tables than the requested one are checked for being on-/offline (see Description of HBASE-627). That doesn't seem right to me. If any one of those other regions is on-/offline by chance, there will be a wrong result.
- Shouldn't a table be either online or offline? In that case, why are there two different functions for determining the table's state instead of using e.g. !isTableOnline().


;;;","26/Jun/08 08:01;jimk;HConnection
- tableExists now throws MasterNotRunningException if instance is not on-line
- added method isTableEnabled

HConnectionManager
- tableExists, getRegionLocation, listTables, locateRegion, relocateRegion, getHRegionConnection, getRegionServerWithRetries now check to see if master is running
- added method isTableEnabled
- getHTableDescriptor now checks if table exists, uses ScannerCallable instead of HTable.getScanner

HBaseAdmin
- enableTable, disableTable are now synchronous and wait until entire table is enabled/disabled and use HConnection.isTableEnabled to make the check
- added methods isTableEnabled(Text), isTableEnabled(String), isTableEnabled(byte[]). They use HConnection.isTableEnabled

HTable
- renamed isTableOnline methods to isTableEnabled, now use HConnection.isTableEnabled
- removed isTableOffline methods since they are redundant
;;;","26/Jun/08 08:02;jimk;patch.txt should address remaining issues. All tests pass locally. Please review.;;;","27/Jun/08 15:18;stack;HBASE-713 is about offliining/onlining not being reliiable on branch.  I got around the issue by writing a script that went through the regions that wouldn't offline individually offlining them.  I was then able to add my new column.  But I just noticed that the add column didn't apply to all regions, just to most.  Would suggest that this issue also cover this case; i.e. not just enabling/disabling but also guarantee that add/edit/delete of columns works too.;;;","27/Jun/08 17:54;jimk;> stack - 27/Jun/08 08:18 AM
> HBASE-713 is about offliining/onlining not being reliiable on branch. I got around the
> issue by writing a script that went through the regions that wouldn't offline individually
> offlining them. I was then able to add my new column. But I just noticed that the add
> column didn't apply to all regions, just to most. Would suggest that this issue also
> cover this case; i.e. not just enabling/disabling but also guarantee that
> add/edit/delete of columns works too. 

This is a bug in both column operations and table deletion.

If they think a region is being served that region is not added to the list of regions to process.
Unfortunately, determination of whether a region is being served is imprecise, because it
is based on info:server and info:startcode matching a server that the master knows about.

When a region is offlined, it should delete the server and startcode fields. Apparently sometimes, these fields are not deleted and those regions never get processed, even though the region info indicates the region is disabled.;;;","27/Jun/08 17:59;jimk;Modifying TableOperation$ProcessTableOperation.call to insert the region into the unservedRegions list if it is not being served or is not enabled will fix this problem.;;;","27/Jun/08 18:00;jimk;Make column operations and table deletion reliable;;;","27/Jun/08 18:07;jimk;Added fix for reliable column operations and table deletion;;;","27/Jun/08 18:08;jimk;New patch. Please review.;;;","27/Jun/08 20:14;jdcryans;Passes test that it should. +1;;;","27/Jun/08 21:47;stack;J-D: Did you review Jim's patch also?  If you have the time, do so.  Give it +/- 1 so Jim can (or cannot) commit.  Good stuff.;;;","27/Jun/08 21:52;jdcryans;Yes, patch was reviewed. +1;;;","27/Jun/08 21:58;stack;Thanks;;;","28/Jun/08 02:45;jimk;Committed.;;;","26/Nov/11 15:15;erwin00776;why this bug not fixed for deleteTable(...)?;;;","29/Nov/11 19:50;jdcryans;@Erwin

This jira was resolved 3 years ago, if you have an issue would you mind opening a new jira?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testmergetool failing in branch and trunk since hbase-618 went in,HBASE-620,12395575,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,07/May/08 23:20,22/Aug/08 21:13,01/Jul/25 07:49,07/May/08 23:49,0.1.2,0.2.0,,,,0.1.2,0.2.0,,,,,,0,The hbase-618 fix revealed that testmergetool depends on compactions running.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/08 23:21;stack;merge.patch;https://issues.apache.org/jira/secure/attachment/12381643/merge.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25298,,,,,Wed May 07 23:49:19 UTC 2008,,,,,,,,,,"0|i0h8fb:",98627,,,,,,,,,,,,,,,,,,,,,"07/May/08 23:21;stack;Add a force flag to HRegion.compactStores and to HStore.compact.;;;","07/May/08 23:49;stack;Committed to branch and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"We always compact if 2 files, regardless of the compaction threshold setting",HBASE-618,12395481,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,stack,stack,07/May/08 04:26,22/Aug/08 21:13,01/Jul/25 07:49,07/May/08 17:01,0.1.2,0.2.0,,,,0.1.2,0.2.0,,,,,,0,"We will always compact if there are two files in a store.  Here is an illustration from a loading run against 0.1.2 candidate:

{code}
2008-05-06 18:25:42,255 INFO org.apache.hadoop.hbase.HRegion: starting compaction on region category_rule_pricebin_statistics,,1210116131965
2008-05-06 18:25:42,259 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 2 files [329657396/confidence_interval/1251369679869294899, 329657396/confidence_interval/5238351815319958452] into /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/confidence_interval/mapfiles/6688946093979715350
2008-05-06 18:25:46,223 DEBUG org.apache.hadoop.hbase.HStore: moving /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/confidence_interval/mapfiles/6688946093979715350 to /hbase/category_rule_pricebin_statistics/329657396/confidence_interval/mapfiles/6019580165435904305
2008-05-06 18:25:46,329 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 2 files [329657396/rule_id/4877828519309794708, 329657396/rule_id/3736239181369788409] into /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/rule_id/mapfiles/6451418039787481756
2008-05-06 18:25:50,273 DEBUG org.apache.hadoop.hbase.HStore: moving /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/rule_id/mapfiles/6451418039787481756 to /hbase/category_rule_pricebin_statistics/329657396/rule_id/mapfiles/1365174520347083269
2008-05-06 18:25:50,338 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 2 files [329657396/hidden_variable/7348598912095388790, 329657396/hidden_variable/1402264537929464657] into /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/hidden_variable/mapfiles/7895992615693344978
2008-05-06 18:25:54,103 DEBUG org.apache.hadoop.hbase.HStore: moving /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/hidden_variable/mapfiles/7895992615693344978 to /hbase/category_rule_pricebin_statistics/329657396/hidden_variable/mapfiles/4450886729060218942
2008-05-06 18:25:54,155 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 2 files [329657396/category_id/6976628214412388959, 329657396/category_id/8426537623290869905] into /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/category_id/mapfiles/4017716533879305176
2008-05-06 18:25:57,698 DEBUG org.apache.hadoop.hbase.HStore: moving /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/category_id/mapfiles/4017716533879305176 to /hbase/category_rule_pricebin_statistics/329657396/category_id/mapfiles/657561173732096591
2008-05-06 18:25:57,747 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 2 files [329657396/price_bin_id/165701488423589566, 329657396/price_bin_id/5537046322320665760] into /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/price_bin_id/mapfiles/3214618236668106036
2008-05-06 18:26:01,135 DEBUG org.apache.hadoop.hbase.HStore: moving /hbase/category_rule_pricebin_statistics/compaction.dir/329657396/price_bin_id/mapfiles/3214618236668106036 to /hbase/category_rule_pricebin_statistics/329657396/price_bin_id/mapfiles/8727588456978537416
2008-05-06 18:26:01,181 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region category_rule_pricebin_statistics,,1210116131965 in 18sec
{code}

In the above, the region has 6 families, each of which is being loaded fairly evenly.   Every time through we'll compact a store if two files, which just so happens to be most of the time in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/08 04:53;stack;compaction.patch;https://issues.apache.org/jira/secure/attachment/12381556/compaction.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25296,,,,,Wed May 07 17:01:10 UTC 2008,,,,,,,,,,"0|i0h8ev:",98625,,,,,,,,,,,,,,,,,,,,,"07/May/08 04:53;stack;Respect the compaction threshold.  Also clean up logging.

Removed the 'nothing to compact' messages and changed 'starting compaction' to 'checking compaction'.

Here's example of 'nothing to compact' when 6 families:

{code}
2008-05-07 04:50:34,016 INFO org.apache.hadoop.hbase.HRegion: starting compaction on region category_rule_pricebin_statistics,,1210135411279
2008-05-07 04:50:34,017 DEBUG org.apache.hadoop.hbase.HStore: nothing to compact for 1494256927/rule_id
2008-05-07 04:50:34,017 DEBUG org.apache.hadoop.hbase.HStore: nothing to compact for 1494256927/confidence_interval
2008-05-07 04:50:34,017 DEBUG org.apache.hadoop.hbase.HStore: nothing to compact for 1494256927/hidden_variable
2008-05-07 04:50:34,017 DEBUG org.apache.hadoop.hbase.HStore: nothing to compact for 1494256927/price_bin_id
2008-05-07 04:50:34,017 DEBUG org.apache.hadoop.hbase.HStore: nothing to compact for 1494256927/category_id
2008-05-07 04:50:34,017 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region category_rule_pricebin_statistics,,1210135411279 in 0sec
{code};;;","07/May/08 05:13;stack;Looking for a review.;;;","07/May/08 15:00;bryanduxbury;+1;;;","07/May/08 17:01;stack;Committed branch and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Region balancer oscillates during cluster startup,HBASE-615,12395373,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,bryanduxbury,jimk,jimk,06/May/08 01:33,22/Aug/08 21:13,01/Jul/25 07:49,19/Jun/08 16:30,0.2.0,,,,,0.2.0,,,master,,,,1,"When starting a cluster with four region servers and a large table (49 regions) (+root +meta) = 51 total regions, the region balancer oscillates for a very long time and does not seem to reach a steady state.

Additionally, for whatever reason, it seems reluctant to assign regions to the first of four region servers, which may be the root cause. In my test, the first server had 10 regions assigned, the second and fourth had 13 regions assigned, and the master would continually assign and deassign 2 regions to the third server, which oscillated between 13 and 15 regions.  If it assigned the two fluctuating regions to the first server, it would achieve the best balance possible: 12, 13, 13, 13.

After 20 minutes, it had not stopped oscillating. An application trying to work against this cluster would run very slowly as it would be continually re-finding the two regions in flux.

When the table was being created, regions were nicely balanced. On restart, however, it just would not settle down.

Perhaps the balancer should set a target number of regions for each server which when the server achieved +/- 1 regions, the rebalancer would not try to change unless the number of regions changed.",,,,,,,,,,,,,HBASE-627,,,,,HBASE-63,,,,,,,,,,,"22/May/08 14:12;bryanduxbury;615-lite.patch;https://issues.apache.org/jira/secure/attachment/12382565/615-lite.patch","19/Jun/08 15:07;bryanduxbury;615-v2.patch;https://issues.apache.org/jira/secure/attachment/12384299/615-v2.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25293,,,,,Thu Jun 19 16:30:41 UTC 2008,,,,,,,,,,"0|i0h8e7:",98622,,,,,,,,,,,,,,,,,,,,,"22/May/08 14:11;bryanduxbury;There's two ways this could go. Either we can fix it by widening the margin by which you have to be ""over"" the load average before we start rebalancing region assignments, or we have to redo this code to be more sensitive to the load as it is being computed on the assignment side of things. 

I'll do the first no matter what as a test because it will be very simple. However, if the problem is that the definitions of ""overloaded"" from the perspective of load reported by the regionserver (this is some combo of regions and requests right now, yes?) and the math done by the rebalancing code are inherently different, then we'll need to either make the existing load balancing on assignment dumber or the rebalancing smarter. In the long run, we're definitely going to want to do the latter, but it requires us to start tracking requests and anything else that goes into the balancing computation at the region level, as well as actually reporting that information when the workers check in with their short list of reassignable regions. That way, when we're deciding how many regions to unassign, we can make informed decisions, rather than just trying to equalize averages.

;;;","22/May/08 14:12;bryanduxbury;Here's a patch for the easiest of the ideas. Jim, can you apply this and try it out with your 51-region table?;;;","22/May/08 22:35;jimk;Well now it oscillates moving three regions around. There are 40 regions including root and meta, 4 region servers, but the master refuses to give more regions to the region server hosting the meta region. It has 4 regions and the rest have 11, 12 or 13;;;","11/Jun/08 16:43;rafan;Please correct me if I am wrong or overlook something.

During startup, META will be requested more than other regions. Therefore,
the RegionServer that serves META will be considered more ""loaded"" than
others. So, we tends not to assign more regions to that one. However,
our rebalance algo currently considers only # of loaded regions as the ""load""
for region servers. That's the cause of oscillation at startup.

I'm thinking of the possibility that during startup, we just use assign
evenly to all region servers. Once this is stabilized, we start to consider
# of requests as part of the server load. Moreover, the ""# of requests"" here
should be calculated from a period of time, otherwise, we may moving regions
just because some spikes.

just my 2 cents.;;;","18/Jun/08 20:26;jimk;> Rong-En Fan - 11/Jun/08 09:43 AM
> Please correct me if I am wrong or overlook something.
> 
> During startup, META will be requested more than other regions. Therefore,
> the RegionServer that serves META will be considered more ""loaded"" than
> others. So, we tends not to assign more regions to that one. However,
> our rebalance algo currently considers only # of loaded regions as the ""load""
> for region servers. That's the cause of oscillation at startup.
> 
> I'm thinking of the possibility that during startup, we just use assign
> evenly to all region servers. Once this is stabilized, we start to consider
>
>   1. of requests as part of the server load. Moreover, the ""# of requests"" here
>       should be calculated from a period of time, otherwise, we may moving regions
>       just because some spikes.

You are absolutely correct. During startup, the server hosting the meta region gets all the requests,
so its one region gets multiplied by the number of requests giving a ""load"" that is far greater than all
the other servers which are getting no requests and consequently their ""load"" == number of regions
they are serving.

Should be a fairly easy fix to ignore requests during startup.

BTW, I verified this by changing HServerLoad.getLoad() to just return the number of regions. The
cluster had all regions on-line within a couple of minutes and they were balanced. When the
number of requests was factored in during startup, the cluster did not achieve a steady state
for 1/2 hour (after which I gave up).

just my 2 cents.
;;;","18/Jun/08 22:02;bryanduxbury;Should we just disable factoring in requests in a region server's load for the moment? It might lead to a worse distribution of regions, but it might also make no difference, if on average all regions are equally busy, which would be the case if you're running big time map/reduce jobs. If you have hot rows/regions, then this could hurt you, but it's hard to say by how much.;;;","18/Jun/08 22:29;stack;Would that mean no balancing?  Or just that balancing would be every server has ~ same number of regions?  If latter, lets do that for now.  It would still be massive improvement over what we have in 0.1 branch.   We can make a new issue for making balancing code consider loading.;;;","19/Jun/08 02:51;rafan;I believe Bryan is saying the latter way.;;;","19/Jun/08 15:07;bryanduxbury;Here's a patch that removes the number of requests from the load considerations. It passes unit tests (except TestMetaUtils, which times out, but appears unrelated). 

Jim, can you test this on your cluster and see if it reduces oscillation?;;;","19/Jun/08 15:17;rafan;I actually have the same patch applied locally, the cluster will start in a balance state
within few minutes.;;;","19/Jun/08 16:24;jimk;I had applied a similar change to my cluster and and it worked just fine. Reviewed patch. +1;;;","19/Jun/08 16:30;bryanduxbury;I just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamp-anchored scanning fails to find all records,HBASE-613,12395234,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,stack,stack,02/May/08 23:47,22/Aug/08 21:13,01/Jul/25 07:49,26/Jun/08 04:54,0.1.0,0.1.1,0.1.2,,,0.1.3,0.2.0,,Client,,,,0,"If I add 3 versions of a cell and then scan across the first set of added cells using a timestamp that should only get values from the first upload, a bunch are missing (I added 100k on each of the three uploads).  I thought it the fact that we set the number of cells found back to 1 in HStore when we move off current row/column but that doesn't seem to be it.  I also tried upping the MAX_VERSIONs on my table and that seemed to have no effect.  Need to look closer.

Build a unit test because replicating on cluster takes too much time.",,,,,,,,,,,,,,,,,,,,HBASE-681,,,,,,,,,"20/Jun/08 01:58;jimk;613.patch;https://issues.apache.org/jira/secure/attachment/12384339/613.patch","21/May/08 15:23;jimk;TestTimestampScanning.java;https://issues.apache.org/jira/secure/attachment/12382482/TestTimestampScanning.java","26/May/08 22:04;jimk;Timestamp.patch;https://issues.apache.org/jira/secure/attachment/12382806/Timestamp.patch","26/May/08 23:01;jimk;nogood.patch;https://issues.apache.org/jira/secure/attachment/12382807/nogood.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25292,,,,,Thu Jun 26 04:54:27 UTC 2008,,,,,,,,,,"0|i0h8dr:",98620,,,,,,,,,,,,,,,,,,,,,"05/May/08 17:30;stack;Fixed up subject and description.  Moved this out of 0.1.2.  Its bad but not critically bad.;;;","21/May/08 15:23;jimk;I have been unable to reproduce this by writing a test case. The one I wrote is attached. Maybe it needs lots more rows or random writes or multiple regions.;;;","21/May/08 16:21;stack;Thanks for writing a unit test.  Have you tried the original prescription above, using PE sequentialWrite three times in a row?  After each upload, run a scan to find the latest timestamp for the upload... then confirm that a scanner against the latest timestamp returns all cells?  Doing this procedure three times, I found that the first upload was missing about half of its records after 3rd upload (I had a bit of jython for doing the scans but seem to have mislaid it).

;;;","23/May/08 01:26;jimk;This is ugly.

Because InternalScanner specifies next as:
{code}
public boolean next(HStoreKey key, SortedMap<byte[], byte[]> results)
{code}
it is only possible to return one timestamp for all the results of a row. Consequently, Cells on the client side are meaningless with respect to timestamp. MemcacheScanner.next sets the timestamp in the HStoreKey to the timestamp requested when the scanner was created. So if the timestamp requested when the scanner was created was HConstants.LATEST_TIMESTAMP, that is what gets set. Similarly, if a specific timestamp was set when the scanner was created, older entries in the cache may not be found.

So what I propose is to change InternalScanner.next to be:
{code}
public byte[] next(SortedMap<byte[], Cell> results)
{code}
where the return value is the row key or null if there are not results and the results map key is the column name and value is the Cell (value, timestamp) pair. This should make it easier to determine what results should be returned.
;;;","23/May/08 04:19;stack;+1 on your suggestion.  What we had previously where we were dropping cell timestamp was just plain broke.;;;","26/May/08 22:04;jimk;This is a program that can be used to demonstrate the bug. See comments for how.;;;","26/May/08 22:46;jimk;Demonstrating the bug:

On an unpatched trunk, run Performance evaluation:
{code}
$ hadoop-0.17.0/bin/hadoop org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1
{code}

Apply Timestamp.patch (to get the test program), ant compile-test and run it:
{code}
$ hbase/bin/hbase org.apache.hadoop.hbase.Timestamp time
latest timestamp: 9223372036854775807
{code}

Note that the timestamp returned is the value of HConstants.LATEST_TIMESTAMP

Counting the number of rows with the returned value returns the correct result:
{code}
$ hbase/bin/hbase org.apache.hadoop.hbase.Timestamp count 9223372036854775807
number of rows: 1048576
{code}

of course that is not really the timestamp of the most recent row inserted. So shut down hbase and restart it (and allow it to settle down wrt region balancing), this flushes the caches and may cause compactions on the restart.
{code}
$ hbase/bin/stop-hbase.sh 
stopping master.......................
$ hbase/bin/start-hbase.sh 
starting master, logging to /bfd/jim/hbase/logs/hbase-jim-master-xx.foo.com.out
xx.foo.com: starting regionserver, logging to /bfd/jim/hbase/logs/hbase-jim-regionserver-xx.foo.com.out
yy.foo.com: starting regionserver, logging to /bfd/jim/hbase/logs/hbase-jim-regionserver-yy.foo.com.out
zz.foo.com: starting regionserver, logging to /bfd/jim/hbase/logs/hbase-jim-regionserver-zz.foo.com.out
vv.foo.com: starting regionserver, logging to /bfd/jim/hbase/logs/hbase-jim-regionserver-vv.foo.com.out
{code}

Running the program to get the timestamp of the latest cell inserted we get:
{code}
$ hbase/bin/hbase org.apache.hadoop.hbase.Timestamp time
latest timestamp: 1211839273332
{code}

a much more reasonable value. Even counting the number of rows with this timestamp works properly:
{code}
$ hbase/bin/hbase org.apache.hadoop.hbase.Timestamp count 1211839273332
number of rows: 1048576
{code}

If we run the PerformanceEvaluation test again (without shutting down or re-initializing the table), we get the wrong number of rows for the original timestamp:
{code}
$ hbase/bin/hbase org.apache.hadoop.hbase.Timestamp count 1211839273332
number of rows: 224384
{code}

and the value of the latest timestamp is:
{code}
$ hbase/bin/hbase org.apache.hadoop.hbase.Timestamp time
latest timestamp: 9223372036854775807
{code}

;;;","26/May/08 22:53;jimk;Clearly there are (at least) two problems here:

1. A client should never receive a Cell whose timestamp == HConstants.LATEST_TIMESTAMP
2. Scanning with a specified timestamp appears to be broken.
;;;","26/May/08 23:01;jimk;I started looking at what would change if I changed the API of InternalScanner.next to be
{code}
public byte[] next(SortedMap<byte[], Cell> results)
{code}

and found that changes to (at a minium) source files: HAbstractScanner, HRegion, HRegionServer, HStore, HStoreScanner, Memcache, StoreFileScanner, MetaUtils, HMerge and to tests: TestGet2, TestHMemcache, TestHRegion, TestScanner, TestSplit, HBaseTestCase. If these files are changed, changes could cascade outward to modules that depend on them or other modules they depend on.

This seemed far to large a scope, so instead I tried this more minimal patch. While it got the latest timestamp right, it still got the wrong row count.
;;;","26/May/08 23:11;jimk;Given the complicated nature of this bug, I would agree with Stack that we not plan to fix it in 0.1.x;;;","29/May/08 17:48;stack;Would it be possible changing memcache scanner so it didn't set timestamp to LATEST but instead to the actual cell value?  Would this be a simple fix for at least the case where users ask explicitly for a single cell?;;;","18/Jun/08 02:38;jimk;I finally found the problem (I think)... not only does the supplied timestamp apply to rows within the regions being scanned, it also applies to the regions being found in META. Thus if you specify a timestamp that is older than some of the regions in the META, you will only scan those regions and not all the regions in the table.This is really nasty, because you want to use HConstants.TIMESTAMP_LATEST to scan the META, and then use the user supplied timestamp for filtering results from scanners over those regions. Yuk!;;;","18/Jun/08 17:11;stack;How does the requested timestamp effect the lookup of region in .META.?  I'd think there should be no timestamping in the mix when you are trying to figure which region holds wanted row?;;;","18/Jun/08 20:08;jimk;What is happening:

The first region scans just fine (latest, timestamp > last insert, timestamp = previous version). There are 98016 rows in the first region and that is the number reported when timestamp = previous version.

Other regions scan ok when timestamp = latest or timestamp > last insert, but return no results when timestamp = previous version.

So the question is why does the first region scan just fine in all cases and all the other regions return no results when timestamp = previous version?

;;;","18/Jun/08 20:25;stack;Hey, I thought you'd figured it out (smile)?

Do the non-first regions have cells with entries that are timestamp <= previous?;;;","18/Jun/08 20:31;jimk;> stack - 18/Jun/08 01:25 PM

> Do the non-first regions have cells with entries that are timestamp <= previous?

No. The way I calculate previous is after the first run of PerformanceEvaluation. I scan the whole table (using latest) and take the maximum timestamp found as previous.

Then I wait for a bit before the second run of PerformanceEvaluation (just to be sure all the timestamps will be > previous) and that's when I run the test.

Am still trying to figure why the first region works fine but none of the others do.
;;;","18/Jun/08 20:35;jimk;> Do the non-first regions have cells with entries that are timestamp <= previous?

er, rather yes they do. As the value I use for previous is the largest over the whole table, all the regions have entries with timestamp <= previous.;;;","18/Jun/08 20:43;stack;The second scan actually visits the non-first regions?  (DEBUG emissions from client will log region transition?);;;","18/Jun/08 20:48;jimk;Yes, all the regions are visited. I instrumented ScannerCallable to show me.

timestamp = previous scan output:

{code}
TestTable,,99999999999999
.META.,TestTable,,99999999999999,99999999999999
instantiateServer
openScanner: regionName=TestTable,,1213741686408, columns[info:], row=, timestamp=1213742072014, filter null
closing scanner. largest timestamp=1213741622230
instantiateServer
TestTable,0000098016,99999999999999
openScanner: regionName=TestTable,0000098016,1213747903044, columns[info:], row=0000098016, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000167456,99999999999999
openScanner: regionName=TestTable,0000167456,1213747903045, columns[info:], row=0000167456, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000245120,99999999999999
openScanner: regionName=TestTable,0000245120,1213747962641, columns[info:], row=0000245120, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000316544,99999999999999
openScanner: regionName=TestTable,0000316544,1213747962642, columns[info:], row=0000316544, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000400128,99999999999999
openScanner: regionName=TestTable,0000400128,1213748040922, columns[info:], row=0000400128, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000471056,99999999999999
openScanner: regionName=TestTable,0000471056,1213748040923, columns[info:], row=0000471056, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000553152,99999999999999
openScanner: regionName=TestTable,0000553152,1213748105169, columns[info:], row=0000553152, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000623792,99999999999999
openScanner: regionName=TestTable,0000623792,1213748105169, columns[info:], row=0000623792, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000705024,99999999999999
openScanner: regionName=TestTable,0000705024,1213748171658, columns[info:], row=0000705024, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000776528,99999999999999
openScanner: regionName=TestTable,0000776528,1213748171659, columns[info:], row=0000776528, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000860384,99999999999999
openScanner: regionName=TestTable,0000860384,1213748251702, columns[info:], row=0000860384, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
instantiateServer
TestTable,0000923760,99999999999999
openScanner: regionName=TestTable,0000923760,1213748251704, columns[info:], row=0000923760, timestamp=1213742072014, filter null
closing scanner. largest timestamp=0
number of rows: 98016
{code}

timestamp > last insert scan:

{code}
TestTable,,99999999999999
.META.,TestTable,,99999999999999,99999999999999
instantiateServer
openScanner: regionName=TestTable,,1213741686408, columns[info:], row=, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213747836722
instantiateServer
TestTable,0000098016,99999999999999
openScanner: regionName=TestTable,0000098016,1213747903044, columns[info:], row=0000098016, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213747864149
instantiateServer
TestTable,0000167456,99999999999999
openScanner: regionName=TestTable,0000167456,1213747903045, columns[info:], row=0000167456, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213747895166
instantiateServer
TestTable,0000245120,99999999999999
openScanner: regionName=TestTable,0000245120,1213747962641, columns[info:], row=0000245120, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213747928813
instantiateServer
TestTable,0000316544,99999999999999
openScanner: regionName=TestTable,0000316544,1213747962642, columns[info:], row=0000316544, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213747976497
instantiateServer
TestTable,0000400128,99999999999999
openScanner: regionName=TestTable,0000400128,1213748040922, columns[info:], row=0000400128, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748004632
instantiateServer
TestTable,0000471056,99999999999999
openScanner: regionName=TestTable,0000471056,1213748040923, columns[info:], row=0000471056, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748039792
instantiateServer
TestTable,0000553152,99999999999999
openScanner: regionName=TestTable,0000553152,1213748105169, columns[info:], row=0000553152, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748070983
instantiateServer
TestTable,0000623792,99999999999999
openScanner: regionName=TestTable,0000623792,1213748105169, columns[info:], row=0000623792, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748103050
instantiateServer
TestTable,0000705024,99999999999999
openScanner: regionName=TestTable,0000705024,1213748171658, columns[info:], row=0000705024, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748137599
instantiateServer
TestTable,0000776528,99999999999999
openScanner: regionName=TestTable,0000776528,1213748171659, columns[info:], row=0000776528, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748194858
instantiateServer
TestTable,0000860384,99999999999999
openScanner: regionName=TestTable,0000860384,1213748251702, columns[info:], row=0000860384, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748220745
instantiateServer
TestTable,0000923760,99999999999999
openScanner: regionName=TestTable,0000923760,1213748251704, columns[info:], row=0000923760, timestamp=1213748311328, filter null
closing scanner. largest timestamp=1213748311328
number of rows: 1048576
{code}
;;;","18/Jun/08 22:08;stack;I have no suggestion under than what you are probably already doing, adding dumbass logging of everything then trying to sort through the mess of emissions.

FYI, enabling DEBUG on client shows the region transitions.  Shouldn't have to add instrumentation.;;;","18/Jun/08 23:20;jimk;Thought HADOOP-3472 might be the cause so build a 0.17 latest Hadoop. No difference. Must be something we're doing.;;;","20/Jun/08 01:58;jimk;HAbstractScanner
- remove HAbstactScanner.iterator() - iterator is not a method on InternalScanner

HRegion
- make getScanner more efficient by iterating only once to find the stores we need to scan
- only pass columns relevant to a store to a HStoreScanner
- remove HScanner.iterator() - iterator is not a method on InternalScanner

MemcacheScanner
- never return HConstants.LATEST_TIMESTAMP as the timestamp value for a row. Instead use the largest timestamp from the cells being returned. This allows a scanner to determine a timestamp that can be used to fetch the same data again should new versions be inserted later.

StoreFileScanner
- getNextViableRow would find a row that matched the row key, but did not consider the requested timestamp. Now if the row it finds has a timestamp greater than the one desired it advances to determine if a row with a timestamp less than or equal to the requested one exists since timestamps are sorted descending.
- removed an unnecessary else

Timestamp
- The program that was used to find the problem and test the fix.

TestScanMultipleVersions
- Test program that fails on current trunk but passes when this patch is applied.

NOTE: TestHRegionServerExit failed on both Windows and Linux, but TestRegionRebalancing passed on Linux and failed on Windows.

All other tests passed, and when I ran TestScanMultipleVersions against unpatched trunk, it failed.

Please review.
;;;","20/Jun/08 01:59;jimk;Please review.;;;","20/Jun/08 17:00;stack;Add pointer to this issue as class comment on TestScanMultipleVersions?

The setup of your test creating root and meta duplicates code; this code is duplicated alot in tests if you search.  Would suggest making a method either in base test class or in MetaUtils (Otherwise, nice test).

Timestamp class needs class comment describing what its for.  But is this class needed in TRUNK now we have a working shell?  You can pass timestamps to scanner and shell does count for you: ""scan 'TABLENAME', {TIMESTAMP => xxxxxx}""

Regards the Memcache edits, where you ensure we not return LATEST_TIMESTAMP, what if we returned an empty ts so there was no danger of ts being misread?    Force examination of Cell payload if timestamp is what is wanted?

Otherwise patch looks good. +1 after above changes.

Do you think this issue came about because refactorings?  Do we have this issue in branch?





;;;","20/Jun/08 20:48;jimk;> stack - 20/Jun/08 10:00 AM
> Add pointer to this issue as class comment on TestScanMultipleVersions?

Will do.

> The setup of your test creating root and meta duplicates code; this code is duplicated alot in tests if you search. Would suggest
> making a method either in base test class or in MetaUtils (Otherwise, nice test).

Will do for trunk, but not branch.

> Timestamp class needs class comment describing what its for. But is this class needed in TRUNK now we have a working
> shell? You can pass timestamps to scanner and shell does count for you: ""scan 'TABLENAME', {TIMESTAMP => xxxxxx}""

No, it is not needed. I will remove Timestamp

> Regards the Memcache edits, where you ensure we not return LATEST_TIMESTAMP, what if we returned an empty ts so
> there was no danger of ts being misread? Force examination of Cell payload if timestamp is what is wanted?

I did try this, but unfortunately HStoreScanner and HRegion.HScanner need some timestamp to determine what they are going to include in the results. Tried to work around it, but scanners are so complicated now, I decided to take a more pragmatic approach and just make them work, rather than spend a couple more weeks trying to figure out how to make the upstream scanners less timestamp dependent. 

> Do you think this issue came about because refactorings? Do we have this issue in branch?

As we now know. This was not caused by refactorings. It is present in 0.1 branch so has been an issue for some time.

;;;","21/Jun/08 02:56;jimk;Committed to branch and trunk.;;;","25/Jun/08 16:34;jimk;It turns out that HRegionServer.next (in both trunk and 0.1 branch) will return an empty result instead of null if there are no results.;;;","26/Jun/08 04:54;jimk;Fixed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,
HRegionServer::getThisIP() checks hadoop config var for dns interface name,HBASE-608,12395082,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,jimbojw,jimbojw,30/Apr/08 16:42,22/Aug/08 21:13,01/Jul/25 07:49,01/May/08 04:19,0.1.1,0.1.2,,,,0.1.2,0.2.0,,regionserver,,,,0,"The getThisIP() method of the HRegionServer class checks for the hadoop config var ""dfs.datanode.dns.interface"" rather than an hbase-specific configuration property.  I propose a new config var called ""hbase.regionserver.dns.interface"" to check instead.  Will attach patch shortly.","Red Hat Enterprise Linux, Java 1.5, Hadoop 0.16",,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/08 16:47;jimbojw;hbase-regionserver-dns-interface.diff;https://issues.apache.org/jira/secure/attachment/12381199/hbase-regionserver-dns-interface.diff",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25289,,,,,Thu May 01 20:07:08 UTC 2008,,,,,,,,,,"0|i0h8cn:",98615,,,,,,,,,,,,,,,,,,,,,"30/Apr/08 16:47;jimbojw;Alters HRegionServer::getThisIP() to check ""hbase.regionserver.dns.interface"" instead of ""dfs.datanode.dns.interface"", and adds new property to conf/hbase-default.xml;;;","30/Apr/08 16:51;stack;+1 on patch.  I'll apply it to branch and trunk;;;","30/Apr/08 17:32;stack;Can't commit at moment 'cos svn is down.  Will do when it comes back online.;;;","01/May/08 04:19;stack;Applied to patch and trunk.  Thanks for the patch Jim.;;;","01/May/08 20:07;jimbojw;Cool cool - thanks stack!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowFilterInterface.rowProcessed() is called *before* fhe final filtering decision is made,HBASE-595,12394351,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,clint.morgan,clint.morgan,18/Apr/08 23:24,22/Aug/08 21:13,01/Jul/25 07:49,21/Apr/08 16:43,0.1.1,0.2.0,,,,0.1.2,0.2.0,,Filters,,,,0,"rowProcessed is called in HStoreScanner, however, the final filtering decision is not made until the full row has been assembled in HRegion",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/08 23:34;clint.morgan;hbase-595.patch;https://issues.apache.org/jira/secure/attachment/12380549/hbase-595.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25284,,,,,Mon Apr 21 16:43:43 UTC 2008,,,,,,,,,,"0|i0h89r:",98602,,,,,,,,,,,,,,,,,,,,,"18/Apr/08 23:34;clint.morgan;patch to fix;;;","20/Apr/08 15:39;stack;Clint: The way things are now, we may make decision prematurely after looking at just one store?  Should I apply patch to branch too?;;;","21/Apr/08 15:52;clint.morgan;The problem is that we are notifying the filters of the filtering
decision prematurely. Hstores use the filters on row key and
individual columns, but the final filtering decision does not come
until all of the stores' columns are assembled in the hregion. So only
after this decision has been made can we notify rowProcessed() to filters (in the hregion).

I say we should patch branches.

But its probably not a big deal. Looking over existing filter impls, only
PageRowFilter would be affected by the bug. (And this filter offerers little utility
because it only skips the rows after the page. Instead you could save
the trouble and just stop the advancing the scanner in the first
place.);;;","21/Apr/08 16:43;stack;Committed.  Thanks for the patch Clint.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase migration tool does not get correct FileSystem or root directory if configuration is not correct.,HBASE-590,12394248,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,jimk,jimk,17/Apr/08 21:36,22/Aug/08 21:13,01/Jul/25 07:49,17/Apr/08 22:14,0.1.1,,,,,0.1.2,0.2.0,,util,,,,0,"The HBase migration tool does not validate hbase.rootdir as a valid URI that contains a scheme (e.g., file:// or hdfs://) and fails to find the root directory and the file system if hbase.rootdir is not a URI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/08 21:45;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380448/patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25280,,,,,Thu Apr 17 22:14:35 UTC 2008,,,,,,,,,,"0|i0h88n:",98597,,,,,,,,,,,,,,,,,,,,,"17/Apr/08 22:14;jimk;Passes local tests. Committing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE getting scanner,HBASE-577,12393838,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,stack,stack,13/Apr/08 17:55,22/Aug/08 21:13,01/Jul/25 07:49,14/Apr/08 19:26,0.1.2,,,,,0.1.2,0.2.0,,,,,,0,"Saw following in a 0.1.1 install:

{code}
2008-04-11 16:59:09,820 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 60020, call openScanner(enwiki_test10k,,1197341035929, null, k4xE4Y6SK4dq7I2F2uhRn-==, 9223372036854775807, null) from 208.76.44.136:39230: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1195)
        at org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1449)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/08 18:58;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380091/patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25274,,,,,Mon Apr 14 19:26:30 UTC 2008,,,,,,,,,,"0|i0h85r:",98584,,,,,,,,,,,,,,,,,,,,,"14/Apr/08 18:02;stack;Could have been stacktrace from patched 0.16.0. If so, here is the code that was running with line numbers:

{code}
   1186   public HScannerInterface getScanner(Text[] cols, Text firstRow,
   1187       long timestamp, RowFilterInterface filter) throws IOException {
   1188     lock.readLock().lock();
   1189     try {
   1190       if (this.closed.get()) {
   1191         throw new IOException(""Region "" + this.getRegionName().toString() +
   1192           "" closed"");
   1193       }
   1194       TreeSet<Text> families = new TreeSet<Text>();
   1195       for(int i = 0; i < cols.length; i++) {
   1196         families.add(HStoreKey.extractFamily(cols[i]));
   1197       }
   1198       List<HStore> storelist = new ArrayList<HStore>();
   1199       for (Text family: families) {
   1200         HStore s = stores.get(family);
   1201         if (s == null) {
   1202           continue;
   1203         }
   1204         storelist.add(stores.get(family));
   1205 
   1206       }
   1207       return new HScanner(cols, firstRow, timestamp,
   1208         storelist.toArray(new HStore [storelist.size()]), filter);
   1209     } finally {
   1210       lock.readLock().unlock();
   1211     }
   1212   }
{code};;;","14/Apr/08 18:58;jimk;Checks arguments in HRegionServer.openScanner for null, if so throws IOException that wraps NPE.;;;","14/Apr/08 18:59;jimk;Please review.;;;","14/Apr/08 19:05;stack;+1;;;","14/Apr/08 19:26;jimk;Committed to 0.1.2 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase master dies with stack overflow error if rootdir isn't qualified,HBASE-575,12393775,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,bien,bien,11/Apr/08 17:52,22/Aug/08 21:13,01/Jul/25 07:49,15/Apr/08 21:43,0.1.1,,,,,0.1.2,0.2.0,,,,,,0,"With a relative rootdir (/hbase/), the hbase master throws this on startup:

08/04/11 17:53:00 ERROR hbase.HMaster: Can not start master
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.apache.hadoop.hbase.HMaster.doMain(HMaster.java:3329)
        at org.apache.hadoop.hbase.HMaster.main(HMaster.java:3363)
Caused by: java.lang.StackOverflowError
        at java.net.URI$Parser.checkChars(Unknown Source)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/08 18:06;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380201/patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25273,,,,,Tue Apr 15 21:43:30 UTC 2008,,,,,,,,,,"0|i0h853:",98581,,,,,,,,,,,,,,,,,,,,,"11/Apr/08 22:49;stack;I asked Michael to make this a blocker since its the first thing users will see migrating from 0.16.x to 0.1.0 (if they don't read the doc. that is);;;","15/Apr/08 18:07;jimk;Detects invalid root directory paths. Includes a test case. Please review.;;;","15/Apr/08 19:12;stack;Why inline the rootdir test into the HMaster constructor?  Why not make a utility method hosted elsewhere --- FSUtils -- and have the HMaster constructor use that?  Have the test exercise the utility method rather than HMaster (as is where it catches any IOE, its overly broad).;;;","15/Apr/08 21:43;jimk;Committed to 0.1 branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase does not load hadoop native libs,HBASE-574,12393722,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,rafan,rafan,11/Apr/08 05:52,22/Aug/08 21:13,01/Jul/25 07:49,14/Apr/08 19:26,0.1.0,0.1.1,0.1.2,0.2.0,,0.1.2,0.2.0,,io,,,,0,"After moving out from hadoop/contrib, the standalone release does not include hadoop native libs in hbase/lib/native while it still includes hadoop-core.jar. I think they should be included as well to improve speed for compression and decompression.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/08 06:47;rafan;hbase-native.diff;https://issues.apache.org/jira/secure/attachment/12380045/hbase-native.diff",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25272,,,,,Mon Apr 14 19:32:18 UTC 2008,,,,,,,,,,"0|i0h84v:",98580,,,,,,,,,,,,,,,,,,,,,"14/Apr/08 06:48;rafan;I must overlook the log. In addition to put lib/native, we still need to set proper environment in bin/hbase so that java can find the native libs. Please check the enclosed patch.;;;","14/Apr/08 15:04;stack;+1 on patch.  Lets get this into 0.1.2 release.   Changed it to blocker for that release.  Need to commit the native libs from 0.16.3 hadoop at same time.;;;","14/Apr/08 19:26;stack;Thanks for the patch Rong-En Fan.

Added native libs from hadoop-0.16.2 to hbase branch.

Confirmed on a linux machine that stuff is loaded:

{code}
2008-04-14 19:21:04,392 INFO org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
{code}

When we upgrade branch to 0.16.3 hadoop, need to upgrade native libs at same time.;;;","14/Apr/08 19:32;stack;Added 0.16.2 native libs to TRUNK as placeholder until 0.17 release.  Applied patch for bin/hbase to TRUNK too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a flush file of zero entries,HBASE-564,12393212,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jimk,stack,stack,05/Apr/08 03:31,22/Aug/08 21:13,01/Jul/25 07:49,07/Apr/08 21:09,0.2.0,,,,,0.2.0,,,,,,,0,"Saw this in log in TRUNK:

{code}
    [junit] 2008-04-04 20:22:40,943 DEBUG [RegionServer:0.cacheFlusher] regionserver.HStore(676): Added 1403560700/text/8075392345773720818 with 0 entries, sequence id 537, data size 0.0, file size 110.0 for 1403560700/text
{code}

I thought that we'd fixed flushing zero-entry files",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/08 20:33;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12379597/patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25268,,,,,Mon Apr 07 21:09:04 UTC 2008,,,,,,,,,,"0|i0h82n:",98570,,,,,,,,,,,,,,,,,,,,,"07/Apr/08 17:09;jimk;It turns out that we only check for a zero memcache size at the region level. Adding a check at the HStore level will be a simple patch.;;;","07/Apr/08 20:33;jimk;There were thee paths that bypassed the size checks:
- HStore.doReconstructionLog called by HStore constructor
- HStore.flushCache called by HRegion.internalFlushCache called by either HRegion.close or HRegion.flushCache

Added a check in HStore.internalFlushCache to just return if cache has zero entries.
;;;","07/Apr/08 20:35;jimk;Tests pass locally, please review.;;;","07/Apr/08 20:37;stack;+1;;;","07/Apr/08 21:06;jimk;Committed.;;;","07/Apr/08 21:08;jimk;Not sure how this got cannot reproduce status. It should be fixed.;;;","07/Apr/08 21:09;jimk;Setting resolution to fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Only one Worker in HRS; on startup, if assigned tens of regions, havoc of reassignments because open processing is done in series",HBASE-555,12392787,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,31/Mar/08 23:04,12/Apr/08 16:12,01/Jul/25 07:49,01/Apr/08 22:10,0.1.0,0.16.0,0.2.0,,,0.1.1,,,regionserver,,,,0,"On the Lars clusters, he's up into the thousands of regions.  Starting this cluster, there is a load of churn in the master log as we assign regions, they report their opening and then after the hbase.hbasemaster.maxregionopen of one minute elapses, we assign the region elsewhere.

Problem seems to be the fact that we only run a single Worker thread in our regionserver; means that region opens are processed in series.

For example, the below shows when a master assigned a region and then the regionserver side log when it got around to opening it:

{code}
2008-03-29 04:48:51,638 INFO org.apache.hadoop.hbase.HMaster: assigning region pdc-docs,US20060158177_20060720,1205765009844 to server 192.168.105.19:60020
..
2008-03-29 04:50:58,124 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : pdc-docs,US20060158177_20060720,1205765009844
{code}

There is > 2 minutes between the two loggings (I checked clocks on this cluster and they are synced).

Looking in the regionserver log, its just filled with logging on the opening of regions.  The region opens are running pretty fast at about a second each but there are hundreds of regions to open in this case so its easy to go over our default of 60 seconds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/08 20:14;stack;555-0.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12379061/555-0.1-v2.patch","01/Apr/08 04:32;stack;555-0.1.patch;https://issues.apache.org/jira/secure/attachment/12379002/555-0.1.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25263,,,,,Tue Apr 01 22:10:46 UTC 2008,,,,,,,,,,"0|i0h80n:",98561,,,,,,,,,,,,,,,,,,,,,"31/Mar/08 23:38;stack;Couple of ideas:

Create a worker thread for every message.  That'd be a worker per region to open.  If hundreds, not so smart as all would be contending.  So perhaps an upper bound on threads created.  But then we'd just have same issue again where we'd have queued opens that were not being serviced?

So maybe single Worker ain't so bad.  Issue then is making it so we report the master that regions are being worked on.  Could take out an iterator and queue a PROCESSING message per queued region.

Trying to come up w/ minimal patch for 0.1.  We can fix better in 0.2.;;;","01/Apr/08 04:32;stack;{code}
M  src/java/org/apache/hadoop/hbase/HRegionServer.java
   (housekeeping): Call a housekeeping method before we go into
   hibernation.  Currently its only task is review of the todo
   list and adding MSG_REGION_PROCESS_OPEN if any regions waiting
   to be opened in the queue.
   (addProcessMessage): Method to add MSG_REPORT_PROCESS_OPEN to
   messages to send the server.
{code};;;","01/Apr/08 19:48;stack;When .19 server shows up, gets 100+ regions.  This patch should include upper bound on how many regions we assign at a time.

Looking at the Lars cluster, this patch seems to be doing what its supposed to.... Its ugly in that currently there is a log for every MSG_REPORT_PROCESS_OPEN message -- one per region every time it reports in -- but its just during startup (Previous our startup logs were clogged with reassigning regions already assigned).

Here is illustration that patch is basically working...  After assignment, 7 seconds after last assigned region message is logged we see this:

{code}
2008-04-01 00:07:05,460 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,US20070223009_20070927,1205860531876 from 192.168.105.19:60020
{code}

A few regions open, then we get this:

{code}
2008-04-01 00:07:11,528 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,US4881767_19891121,1205704528908 from 192.168.105.19:60020
{code}

... about 6 seconds after one from previous batch... then...

{code}
2008-04-01 00:07:14,534 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,US5399923_19950321,1205793104251 from 192.168.105.19:60020
{code}

later....

{code}
2008-04-01 00:07:23,614 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,EP04011653NWA1,1205771873299 from 192.168.105.19:60020
{code}

etc.
;;;","01/Apr/08 20:14;stack;Add to original patch assigning max of ten regions at a time.;;;","01/Apr/08 20:46;jimk;Reviewed new patch. +1;;;","01/Apr/08 21:31;stack;Going to commit this thing.  Just tested it over on the Lars cluster (2100 regions on 20 servers).  The two fixes in this patch made it so on restart, there were no more ""should not have opened region's"" -- all came up smoothly and regions are spread pretty evenly (looks better than what it used to be but I only did one restart).;;;","01/Apr/08 22:10;stack;Committed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
filters generate StackOverflowException,HBASE-554,12392679,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,stack,stack,30/Mar/08 20:36,12/Apr/08 16:12,01/Jul/25 07:49,08/Apr/08 18:58,0.1.0,0.16.0,0.2.0,,,0.1.1,0.2.0,,Filters,,,,0,"Below is from list.

You're doing nothing wrong.

The filters as written recurse until they find a match.  If long stretches between matching rows, then you will get a StackOverflowError.  Filters need to be changed.  Thanks for pointing this out.  Can you do without them for the moment until we get a chance to fix it?

St.Ack

David Alves wrote:
> Hi St.Ack and all
> 	
> 	The error always occurs when trying to see if there are more rows to
> process.
> 	Yes I'm using a filter(RegExpRowFilter) to select only the rows (any
> row key) that match a specific value in one of the columns.
> 	Then I obtain the scanner just test the hasNext method, close the
> scanner and return.
> 	Am I doing something wrong?
> 	Still StackOverflowError is not supposed to happen right?
>
> Regards
> David Alves
> On Thu, 2008-03-27 at 12:36 -0700, stack wrote:
>> You are using a filter?  If so, tell us more about it.
>> St.Ack
>>
>> David Alves wrote:
>>> Hi guys 
>>>
>>> 	I 'm using HBase to keep data that is later indexed.
>>> 	The data is indexed in chunks so the cycle is get XXXX records index
>>> them check for more records etc...
>>> 	When I tryed the candidate-2 instead of the old 0.16.0 (which I
>>> switched to do to the regionservers becoming unresponsive) I got the
>>> error in the end of this email well into an indexing job.
>>> 	So you have any idea why? Am I doing something wrong?
>>>
>>> David Alves
>>>
>>> java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException:
>>> java.io.IOException: java.lang.StackOverflowError
>>>         at java.io.DataInputStream.readFully(DataInputStream.java:178)
>>>         at java.io.DataInputStream.readLong(DataInputStream.java:399)
>>>         at org.apache.hadoop.dfs.DFSClient
>>> $BlockReader.readChunk(DFSClient.java:735)
>>>         at
>>> org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:234)
>>>         at
>>> org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)
>>>         at
>>> org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)
>>>         at
>>> org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)
>>>         at org.apache.hadoop.dfs.DFSClient
>>> $BlockReader.read(DFSClient.java:658)
>>>         at org.apache.hadoop.dfs.DFSClient
>>> $DFSInputStream.readBuffer(DFSClient.java:1130)
>>>         at org.apache.hadoop.dfs.DFSClient
>>> $DFSInputStream.read(DFSClient.java:1166)
>>>         at java.io.DataInputStream.readFully(DataInputStream.java:178)
>>>         at org.apache.hadoop.io.DataOutputBuffer
>>> $Buffer.write(DataOutputBuffer.java:56)
>>>         at
>>> org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)
>>>         at org.apache.hadoop.io.SequenceFile
>>> $Reader.next(SequenceFile.java:1829)
>>>         at org.apache.hadoop.io.SequenceFile
>>> $Reader.next(SequenceFile.java:1729)
>>>         at org.apache.hadoop.io.SequenceFile
>>> $Reader.next(SequenceFile.java:1775)
>>>         at org.apache.hadoop.io.MapFile$Reader.next(MapFile.java:461)
>>>         at org.apache.hadoop.hbase.HStore
>>> $StoreFileScanner.getNext(HStore.java:2350)
>>>         at
>>> org.apache.hadoop.hbase.HAbstractScanner.next(HAbstractScanner.java:256)
>>>         at org.apache.hadoop.hbase.HStore
>>> $HStoreScanner.next(HStore.java:2561)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1807)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>>         at org.apache.hadoop.hbase.HRegion
>>> $HScanner.next(HRegion.java:1843)
>>> ...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/08 17:54;clint.morgan;hbase-554-v2.patch;https://issues.apache.org/jira/secure/attachment/12379415/hbase-554-v2.patch","31/Mar/08 23:41;clint.morgan;hbase-554.patch;https://issues.apache.org/jira/secure/attachment/12378988/hbase-554.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25262,,,,,Tue Apr 08 18:58:24 UTC 2008,,,,,,,,,,"0|i0h80f:",98560,,,,,,,,,,,,,,,,,,,,,"31/Mar/08 23:41;clint.morgan;The culprit is in the last patch I submitted. Upon filtering an assembled row, it recursively calls next. Technically this is a tail-rec call, but I guess not all compilers will recognize this.

This patch uses explicit iteration instead. Let me know if it works for you...;;;","31/Mar/08 23:47;stack;Clint: Want to write the list in case Dave Alves is not watching this issue telling him you made a possible fix?  (Thanks for the patch).;;;","04/Apr/08 17:54;clint.morgan;My last patch was missing part of the header, and so eclipse would not apply it. This fixes it.;;;","08/Apr/08 18:58;jimk;Committed to 0.1 branch and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom filter bugs,HBASE-552,12392645,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ab,ab,29/Mar/08 22:13,28/Aug/08 18:18,01/Jul/25 07:49,01/Apr/08 17:55,0.1.0,,,,,0.1.1,0.18.0,0.2.1,,,,,0,There are some bugs in Bloom filters in the code that deals with initialization and (de)serialization.,,,,,,,,,,,,,,,,,,,,,,,HADOOP-3063,,,,,,"29/Mar/08 22:13;ab;bloom.patch;https://issues.apache.org/jira/secure/attachment/12378874/bloom.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25261,,,,,Thu Aug 28 17:46:47 UTC 2008,,,,,,,,,,"0|i0h7zz:",98558,,,,,,,,,,,,,,,,,,,,,"29/Mar/08 22:13;ab;Patch to fix the bugs.;;;","29/Mar/08 22:15;ab;The patched version of these classes (with some formatting changes) is included in that Hadoop patch.;;;","01/Apr/08 17:55;jimk;I just committed this. Thanks Andrzej!;;;","28/Aug/08 14:18;ab;Changes from this patch were never applied to trunk, instead a partial patch in HBASE-744 was applied. Consequently, DynamicBloomFilter is broken in trunk. Could you please re-apply the remaining parts of this patch to trunk?;;;","28/Aug/08 16:36;jimk;In trunk, the only bloom filter that is used by HBase is BloomFilter. It is not possible to use a Dynamic or Counting bloom filter on a column. 

Do you have other uses for bloom filters such as in your applications?;;;","28/Aug/08 17:46;ab;Yes - the linked issue HADOOP-3063 uses DynamicBloomFilter to maintain a filter for a MapFile. Maintaining a dynamic filter costs less than calculating a filter once the MapFile is closed. Also, with the recently added ability to append to files you will be able to append to a MapFile, and then you would have to recalculate the whole filter - instead, if we use DynamicBloomFilter we will be able to update it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Master stuck splitting server logs in shutdown loop; on each iteration, edits are aggregated up into the millions",HBASE-551,12392643,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,29/Mar/08 22:00,12/Apr/08 16:12,01/Jul/25 07:49,31/Mar/08 20:53,0.1.0,0.1.1,0.2.0,,,0.1.1,0.2.0,,,,,,0,"Lars cluster is sick with master trying to split logs.   The logs its replaying have millions of edits in them.  

Here is sample from log.   First we get the shutdown and then in the shutdown process, we start to split up the shutdown servers log:

{code}
2008-03-28 16:29:45,305 INFO org.apache.hadoop.hbase.HMaster: process shutdown of server 192.168.105.37:60020: logSplit: false, rootRes
canned: false, numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
2008-03-28 16:29:45,310 INFO org.apache.hadoop.hbase.HLog: splitting 3 log(s) in hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/log_192
.168.105.37_1206741382563_60020
2008-03-28 16:29:45,311 DEBUG org.apache.hadoop.hbase.HLog: Splitting 0 of 3: hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/log_192.16
8.105.37_1206741382563_60020/hlog.dat.002
2008-03-28 16:29:45,380 DEBUG org.apache.hadoop.hbase.HLog: Creating new log file writer for path hdfs://lv1-xen-pdc-2.worldlingo.com:9
000/hbase/pdc-docs/488338803/oldlogfile.log and region pdc-docs,EP01108687NWA2,1205739919655
2008-03-28 16:29:45,390 DEBUG org.apache.hadoop.hbase.HLog: Creating new log file writer for path hdfs://lv1-xen-pdc-2.worldlingo.com:9
000/hbase/pdc-docs/447465883/oldlogfile.log and region pdc-docs,EP01900680NWA1,1205754584444
2008-03-28 16:29:45,403 DEBUG org.apache.hadoop.hbase.HLog: Creating new log file writer for path hdfs://lv1-xen-pdc-2.worldlingo.com:9
000/hbase/pdc-docs/2035706226/oldlogfile.log and region pdc-docs,EP01119588NWA2,1205754281917
2008-03-28 16:29:45,428 DEBUG org.apache.hadoop.hbase.HLog: Creating new log file writer for path hdfs://lv1-xen-pdc-2.worldlingo.com:9
000/hbase/pdc-docs/437772136/oldlogfile.log and region pdc-docs,EP00200190NWA2,120576451593
...
{code}

We open a file in each region to take edits.  We then start replaying the 3 WAL files from the regionserver.

On the second one, we get exception... 

{code}
2008-03-28 16:40:36,537 WARN org.apache.hadoop.hbase.HLog: Old log file hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/1045858
46/oldlogfile.log already exists. Copying existing file to new file
2008-03-28 16:40:36,545 DEBUG org.apache.hadoop.hbase.HLog: Creating new log file writer for path hdfs://lv1-xen-pdc-2.worldlingo.com:9
000/hbase/pdc-docs/104585846/oldlogfile.log and region pdc-docs,EP96104830NWA1,1205768785572
2008-03-28 16:40:36,979 DEBUG org.apache.hadoop.hbase.HLog: Copied 220000 edits
2008-03-28 16:40:38,853 DEBUG org.apache.hadoop.hbase.HLog: Applied 222812 total edits
2008-03-28 16:40:38,853 DEBUG org.apache.hadoop.hbase.HLog: Splitting 1 of 3: hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/log_192.16
8.105.37_1206741382563_60020/hlog.dat.003
2008-03-28 16:40:56,883 WARN org.apache.hadoop.hbase.HLog: Old log file hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/2118067
194/oldlogfile.log already exists. Copying existing file to new file
2008-03-28 16:40:56,891 DEBUG org.apache.hadoop.hbase.HLog: Creating new log file writer for path hdfs://lv1-xen-pdc-2.worldlingo.com:9
000/hbase/pdc-docs/2118067194/oldlogfile.log and region pdc-docs,EP97302517NWA2,1205726201776
2008-03-28 16:41:12,910 DEBUG org.apache.hadoop.hbase.HLog: Applied 36638 total edits
2008-03-28 16:41:12,910 DEBUG org.apache.hadoop.hbase.HLog: Splitting 2 of 3: hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/log_192.16
8.105.37_1206741382563_60020/hlog.dat.004
2008-03-28 16:41:18,684 WARN org.apache.hadoop.hbase.HMaster: Processing pending operations: ProcessServerShutdown of 192.168.105.37:60
020
java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:56)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1829)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1729)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1775)
        at org.apache.hadoop.hbase.HLog.splitLog(HLog.java:540)
        at org.apache.hadoop.hbase.HMaster$ProcessServerShutdown.process(HMaster.java:2167)
        at org.apache.hadoop.hbase.HMaster.run(HMaster.java:1085)

{code}

A finally clause makes sure we close up all the new files we've made in all regions.  These new files have accumulated some edits from the splitting of the first file.

Because we got an exception, the shutdown processing runs again.

Because regions have files in place with edits, we won't overwrite them second time through.  We instead copy the old into a new file to which we start appending until the exception happens again.

After a couple of hours, we're up into the millions of edits.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/08 22:17;stack;551.patch;https://issues.apache.org/jira/secure/attachment/12378875/551.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25260,,,,,Mon Mar 31 20:53:05 UTC 2008,,,,,,,,,,"0|i0h7zr:",98557,,,,,,,,,,,,,,,,,,,,,"29/Mar/08 22:17;stack;M  src/java/org/apache/hadoop/hbase/HLog.java
    (splitLog): If an exception processing a split, catch it.
    In finally, close and delete the split.  Don't try retrying.
    While in some circumstance, we might recover, its also
    likely that we just get same exception again.  If so, and
    multiple files, we'll just accumulate edits until the
    kingdom comes.;;;","29/Mar/08 22:29;stack;Here's sampling from logs after the loop has run for a few hours:

{code}
...
2008-03-29 02:01:45,609 DEBUG org.apache.hadoop.hbase.HLog: Applied 1893832 total edits
2008-03-29 02:03:39,417 DEBUG org.apache.hadoop.hbase.HLog: Applied 94150 total edits
2008-03-29 02:40:41,452 DEBUG org.apache.hadoop.hbase.HLog: Applied 1958102 total edits
2008-03-29 02:42:36,340 DEBUG org.apache.hadoop.hbase.HLog: Applied 96362 total edits
2008-03-29 03:20:39,816 DEBUG org.apache.hadoop.hbase.HLog: Applied 2022372 total edits
...
{code}

Root cause was regionserver running out of memory.;;;","30/Mar/08 23:04;jimk;Patch available for 0.1.1;;;","30/Mar/08 23:06;jimk;Reviewed patch. +1;;;","31/Mar/08 20:53;stack;Applied branch and TRUNK.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOF trying to read reconstruction log stops region deployment,HBASE-550,12392636,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,29/Mar/08 17:38,12/Apr/08 16:12,01/Jul/25 07:49,31/Mar/08 19:25,0.1.0,0.1.1,0.16.0,0.2.0,,0.1.1,0.2.0,,,,,,0,"Regions are just being reallocated over and over again because log file is hosed:

{code}
2008-03-29 10:37:53,762 ERROR org.apache.hadoop.hbase.HRegionServer: error opening region pdc-docs,EP92114798NWA1,1205741702057
java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readFully(DataInputStream.java:152)
        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1421)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1398)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1387)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1382)
        at org.apache.hadoop.hbase.HStore.doReconstructionLog(HStore.java:839)
        at org.apache.hadoop.hbase.HStore.<init>(HStore.java:773)
        at org.apache.hadoop.hbase.HRegion.<init>(HRegion.java:389)
        at org.apache.hadoop.hbase.HRegionServer.openRegion(HRegionServer.java:1159)        at org.apache.hadoop.hbase.HRegionServer$Worker.run(HRegionServer.java:1105)
        at java.lang.Thread.run(Thread.java:595)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/08 18:01;stack;550.patch;https://issues.apache.org/jira/secure/attachment/12378866/550.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25259,,,,,Mon Mar 31 19:25:01 UTC 2008,,,,,,,,,,"0|i0h7zj:",98556,,,,,,,,,,,,,,,,,,,,,"29/Mar/08 17:45;stack;Here is the infinite loop the above exception makes as seen from the master log:

{code}
2008-03-29 10:44:30,239 DEBUG org.apache.hadoop.hbase.HMaster: Main processing loop: ProcessRegionClose of pdc-docs,EP02075940NWA2,1205761493128, true, false
2008-03-29 10:44:30,239 INFO org.apache.hadoop.hbase.HMaster: region closed: pdc-docs,EP02075940NWA2,1205761493128
2008-03-29 10:44:30,239 INFO org.apache.hadoop.hbase.HMaster: reassign region: pdc-docs,EP02075940NWA2,1205761493128
2008-03-29 10:44:33,146 INFO org.apache.hadoop.hbase.HMaster: assigning region pdc-docs,EP02075940NWA2,1205761493128 to server 192.168.105.43:60020
2008-03-29 10:44:36,150 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,EP02075940NWA2,1205761493128 from 192.168.105.43:60020
2008-03-29 10:44:36,151 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_CLOSE : pdc-docs,EP02075940NWA2,1205761493128 from 192.168.105.43:60020
2008-03-29 10:44:36,151 INFO org.apache.hadoop.hbase.HMaster: 192.168.105.43:60020 no longer serving regionname: pdc-docs,EP02075940NWA2,1205761493128, startKey: <EP02075940NWA2>, endKey: <EP02080373NWA2>, encodedName: 456923460, tableDesc: {name: pdc-docs, families: {contents:={name: contents, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, language:={name: language, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, mimetype:={name: mimetype, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}}
2008-03-29 10:44:36,154 DEBUG org.apache.hadoop.hbase.HMaster: Main processing loop: ProcessRegionClose of pdc-docs,EP02075940NWA2,1205761493128, true, false
2008-03-29 10:44:36,154 INFO org.apache.hadoop.hbase.HMaster: region closed: pdc-docs,EP02075940NWA2,1205761493128
2008-03-29 10:44:36,154 INFO org.apache.hadoop.hbase.HMaster: reassign region: pdc-docs,EP02075940NWA2,1205761493128
2008-03-29 10:44:36,243 INFO org.apache.hadoop.hbase.HMaster: assigning region pdc-docs,EP02075940NWA2,1205761493128 to server 192.168.105.49:60020
2008-03-29 10:44:39,248 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,EP02075940NWA2,1205761493128 from 192.168.105.49:60020
2008-03-29 10:44:39,249 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_CLOSE : pdc-docs,EP02075940NWA2,1205761493128 from 192.168.105.49:60020
2008-03-29 10:44:39,249 INFO org.apache.hadoop.hbase.HMaster: 192.168.105.49:60020 no longer serving regionname: pdc-docs,EP02075940NWA2,1205761493128, startKey: <EP02075940NWA2>, endKey: <EP02080373NWA2>, encodedName: 456923460, tableDesc: {name: pdc-docs, families: {contents:={name: contents, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, language:={name: language, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, mimetype:={name: mimetype, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}}
2008-03-29 10:44:39,251 DEBUG org.apache.hadoop.hbase.HMaster: Main processing loop: ProcessRegionClose of pdc-docs,EP02075940NWA2,1205761493128, true, false
2008-03-29 10:44:39,251 INFO org.apache.hadoop.hbase.HMaster: region closed: pdc-docs,EP02075940NWA2,1205761493128
2008-03-29 10:44:39,251 INFO org.apache.hadoop.hbase.HMaster: reassign region: pdc-docs,EP02075940NWA2,1205761493128
{code}

A concurrent uploading is failing because regions never make it online (This is Lars' cluster).;;;","29/Mar/08 18:01;stack;{code}
M  src/java/org/apache/hadoop/hbase/HStore.java
    (Constructor) If an exception out of reconstructionLog method, log it and
    keep going.  Presumption is that its result of a lack of
    HADOOP--1700.
    (reconstructionLog): Check for empty log file.
{code};;;","30/Mar/08 22:56;jimk;Patch is available for 0.1.1;;;","30/Mar/08 23:01;jimk;Reviewed patch. +1 for 0.1.1
;;;","31/Mar/08 19:25;stack;Applied to branch and TRUNK.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A region's state is kept in several places in the master opening the possibility for race conditions,HBASE-543,12392212,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,jimk,jimk,24/Mar/08 22:26,13/Sep/09 22:26,01/Jul/25 07:49,24/Dec/08 01:38,0.1.0,0.1.1,0.2.0,,,0.19.0,,,master,,,,0,"A region's state exists in multiple maps in the RegionManager: unassignedRegions, pendingRegions, regionsToClose, closingRegions, regionsToDelete, etc.

One of these race conditions was found in HBASE-534.

For HBase-0.1.x, we should just patch the holes we find.

The ultimate solution (which requires a lot of changes in HMaster) should be applied to HBase trunk.

Proposed solution:

Create a class that encapsulates a region's state and provide synchronized access to the class that validates state changes.
There should be a single structure that holds regions in these transitional states and it should be a synchronized collection of some kind.
",,,,,,,,,,,,,HBASE-504,HBASE-546,,,,,,,,,HBASE-549,HBASE-1077,HBASE-599,,,,"22/Dec/08 21:32;jimk;543.patch;https://issues.apache.org/jira/secure/attachment/12396631/543.patch","21/Dec/08 04:18;jimk;543.patch;https://issues.apache.org/jira/secure/attachment/12396556/543.patch","25/Jun/08 06:01;stack;543.patch;https://issues.apache.org/jira/secure/attachment/12384651/543.patch","22/Dec/08 22:30;jimk;543.patch-4;https://issues.apache.org/jira/secure/attachment/12396638/543.patch-4","23/Dec/08 03:15;jimk;543.patch-5;https://issues.apache.org/jira/secure/attachment/12396647/543.patch-5","23/Dec/08 18:51;jimk;543.patch-6;https://issues.apache.org/jira/secure/attachment/12396689/543.patch-6","16/Jul/08 21:08;stack;recent-changes.patch;https://issues.apache.org/jira/secure/attachment/12386232/recent-changes.patch","18/Dec/08 21:16;jimk;regionstate.txt;https://issues.apache.org/jira/secure/attachment/12396428/regionstate.txt",,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25255,,,,,Wed Dec 24 07:13:29 UTC 2008,,,,,,,,,,"0|i0h7xz:",98549,,,,,,,,,,,,,,,,,,,,,"31/Mar/08 18:39;stack;Do we think we should address this for 0.1.1?  As I see it, this would require a rewrite of a core piece of the HMaster.  It would take some effort inside a pretty complex piece of code.  The effort would then be hard to forward-port because TRUNK has been significantly refactored in this regard.

I'd suggest that regards the branch, lets just patch any HBASE-534 holes we come across and do the proper fix in TRUNK?;;;","31/Mar/08 18:51;jimk;Yes, I think you are probably right. After looking at the scope of changes required here, doing them in 0.1.1 would be out of place. If we patch holes in 0.1.1 and then do the ""real solution"" in trunk, that would probably be more appropriate.;;;","25/Apr/08 00:09;bryanduxbury;This is bad.;;;","24/Jun/08 06:42;stack;Here's some state changes that currently look unsafe:

+ On open, the move from unassigned to pending is done non-transactionally: ServerManager line 438
+ In checkAssigned in BaseScanner, around line 350, we remove entry from unassignedRegions and from pendingRegions non-transactionally.

Looking to see if can squash together unassigned and pending.;;;","25/Jun/08 06:01;stack;javadoc, more specific about method returns, removed unnecessary treemaps, etc.

Still work to do.;;;","16/Jul/08 21:08;stack;Last set of changes made working toward this issue.;;;","16/Jul/08 21:11;stack;Moving out of 0.2.

This is bad but punting so can get a 0.2 RC out.  In my experience, we've not been burned by this issue recently.  We can live with ugly state of affairs a while longer.;;;","29/Oct/08 18:32;stack;Moving out of 0.19.0.  ZK will fix this.  Its looking like it might happen before this issue gets fixed.;;;","18/Dec/08 21:16;jimk;This file shows all the maps that are associated with region state, and how they change. There are several circumstances in which entries are not cleaned up if a server dies. There are also many possibilities for race conditions because the state of a region is maintained in multiple places, and a lot of them are not synchronized when referencing multiple maps.;;;","21/Dec/08 04:14;jimk;This is the fundamental root cause of HBASE-1051 and HBASE-1046;;;","21/Dec/08 04:19;jimk;Please review. Passes all regresssion tests and PE randomWrite 4 with 1 or 4 region servers.;;;","21/Dec/08 07:31;stack;How do I know this fixes the issues?  There are no tests nor description of how this addresses issues seen in referenced issues.  For example, it would be helpful if you explained how you replicated said issues so could test this patch actually addresses them.

There is a bunch of redoing of state processing but no explanation as to why?  For example:

{code}
+   * Remove a region from the region state map.
+   * 
+   * @param info
+   */
+  public void removeRegion(HRegionInfo info) {
{code}

Under what circumstance would I remove a region from state map?

The region state map itself has no explanation:

{code}
+  // Needs to be SortedMap so we can specify a comparator
+  private final SortedMap<byte[], RegionState> regionState =
+    Collections.synchronizedSortedMap(
+        new TreeMap<byte[], RegionState>(Bytes.BYTES_COMPARATOR));
{code}

The maps it would replace tried to explain what they were about.

Nor does the new RegionState map.

With above said, looks like this could be an improvement in that state is all in one place.

Should RegionState be looking for illegal states?  It doesn't seem to do any checking.  This would be a good place to check we're doing transitions properly.

Should resetting of connection root region be done inside unsetRootRegion in below so the two actions are tied together:

{code}
+      master.connection.setRootRegionLocation(null);
+      master.regionManager.unsetRootRegion();
{code}

Does unsetRootRegion set root region to null in regionManger?  Maybe connection and regionManager both need an unsetRootRegion method (or both a setRootRegionLocation that takes null) so same action in two places uses similarily named methods (This stuff preexisted your patch).

At first I thought all these things unsafe:

{code}
+      for (RegionState s: regionsToAssign) {
{code}

.. but now I see your comment that there is a lock held higher up on regionsToAssign.... good.

Enough for now.

Good stuff.;;;","21/Dec/08 08:18;apurtell;I am running with this patch applied now. ;;;","21/Dec/08 16:00;apurtell;+1 

The patch appears to fix the problems I've been seeing where errors take down a regionserver but the master gets confused and fails to reassign the affected regions. ;;;","21/Dec/08 23:32;stack;Andrew: Thats good news.

More comments on patch for Jim:

In RegionState, data members are named isUnassigned, isClosing, etc.  Usual javabean convention is that data members are named unassigned, closing, etc. and that isClosing is one of the names getters have.

Why dumpState and not toString?





;;;","22/Dec/08 18:32;jimk;Replicating problems:

HBASE-1046: Region assigned to two regionservers after split

Running a slightly modified version of TestSplit (in which there are
two region servers instead of one) under the debugger (with lease and
client timeouts jacked up), I set a breakpoint in
ServerManager.processSplitRegion. When that was hit, I put a
breakpoint in ServerManager.processRegionOpen just before the call to
RegionManager.setPending. I also cleared the breakpoint in
processSplitRegion but did not resume the thread, yet.

During the period in which the thread stopped in processSplitRegion
was stalled the meta scanner ran, assigning the split regions.

The region server(s) assigned the split regions opened them and
reported back, stopping in processRegionOpen between the point where
the split regions we no longer unassigned and before they became
pending. (This works because there are multiple threads that handle
the region server report messages).

Continuing the thread that had been stopped in processSplitRegion,
it now slid through the window I had created by pausing the region
open reports between removing the region from the unassigned list but
before they were marked pending, and the regions were marked
unassigned again.

RegionManager.setPending has no synchronization, but that doesn't
really matter because the synchronization needs to happen at the level
where multiple Map(s)/Set(s) are being updated (i.e., the region's
state is kept in serveral places opening the possibility for race
conditions).

RegionManager itself could not be synchronized without adding new
methods that performed the multiple operations while synchronized
on each of the structures being modified.
 
HBASE-1051: Regionserver attempting to open reassigned region but
            master ignoring, thinks region is still closing.

Using a method similar to that used for HBASE-1046 (with the test
being TestRegionRebalancing), I was found that there is a similar
window in which a region is marked ""no longer closing"" in
ServerManager.processRegionClose and the region is marked as ""no
longer unassigned"". If the meta scanner runs between these two points,
it will mark the region as unassigned, it could then get assigned, and
then be marked ""no longer unassigned"".

As a result, if the region server reports that it has opened the
region, ServerManager.processRegion open would interpret that
as a duplicate assignment, and tell the region server that just opened
the region to ""close without report"".

Since ProcessRegionClose does not delete the info:server field (nor
should it lest even worse race conditions ensue), the master still
sees a valid server when it scans the meta and will not attempt to
reassign the region.
;;;","22/Dec/08 18:38;jimk;A separate problem I found is related to the regionsToClose map:

{code}
  private final Map<String, Map<byte [], HRegionInfo>> regionsToClose =
    new ConcurrentHashMap<String, Map<byte [], HRegionInfo>>();
{code}

Note that the internal map is a HashMap. HashMaps do not work with byte[] as a key because the hash value will be different for two byte arrays, even if they have the same content. If byte[] is the key, then you must use a TreeMap with a Bytes.BYTES_COMPARATOR. Unless the TreeMap with comparator is used operations on the map such as get(), containsKey(), etc., will not work.;;;","22/Dec/08 18:56;stack;Above sounds good to me.;;;","22/Dec/08 19:06;jimk;@Stack:

> There is a bunch of redoing of state processing but no explanation
> as to why? For example: 
>
{code}
+   * Remove a region from the region state map.
+   * 
+   * @param info
+   */
+  public void removeRegion(HRegionInfo info) {
{code}
>
> Under what circumstance would I remove a region from state map?
>
> The region state map itself has no explanation:
>
{code}
+  // Needs to be SortedMap so we can specify a comparator
+  private final SortedMap<byte[], RegionState> regionState =
+    Collections.synchronizedSortedMap(
+        new TreeMap<byte[], RegionState>(Bytes.BYTES_COMPARATOR));
{code}
>
> The maps it would replace tried to explain what they were about.
> Nor does the new RegionState map.

Good points. The name of the map should really be regionsInTransition,
so removeRegion is called when a region completes the transitions from

unassigned -> assigned -> pending -> open

closing -> closed -> offline

closing -> closed -> unassigned -> assigned -> pending -> open

I will update the documentation and change the name of the map.

> With above said, looks like this could be an improvement in that
> state is all in one place. 

Yes, it is no longer possible for a region to be in multiple states at
once as state transitions always clear all the other states.

> Should RegionState be looking for illegal states? It doesn't seem to
> do any checking. This would be a good place to check we're doing
> transitions properly.

Good point. It should at least check for valid state transitions to
make sure that they happen in the proper order. Will update to include
this.

> Should resetting of connection root region be done inside
> unsetRootRegion in below so the two actions are tied together: 
>
{code}
+      master.connection.setRootRegionLocation(null);
+      master.regionManager.unsetRootRegion();
{code}

Yes. Good idea. The connection is visible inside this method.

> Does unsetRootRegion set root region to null in regionManger?

Yes.

> Maybe connection and regionManager both need an unsetRootRegion
> method (or both a setRootRegionLocation that takes null) so same
> action in two places uses similarily named methods (This stuff
> preexisted your patch). 

Yes. I think connection should have an unsetRootRegion instead of
setRootRegion(null). setRootRegion should only take a valid
HServerAddress. 

> In RegionState, data members are named isUnassigned, isClosing,
> etc. Usual javabean convention is that data members are named
> unassigned, closing, etc. and that isClosing is one of the names
> getters have.

Ok, no problem.

> Why dumpState and not toString?

No good reason. I was using it for debugging, and was going to remove
it, but having a toString is a good idea.

===

Making the above edits now. Do you want to re-review the patch when I'm done, or should I commit?;;;","22/Dec/08 19:31;stack;I'd suggest posting for review since fellas are using TRUNK in production.  Looking forward to it...;;;","22/Dec/08 21:31;jimk;Applying suggested changes;;;","22/Dec/08 21:32;jimk;Patch reflects changes that were requested by comments.;;;","22/Dec/08 21:32;jimk;New patch available that addresses review comments. Please review and comment.;;;","22/Dec/08 22:04;jimk;Oh, crud. I knew I forgot something. 

Need to add unsetRootRegion to HConnectionManager$TableServers and disallow setting rootRegionLocation to null).

If it is ok with everyone, if the new patch passes muster, I will make the above changes just before commit.;;;","22/Dec/08 22:09;stack;Last patch does not apply easily -- has cygwyn stuff in it.

Please name patches with versions so I can tell which is which.

Did you rename removeRegion?

-1 on making change before commit.  I want to test this stuff before it goes in.  Fellas are running production off trunk.  Need to spare them hiccups as much as we can.

Thanks.;;;","22/Dec/08 22:14;stack;Ok, one sec.... 

I've almost done testing this patch; could commit the latest version.;;;","22/Dec/08 22:26;stack;Patch passed with loading and start/stops of hbase.  Also passed unit tests.

Is this safe?
{code}
+        if (!master.regionManager.isUnassigned(i) &&
+            !master.regionManager.isAssigned(i.getRegionName()) &&
+            !master.regionManager.isPending(i.getRegionName())) {
+          master.regionManager.setUnassigned(i, false);
{code}
Should this all be done in a sync block inside in regionManager?  Or is it 'safe' because we process one message at a time?  There are a few of these multiple tests.  This glob of checks is done in a few places.  Put them together in a single method?  Would be easier to read (Latter point is not important for now).

If above is safe, then I'm +1 on committing last version of this patch.   Can address other minor stuff later.

;;;","22/Dec/08 22:30;jimk;New patch with ServerConnection/HConnectionManager.unsetRootRegion.

Also removed the cygwin C:/workspace...hbase-trunk/ prefix for files;;;","22/Dec/08 22:33;jimk;Latest patch is 543.patch-4

Added ServerConnection/HConnectionManager.unsetRootRegion and disallowed ServerConnection/HConnectionManager.setRootRegionLocation(null).

Also removed cygwin patch path (C:/workspace/hbase-trunk/)
;;;","22/Dec/08 22:40;jimk;As Stack pointed out, the multiple calls to check the state are windows of opportunity for race conditions (albeit pretty small ones). Working up a new patch that will synchronize these multiple tests.;;;","23/Dec/08 03:15;jimk;Addresses stack's comments about multiple ops in RegionManager that weren't synchronized.;;;","23/Dec/08 03:17;jimk;Addresses stack's comments about multip ops against RegionManager that were not synchronized. Please review and test.;;;","23/Dec/08 06:03;apurtell;Thanks Jim. I'm testing patch-5 now.;;;","23/Dec/08 07:14;stack;These broad locks look deadlock-prone especially as the lock is done from different threads:

{code}
+      synchronized (master.regionManager) {
{code}

Do you think them safe?

They don't seem to slow down the master as I at first thought (seems to keep up same basic rates).




;;;","23/Dec/08 09:50;apurtell;2008-12-23 04:43:03,882 INFO org.apache.hadoop.hbase.master.ServerManager: Received MSG_REPORT_SPLIT: content,aac240d1215a044b865e0faad888f048,1230019268119: content,aac240d1215a044b865e0faad888f048,1230019268119 split; daughters: content,aac240d1215a044b865e0faad888f048,1230025378559, content,ac7d850812026fa41ea8f31fe3a0b509,1230025378559 from 10.30.94.40:60020
2008-12-23 04:43:03,883 INFO org.apache.hadoop.hbase.master.RegionManager: assig
ning region content,aac240d1215a044b865e0faad888f048,1230025378559 to server 10.
30.94.40:60020
2008-12-23 04:43:03,884 INFO org.apache.hadoop.hbase.master.RegionManager: assigning region content,ac7d850812026fa41ea8f31fe3a0b509,1230025378559 to server 10.30.94.40:60020
2008-12-23 04:43:06,896 INFO org.apache.hadoop.hbase.master.ServerManager: Received MSG_REPORT_PROCESS_OPEN: content,ac7d850812026fa41ea8f31fe3a0b509,1230025378559 from 10.30.94.40:60020
2008-12-23 04:43:06,896 INFO org.apache.hadoop.hbase.master.ServerManager: Received MSG_REPORT_OPEN: content,aac240d1215a044b865e0faad888f048,1230025378559 from 10.30.94.40:60020
2008-12-23 04:43:06,896 INFO org.apache.hadoop.hbase.master.ServerManager: Received MSG_REPORT_OPEN: content,ac7d850812026fa41ea8f31fe3a0b509,1230025378559 from 10.30.94.40:60020
2008-12-23 04:43:06,896 DEBUG org.apache.hadoop.hbase.master.HMaster: Main processing loop: PendingOpenOperation from 10.30.94.40:60020
2008-12-23 04:43:06,896 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server 10.30.94.40:60020 is overloaded. Server load: 7 avg: 6.0, slop: 0.1
2008-12-23 04:43:06,896 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 1 regions. mostLoadedRegions has 7 regions in it.
2008-12-23 04:43:06,896 INFO org.apache.hadoop.hbase.master.ProcessRegionOpen$1: content,aac240d1215a044b865e0faad888f048,1230025378559 open on 10.30.94.40:60020
2008-12-23 04:43:06,896 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region content,aac240d1215a044b865e0faad888f048,1230025378559
2008-12-23 04:43:06,896 INFO org.apache.hadoop.hbase.master.ProcessRegionOpen$1: updating row content,aac240d1215a044b865e0faad888f048,1230025378559 in region .META.,,1 with startcode 1230014187119 and server 10.30.94.40:60020
2008-12-23 04:43:06,897 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 0 on 60000, call regionServerReport(address: 10.30.94.40:60020, startcode: 1230014187119, load: (requests=3, regions=7, usedHeap=713, maxHeap=1777), [Lorg.apache.hadoop.hbase.HMsg;@55c0d3a3, [Lorg.apache.hadoop.hbase.HRegionInfo;@66869e5
0) from 10.30.94.40:41999: error: java.io.IOException: java.lang.IllegalStateException: Cannot transition to closing from any other state. Region: content,aac240d1215a044b865e0faad888f048,1230025378559
java.io.IOException: java.lang.IllegalStateException: Cannot transition to closing from any other state. Region: content,aac240d1215a044b865e0faad888f048,1230025378559
        at org.apache.hadoop.hbase.master.RegionManager.setClosing(RegionManager.java:785)
        at org.apache.hadoop.hbase.master.RegionManager.unassignSomeRegions(RegionManager.java:439)
        at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:192)
        at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:381)
        at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:327)
        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:240)
        at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:570)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:894)
;;;","23/Dec/08 16:16;jimk;Patch has problems.;;;","23/Dec/08 16:56;jimk;@Andrew

Well the good news is that this problem prevented an inconsistent state in the master, as ProcessRegionOpen would have updated the meta with the original server when, in fact it was being to close that region.

The bad news, of course is that the region rebalancing did not work properly. unassignSomeRegions should not choose regions that are unassigned, assigned or pending.

@Stack
Yes, the lock on RegionManager is broad, however it was the only way I could see to guard multiple operations that effect both the regionsInTransition map and the onlineMetaRegions map, which happen in a couple of places. Separate locks for regionsInTransition and onlineMetaRegions would be more deadlock prone I thought. With this approach, every method that performs multiple operations on either map either grabs the RegionManager's monitor or waits while the current owner of the monitor does its thing and gets out. I don't think I grab RegionManager's monitor over any long running operation, but I will reverify that.;;;","23/Dec/08 17:32;stack;Understood.  Please check it out.  If you think it good, we can commit but would be good to avoid fellas in the field finding the deadlock.  My other concern is that this lock makes the master effectively single-threaded.;;;","23/Dec/08 17:33;stack;Other thing is that thats' great that assertions found an illegal transition.  Add more assertions!;;;","23/Dec/08 18:51;jimk;change unassignSomeRegions to skip over ones that are in transition.

verified that use of RegionManager's monitor does not span any long running operations.;;;","23/Dec/08 18:52;jimk;New patch (543.patch-6) addresses issues in previous patch;;;","23/Dec/08 19:28;apurtell;Despite the exception/assertion, or perhaps because of it, our cluster is still up and stable after hours of pounding and 100s of splits.

Will test with patch-6 now.;;;","23/Dec/08 22:25;stack;Patch is messy; some accesss to regionManager are synchronized whereas others are not.  Confusing.   Also still wary of many threads synchronizing on a single important master object afraid that someone in production is going to find a codepath that deadlocks -- and that it makes the master less 'live'.  That said, I ran it under duress and no noticable degradation and all unit tests pass.  Andrew Purtell's testimony in its favor sends me over to +1 on commit.  At a minimum, its a big improvement over what we had before.   Also, this stuff will be rewritten on ZK integration; we can do a better job then.;;;","24/Dec/08 01:38;jimk;Committed to trunk;;;","24/Dec/08 07:05;stack;I seem to be seeing more NSREs then ending in RetriesExhausted with this patch applied.  You Andrew?;;;","24/Dec/08 07:13;stack;Theory is that splits are taking longer.  Its pretty easy to compare old to new.  Will try that over next few days.;;;",,,,,,
We no longer wait on hdfs to exit safe mode,HBASE-537,12392028,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,stack,stack,21/Mar/08 04:42,22/Aug/08 21:13,01/Jul/25 07:49,21/Mar/08 21:43,0.1.0,0.2.0,,,,0.1.0,0.2.0,,,,,,0,"We used wait on hdfs to exit safe mode before going on to startup hbase but this feature is broken since we moved out of hadoop contrib.  Now when you try start with hdfs in safe mode you get:

{code}
08/03/21 04:39:56 FATAL hbase.HMaster: Not starting HMaster because:
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.SafeModeException: Cannot create directory /hbase010. Name node is in safe mode.
Safe mode will be turned off automatically.
        at org.apache.hadoop.dfs.FSNamesystem.mkdirsInternal(FSNamesystem.java:1571)
        at org.apache.hadoop.dfs.FSNamesystem.mkdirs(FSNamesystem.java:1559)
        at org.apache.hadoop.dfs.NameNode.mkdirs(NameNode.java:422)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
{code}

If you are lucky, it appears on STDOUT/ERR but may just be stuffed into logs and all looks like its running properly.

Noticed first by Lars George.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/08 21:06;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12378409/patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25251,,,,,Fri Mar 21 21:43:25 UTC 2008,,,,,,,,,,"0|i0h7wn:",98543,,,,,,,,,,,,,,,,,,,,,"21/Mar/08 05:18;stack;This may be a blocker; those with big clusters will be annoyed having to manually wait on exit of safe mode before starting hbase.  They can put in the way of the hbase startup, invocation of ./bin/hadoop dfsadmin --safemode wait.

In the past, it was suggested that hbase come up anyways, even if in safe mode in hdfs and that the UI and logs show us as blocked waiting on hdfs to exit safe mode.  Shouldn't be hard to do.  Might be our only alternative since hadoop is now elsewhere.  Could do the following pseudo-code in HMaster constructor:

{code}
...
    Path rootRegionDir =
      HRegion.getRegionDir(rootdir, HRegionInfo.rootRegionInfo);
    LOG.info(""Root region dir: "" + rootRegionDir.toString());

    try {
      // Before we make our first fs access, check to see if hdfs is in safe
      // mode and park here until it exists.
      if (ishdfs(this.conf)) {
        DFSAdmin admin = new DFSAdmin(this.conf);
        if (admin.isInSafeMode()) {
          admin.parkUntilWeExitSafeModeLoggingMessageEverySoOften();
        }
      }
...
    Go on to create -ROOT- and .META. if do not exist, etc.
{code}

Would need to move the startup of the master info server before this code with its own query about state of hdfs. ;;;","21/Mar/08 16:08;jimk;Not only should the master wait, but the region servers as well. This would suggest a common method in FSUtils.;;;","21/Mar/08 17:01;bryanduxbury;+1 to Jim's comment.;;;","21/Mar/08 18:01;stack;The regionservers already wait.  They do nothing until master tells them the hdfs to use.

If only master needs the wait facility, should be in master class? (Hadoop already exposes tools for querying state of safemode);;;","21/Mar/08 21:06;jimk;For 0.1.0, this patch works if Hadoop permissions are disabled or if the user starting HBase is the Hadoop Superuser.

Hadoop 0.17 will contain a fix that allows querying the state of the dfs without being superuser.;;;","21/Mar/08 21:09;bryanduxbury;Move ""String message = ..."" outside of the loop. Otherwise +1.;;;","21/Mar/08 21:11;jimk;? It is outside the loop:
{code}
      String message = ""Waiting for dfs to exit safe mode..."";
      while (((DistributedFileSystem) fs).setSafeMode(
          FSConstants.SafeModeAction.SAFEMODE_GET)) {
        System.out.println(message);
        LOG.info(message);
...
{code};;;","21/Mar/08 21:14;bryanduxbury;I misread.;;;","21/Mar/08 21:43;jimk;Committed to both 0.1.0 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Double-assignment at SPLIT-time (WAS: Stores retaining references to long-deleted mapfiles),HBASE-534,12391977,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,stack,stack,20/Mar/08 16:56,22/Aug/08 21:13,01/Jul/25 07:49,24/Mar/08 22:32,0.1.0,0.2.0,,,,0.1.0,0.2.0,,,,,,0,"Saw the following on the Lars clusters (He's up on 0.16.1 and very recent 0.1 branch) trying to run a scan over all his content:

{code}
java.io.IOException: java.io.IOException: HStoreScanner failed construction
        at org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2241)
        at org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2362)
        at org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2152)
        at org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1640)
        at org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1214)
        at org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1448)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
Caused by: java.io.FileNotFoundException: File hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/1733592281/contents/mapfiles/3435064940161142159/data does not exist.
        at org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:341)
        at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:538)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1387)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1382)
        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:254)
        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:242)
        at org.apache.hadoop.hbase.HStoreFile$HbaseMapFile$HbaseReader.<init>(HStoreFile.java:600)
        at org.apache.hadoop.hbase.HStoreFile$BloomFilterMapFile$Reader.<init>(HStoreFile.java:655)
        at org.apache.hadoop.hbase.HStoreFile$HalfMapFileReader.<init>(HStoreFile.java:758)
        at org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:424)
        at org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2216)
        ... 11 more

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:494)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:82)
        at org.apache.hadoop.hbase.HTable$ClientScanner.nextScanner(HTable.java:874)
        at org.apache.hadoop.hbase.HTable$ClientScanner.next(HTable.java:915)
        at org.apache.hadoop.hbase.hql.SelectCommand.scanPrint(SelectCommand.java:233)
        at org.apache.hadoop.hbase.hql.SelectCommand.execute(SelectCommand.java:100)
        at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:50)
        at org.apache.hadoop.hbase.Shell.main(Shell.java:114)
{code}

The scanner breaks when it hits the above exception.  The odd thing is that the referenced mapfile is out of a region that was deleted 4 days ago after purportedly all references had been let go:

{code}
2008-03-16 15:13:36,744 DEBUG org.apache.hadoop.hbase.HRegion: DELETING region hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/1733592281
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/08 21:07;stack;534.patch;https://issues.apache.org/jira/secure/attachment/12378507/534.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25250,,,,,Mon Mar 24 22:32:43 UTC 2008,,,,,,,,,,"0|i0h7vz:",98540,,,,,,,,,,,,,,,,,,,,,"20/Mar/08 20:49;stack;So, in the above exception, the complaint is about a file that was in the parent of this daughter region.

Looking back on the opening of this daughter region, I see following exceptions:

{code}
2008-03-16 14:12:40,624 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 2 files using hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/compaction.dir for 1663054471/contents2008-03-16 14:12:48,142 INFO org.apache.hadoop.fs.DFSClient: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.LeaseExpiredException: No lease on /hbase/pdc-docs/compaction.dir/1663054471/contents/mapfiles/5227279456974119035/data File does not exist. [Lease.  Holder: 44 46 53 43 6c 69 65 6e 74 5f 32 30 39 32 36 34 37 35 33 36, heldlocks: 0, pendingcreates: 3]
        at org.apache.hadoop.dfs.FSNamesystem.checkLease(FSNamesystem.java:1160)
        at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1097)
        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:312)
        at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:899)
        
        at org.apache.hadoop.ipc.Client.call(Client.java:512)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
        at org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2061)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:1954)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:1479)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1593)

... above exception then is retried:

2008-03-16 14:12:48,143 WARN org.apache.hadoop.fs.DFSClient: NotReplicatedYetException sleeping /hbase/pdc-docs/compaction.dir/1663054471/contents/mapfiles/5227279456974119035/data retries left 4
...

Until

2008-03-16 14:12:54,196 ERROR org.apache.hadoop.hbase.HRegionServer: Compaction failed for region pdc-docs,EP05011250NWA2,1205705554585
java.io.IOException: Could not get block locations. Aborting...        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:1824) 
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1479)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1571)
{code}

Perhaps it was the second file of the two files to compact?

Region goes on to open?  30 minutes later we try to compact again but fails with:

{code}
2008-03-16 14:42:40,726 ERROR org.apache.hadoop.hbase.HRegionServer: Compaction failed for region pdc-docs,EP05011250NWA2,1205705554585
java.io.FileNotFoundException: File hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/1733592281/contents/mapfiles/101719651524869690/data does not exist.
{code}

Subsequently, the region continues to 'work' but after every flush tries to compact and fails with:

{code}
2008-03-17 08:42:41,380 WARN org.apache.hadoop.hbase.HStore: Failed with java.io.FileNotFoundException: File hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/1733592281/contents/mapfiles/101719651524869690/data does not exist.: 1663054471/contents/5669005871484475450/1733592281/101719651524869690/bottom 1733592281/101719651524869690/bottom for 1663054471/contents
2008-03-17 08:42:41,422 ERROR org.apache.hadoop.hbase.HRegionServer: Compaction failed for region pdc-docs,EP05011250NWA2,1205705554585
java.io.FileNotFoundException: File hdfs://lv1-xen-pdc-2.worldlingo.com:9000/hbase/pdc-docs/1733592281/contents/mapfiles/101719651524869690/data does not exist.
        at org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:341)
        at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:538)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1387)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1382)
        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:254)
        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:242)
        at org.apache.hadoop.hbase.HStoreFile$HbaseMapFile$HbaseReader.<init>(HStoreFile.java:600)
        at org.apache.hadoop.hbase.HStoreFile$BloomFilterMapFile$Reader.<init>(HStoreFile.java:655)
        at org.apache.hadoop.hbase.HStoreFile$HalfMapFileReader.<init>(HStoreFile.java:758)
        at org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:424) 
        at org.apache.hadoop.hbase.HStore.compactHStoreFiles(HStore.java:1281)
        at org.apache.hadoop.hbase.HStore.compact(HStore.java:1245)
        at org.apache.hadoop.hbase.HRegion.compactStores(HRegion.java:793) 
        at org.apache.hadoop.hbase.HRegion.compactIfNeeded(HRegion.java:725)
        at org.apache.hadoop.hbase.HRegionServer$CompactSplitThread.run(HRegionServer.java:254) 
{code}

This seems like a simple enough exception to handle... should at least remove the Reader that is throwing the FNF from our list of ready-Readers.
;;;","24/Mar/08 20:06;stack;Studying more, root of problem is double assignment of region.  Below is master log excerpts.  See how pdc-docs,EP05011250NWA2,1205705554585 is doubly-assigned:

{code}
2008-03-16 15:12:36,626 DEBUG org.apache.hadoop.hbase.HMaster: Current assignment of pdc-docs,EP05011250NWA2,1205705554585 is not valid: storedInfo: null, startCode: -1, storedInfo.startCode: -1, unassignedRegions: false, pendingRegions: false
2008-03-16 15:12:37,463 INFO org.apache.hadoop.hbase.HMaster: assigning region pdc-docs,EP05011250NWA2,1205705554585 to server 192.168.105.21:60020
2008-03-16 15:12:38,406 INFO org.apache.hadoop.hbase.HMaster: region pdc-docs,EP05011250NWA2,1205704064309 split. New regions are: pdc-docs,EP05011250NWA2,1205705554585, pdc-docs,EP05076594NWA2,1205705554586
2008-03-16 15:12:38,499 INFO org.apache.hadoop.hbase.HMaster: assigning region pdc-docs,EP05011250NWA2,1205705554585 to server 192.168.105.39:60020
2008-03-16 15:12:40,490 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,EP05011250NWA2,1205705554585 from 192.168.105.21:60020
2008-03-16 15:12:40,490 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_OPEN : pdc-docs,EP05011250NWA2,1205705554585 from 192.168.105.21:60020
2008-03-16 15:12:40,490 INFO org.apache.hadoop.hbase.HMaster: 192.168.105.21:60020 serving pdc-docs,EP05011250NWA2,1205705554585
2008-03-16 15:12:40,491 INFO org.apache.hadoop.hbase.HMaster: regionname: pdc-docs,EP05011250NWA2,1205705554585, startKey: <EP05011250NWA2>, endKey: <EP05076594NWA2>, encodedName: 1663054471, tableDesc: {name: pdc-docs, families: {contents:={name: contents, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, language:={name: language, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, mimetype:={name: mimetype, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}} open on 192.168.105.21:60020
2008-03-16 15:12:40,491 INFO org.apache.hadoop.hbase.HMaster: updating row pdc-docs,EP05011250NWA2,1205705554585 in table .META.,,1 with startcode 1205660242301 and server 192.168.105.21:60020
2008-03-16 15:12:41,530 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : pdc-docs,EP05011250NWA2,1205705554585 from 192.168.105.39:60020
2008-03-16 15:12:41,530 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_OPEN : pdc-docs,EP05011250NWA2,1205705554585 from 192.168.105.39:60020
2008-03-16 15:12:41,530 INFO org.apache.hadoop.hbase.HMaster: 192.168.105.39:60020 serving pdc-docs,EP05011250NWA2,1205705554585
2008-03-16 15:12:41,530 INFO org.apache.hadoop.hbase.HMaster: regionname: pdc-docs,EP05011250NWA2,1205705554585, startKey: <EP05011250NWA2>, endKey: <EP05076594NWA2>, encodedName: 1663054471, tableDesc: {name: pdc-docs, families: {contents:={name: contents, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, language:={name: language, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, mimetype:={name: mimetype, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}} open on 192.168.105.39:60020
2006-03-16 15:12:41,530 INFO org.apache.hadoop.hbase.HMaster: updating row pdc-docs,EP05011250NWA2,1205705554585 in table .META.,,1 with startcode 1205660242518 and server 192.168.105.39:60020
2008-03-16 15:13:36,741 DEBUG org.apache.hadoop.hbase.HMaster: pdc-docs,EP05011250NWA2,1205705554585 no longer has references to pdc-docs,EP05011250NWA2,1205704064309
{code}

Happening because regionserver added regions to META, scan happened and marked the regions as needing assignment because server was empty and startcode -1, regions were added to unassignedRegions and actually assigned... then in came the SPLIT region message and regions were added again to unassignedRegions (no check in split block if regions already present).;;;","24/Mar/08 21:07;stack;M HMaster
  On MSG_REPORT_SPLIT, check pendingRegions before adding new region to unassignedRegions (May already be in-processing)
M HStore
  Edit log message.;;;","24/Mar/08 21:46;jimk;Reviewed patch. +1;;;","24/Mar/08 22:32;stack;Committed to 0.1 and TRUNK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Odd interaction between HRegion.get, HRegion.deleteAll and compactions",HBASE-532,12391833,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,jimk,jimk,19/Mar/08 05:33,22/Aug/08 21:13,01/Jul/25 07:49,17/Apr/08 03:20,0.1.1,0.2.0,,,,0.1.2,0.2.0,,,,,,0,"If you apply the patch for HBASE-483 to the 0.1 branch and comment out lines 309 and 315 of MetaUtils.java (which force compactions of the root and meta regions respectively), TestMergeTool fails. Why forcing compactions makes the test succeed is a mystery to me.",,,,,,,,,,,,,,,,,,,,,,,HBASE-29,HBASE-483,,,,,"14/Apr/08 18:28;stack;532.patch;https://issues.apache.org/jira/secure/attachment/12380084/532.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25249,,,,,Thu Apr 17 03:20:52 UTC 2008,,,,,,,,,,"0|i0h7vj:",98538,,,,,,,,,,,,,,,,,,,,,"19/Mar/08 05:34;jimk;I have no idea why this happens, but inserting these two compactions makes the test succeed.;;;","23/Mar/08 03:13;bryanduxbury;Forcing a compaction causes multiple mapfiles to unite, resolving any possible issues with the way get or getFull presume the data to be organized. That is likely the source of the issue.;;;","23/Mar/08 17:44;jimk;Is this a blocker for 0.1.0 or can it be incorporated into 0.1.1?;;;","23/Mar/08 18:37;bryanduxbury;I would say that unless we can confidently isolate the problem, it should be a 0.1.1 issue.;;;","24/Mar/08 01:57;bryanduxbury;I've been poking around with this problem. It is indeed quite odd that this keeps happening. 

A few things that stand out to me. First, it's totally possible that scanners and deleteAll don't play well together. Deletes that are not matched precisely to cells have been known to cause issues, so that might be something in play. 

Second, I noticed that you created the MetaScannerListener class to allow for easy scanning of the meta regions to find HRegionInfos of meta regions that host the regions we're trying to merge. Is there a reason you chose not to use the getClosestRowBefore facility that is commonly used to locate the appropriate meta regions? I'm not sure that this is a problem yet, but it is my suspicion this could be related. Additionally, I feel like there's a chance that the implementation used in Merge.java takes the first region info that could contain the row we're looking for instead of the last, which is what we actually want. I don't have data to back that up yet, though.

What is odd to me is that when I added some code to print out the contents, the row we're looking for really isn't in the region's data. So it's not just a question of not finding the data we're looking for, it's that we're looking for the wrong data up front. 

The fact that this problem comes into play only when there are more than one store file makes me nervous. We've seen the multi-storefile problem bite us in HBASE-524 and others. Could it be that scanners are not safe with multiple store files? Compaction sorts out all these problems by merging all the values and simplifying the scan.;;;","30/Mar/08 00:15;jimk;Is this the same issue as HBASE-29?;;;","01/Apr/08 20:55;jimk;Moving to 0.1.2;;;","09/Apr/08 04:11;stack;There is a hole in our memcache.  On flush, the flusher asks the Memcache for the snapshot to persist.  Memcache hands it over and then zeros out the snapshot.  Flusher goes to work but any subsequent gets will not see what was in snapshot until flusher finishes.  Scanners taken out after the flush starts will never see the edits.;;;","09/Apr/08 05:04;stack;Previous to r596835, 2007-11-20 13:53:30 -0800, HADOOP-2139 (phase 1) Increase parallelism in region servers, we used keep an history array of snapshots.  Scanners would travel over the history array.  Flushers when done would remove a snapshot from history (but because scanners had made a shallow copy of the history array, they'd still have hold of the history elements till scanning was done).  Let me bring back some of this -- at least the part where flusher tells memcache it completed the flush and the memcache keeping around 'beingFlushed' snapshot that we check when getting, and taking out scanners (will try to reduce on some of the copying too).;;;","10/Apr/08 17:33;stack;I was going to reinstate history array and then have the memcache scanner open iterators on all history array members.  Flusher would clear the tail of the array on occasion.  Scanners would still have hold references to history array elements so it could keep going.

Bryan D suggested that we just not open iterators at all.  He suggested that we just refactor Memcache Scanners so they use the get and getFull Memcache primitives.   We'd add a getNext kind of thing that would do like MapFile.getClosest only it would only return the next, not the asked-for row OR the next closest but the next closest only.

Reviewing, it looks like this approach would simplify things, use less memory but be slower -- but because the slowness is all up in memory, it won't show in general scan numbers.  I'm having a go at this latter suggestion.  Patch soon.;;;","14/Apr/08 17:55;stack;Make blocker for 0.1.2.;;;","14/Apr/08 18:28;stack;Below is commit message.  Passes all tests locally.

{code}
M  src/test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java
    Added calling of new clearSnapshot method to runSnapshot method.
    (testGetNextRow): Added. 
M  src/java/org/apache/hadoop/hbase/regionserver/Memcache.java
    Refactor memcache scanner so rather than hold open iterators against
    memcache, snapshot and history, instead we just do a 'get' on each
    scanner 'next'  (A new getNextRow returns the row that follows the
    'current' one -- the result of last call to 'next').  In past, we
    gave false impression that scanners gave the state of the data at
    time scanner was taken out. Fact was that scanners are per region
    and we scan across the table a region at a time -- so each scanner
    would be taken out at different times.  With this patch, rather, we
    embrace the fact that 'next' gives you state of data at time 'next'
    was called; regardless of when scan started.
    Added javadoc and comments.
    (Constructor): Removed. It did nothing.
    (createSortedMap): Added utility.
    (snapshot): Return the snapshot.  Must now be answered by new
    clearSnapshot method.  Also added getSnapshot for tests.
    (getLowest, getNextRow): Added.
    (MemcacheScanner): Refactored.  Got rid of running list of iterators.
    Got rid of copying current state of snapshot into a scanner backing map.
    Changed HAbstractScanner so we don't have to implement getFirstRow and
    getNext and closeSubscanner any more.
M  src/java/org/apache/hadoop/hbase/regionserver/HStore.java
    Add and edit of comments.  Added call to clearSnapshot when done flushing.
M  src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
    Moved code that used to be in scanner superclass here since its particular
    to this implementation, no longer shared by memcache.
M  src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java
    Moved  findFirstRow, closeSubScanner, getNext up into StoreFileScanner
    and out of here (as well as the particular next implementation).
M  src/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java
    Javadoc edit.
{code};;;","15/Apr/08 02:15;bryanduxbury; * If shapshot() is called and there's still entries in the snapshot, I think I'd prefer an exception right then instead of the last snapshot. That way we won't silently forget to call clearSnapshot anywhere.
 * Why should caller of clearSnapshot provide the snapshot back in? Afraid someone will try and clear the wrong snapshot somehow?
 * Is this worth an edit? Moreover, do we need the explicit ""this."" all over the place? (This might be a general stylistic question.)
{{{code}}}
-      memcache.put(key, value);
-      
+      this.memcache.put(key, value);
{{{code}}}
 * There's a note in getNextRow that it's not suppressing deletes. It should, shouldn't it? Otherwise we risk saying the wrong row is the next one. Could create some confusion.
 * What's this do? 
{{{code}}}
-              found_key.getTimestamp());
+              Long.valueOf(found_key.getTimestamp()));
{{{code}}}
 
... more to come. 
;;;","15/Apr/08 02:51;stack;On first comment, its snapshot is going to be called again because flush is going to fail; the way things are currently, we log warning that we're being recalled but we go again to try and flush what was in snapshot.  Your suggestion would make we'd throw an exception and then this region would dead in the water, at least as far as flushing is concerned.

On clearSnapshot, yeah, just a precaution.  Unlikely but if it happened, we'd want it logged.

Adding the 'this' is minor potatoes.  Yes, its stylistic.  I like them -- easy way telling data members from locals -- but I can see you have been actively removing them.

Onk getNextRow not suppressing deletes, yes it should. I was thinking we'd let the result out and then they'd be merged higher up but thats wrong behavior here -- scope is memcache scanner only.

On Long.valueOf, gives chance of internal Long cache returning a ready-made long -- if it exists (also suppresses an eclipse warning to the effect that the long is being auto-boxed up into a Long object).

Making a fix for getNextRow now...;;;","15/Apr/08 03:05;stack;Now I remember.  getNextRow does not suppress deletes because perhaps client is interested in deletes.  In scanner case, next calls getNextRow, then does a getFull.  If row only has deletes, next calls getFull again till it gets a row with content.  I can improve the method comment before I commit -- unless you have other things you'd like me fix?

Thanks for the review.;;;","15/Apr/08 04:38;stack;Happened on the Lars cluster a few days ago:

{code}
2008-04-08 01:41:09,087 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_OPEN : pdc-docs,US6933067_20050823,1207644063095 from 192.168.105.61:60020
2008-04-08 01:41:09,087 INFO org.apache.hadoop.hbase.HMaster: 192.168.105.61:60020 serving pdc-docs,US6933067_20050823,1207644063095
2008-04-08 01:41:09,088 INFO org.apache.hadoop.hbase.HMaster: regionname: pdc-docs,US6933067_20050823,1207644063095, startKey: <US6933067_20050823>, endKey: <US6966561_20051122>, encodedName: 582319641, tableDesc: {name: pdc-docs, families: {contents:={na
e: contents, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, language:={name: language, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, mimetype:={name: 
imetype, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}} open on 192.168.105.61:60020
2008-04-08 01:41:09,088 INFO org.apache.hadoop.hbase.HMaster: updating row pdc-docs,US6933067_20050823,1207644063095 in table .META.,,1 with startcode 1207556447704 and server 192.168.105.61:60020
2008-04-08 01:41:09,320 DEBUG org.apache.hadoop.hbase.HMaster: HMaster.metaScanner regioninfo: {regionname: pdc-docs,US6933067_20050823,1207644063095, startKey: <US6933067_20050823>, endKey: <US6966561_20051122>, encodedName: 582319641, tableDesc: {name: 
dc-docs, families: {contents:={name: contents, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, language:={name: language, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloo
 filter: none}, mimetype:={name: mimetype, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}}}, server: , startCode: -1
2008-04-08 01:41:09,320 DEBUG org.apache.hadoop.hbase.HMaster: Current assignment of pdc-docs,US6933067_20050823,1207644063095 is not valid: storedInfo: null, startCode: -1, storedInfo.startCode: -1, unassignedRegions: false, pendingRegions: false
2008-04-08 01:41:11,919 INFO org.apache.hadoop.hbase.HMaster: assigning region pdc-docs,US6933067_20050823,1207644063095 to server 192.168.105.67:60020
{code}

See how we update .META. with server and start code for 61 but the ongoing .META. scan does not see the new entry -- because we used to copy the current state of memcache into the scanner -- so it reassigns the region.

A mapfile goes missing in the huddle.  I'm going to blame this bug for now (two regions fighting over map files).
;;;","16/Apr/08 18:48;jimk;StoreFileScanner.next should be annotated with @Override

Otherwise +1
;;;","16/Apr/08 19:52;bryanduxbury;Memcache.java
 * What's with the weird ""/* ... \n*/"" comments over the variable declarations at the top? if you're not going to use 3 lines use 1.
 * createSortedMap should be createSyncSortedMap? 
 * I think this comment should be something like ""Must be followed by a call to clearSnapshot"". Leave off the bit about if it's not empty, etc. Let clearSnapshot figure out what to do if it's empty.
{code}
 +   * Must be answered by a call to {@link #clearSnapshot(SortedMap)} if
 +   * returned snapshot is not empty.
{code}
 * Rename ""ss"" parameter in clearSnapshot to something like oldSnapshot? Would be a little clearer to read at a glance.
 * Should get() create a list and pass it into both calls of internalGet? Would save us an object creation.
 * The private version of getNextRow should probably be called internalGetNextRow to follow the pattern of the rest of the methods in Memcache.
 * Private version of getNextRow should have a comment on the inner loop that indicates it's iterating because it needs to skip other cells of the same row. 
 * In MemcacheScanner#next, what's the purpose of the ""rr"" variable? Should have a name that actually describes its purpose.
 ;;;","16/Apr/08 20:11;bryanduxbury;StoreFileScanner.java
 * Should the constructor have timestamp and targetCols also be final?
 * In next, the nested condition  starting on 116 is gigantically miserable. Maybe it should be factored out into one or more methods? It would improve readability and capture the essence of what you're trying to do.
 ;;;","16/Apr/08 20:43;stack;Thanks Jim and Bryan for review.  I did all suggestions except following:

> Rename ""ss"" parameter in clearSnapshot to something like oldSnapshot? Would be a little clearer to read at a glance.
Left it as it was.  oldSnapshot is not correct -- there is no 'old' snapshot only the current snapshot -- and can't call it snapshot cos it clashes with data member.

> Should get() create a list and pass it into both calls of internalGet? Would save us an object creation.
Nah.  Keep that stuff local in the method rather than have client supply it.  If it were an object of some size, then yeah.

> The private version of getNextRow should probably be called internalGetNextRow to follow the pattern of the rest of the methods in Memcache.
Its the other methods that are incorrectly named w/ their internal prefix.  Marking method private should be sufficient to indicate internal-ness.

> In next, the nested condition starting on 116 is gigantically miserable. Maybe it should be factored out into one or more methods? It would improve readability and capture the essence of what you're trying to do.
I'm afraid to touch it.;;;","17/Apr/08 03:20;stack;Applied to TRUNK and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegionServer needs to recover if datanode goes down,HBASE-529,12391781,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,bryanduxbury,bryanduxbury,bryanduxbury,18/Mar/08 19:47,22/Aug/08 21:13,01/Jul/25 07:49,18/Mar/08 21:43,0.2.0,,,,,0.2.0,,,,,,,0,"If I take down a datanode, the regionserver will repeatedly return this error:

java.io.IOException: Stream closed.
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.isClosed(DFSClient.java:1875)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:2096)
        at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:141)
        at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:124)
        at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:41)
        at java.io.DataOutputStream.write(Unknown Source)
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)
        at org.apache.hadoop.hbase.HLog.append(HLog.java:377)
        at org.apache.hadoop.hbase.HRegion.update(HRegion.java:1455)
        at org.apache.hadoop.hbase.HRegion.batchUpdate(HRegion.java:1259)
        at org.apache.hadoop.hbase.HRegionServer.batchUpdate(HRegionServer.java:1433)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)

It appears that hbase/dfsclient does not attempt to reopen the stream.",,,,,,,,,,,,,,,,HBASE-497,,,,,,,,,,,,,"18/Mar/08 21:26;bryanduxbury;529.patch;https://issues.apache.org/jira/secure/attachment/12378174/529.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25248,,,,,Tue Mar 18 21:43:37 UTC 2008,,,,,,,,,,"0|i0h7uv:",98535,,,,,,,,,,,,,,,,,,,,,"18/Mar/08 19:47;bryanduxbury;Cloned so we can commit to 0.1 separately.;;;","18/Mar/08 21:26;bryanduxbury;Patch ported forward. Unit tests pass locally.;;;","18/Mar/08 21:26;bryanduxbury;Please review.;;;","18/Mar/08 21:39;stack;+1;;;","18/Mar/08 21:43;bryanduxbury;I just committed this to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table 'does not exist' when it does,HBASE-528,12391779,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,bryanduxbury,bryanduxbury,bryanduxbury,18/Mar/08 19:37,22/Aug/08 21:13,01/Jul/25 07:49,18/Mar/08 21:47,0.2.0,,,,,0.2.0,,,Client,,,,0,"This one I've seen a few times.  In hql, I do show tables and it shows my table.  I then try to do a select against the table and hql reports table does not exist.  Digging, whats happening is that the getClosest facility is failing to find the first table region in the .META. table.  I hacked up a region reading tool -- attached (for 0.1 branch) -- and tried it against but a copy and the actual instance of the region and it could do the getClosest fine.  I'm pretty sure I restarted the HRS and when it came up again, the master had given it again the .META. and again was failing to find the first region in the table (Looked around in server logs and it seemed 'healthy').",,,,,,,,,,,,,,,,HBASE-514,,,,,,,,,,,,,"18/Mar/08 21:45;bryanduxbury;528.patch;https://issues.apache.org/jira/secure/attachment/12378181/528.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25247,,,,,Tue Mar 18 21:47:27 UTC 2008,,,,,,,,,,"0|i0h7un:",98534,,,,,,,,,,,,,,,,,,,,,"18/Mar/08 19:37;bryanduxbury;Cloned so that we can apply to 0.1 and 0.2 separately.;;;","18/Mar/08 21:45;bryanduxbury;For trunk.;;;","18/Mar/08 21:47;bryanduxbury;I just committed this to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegexpRowFilter does not work when there are columns from multiple families,HBASE-527,12391693,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,clint.morgan,clint.morgan,18/Mar/08 00:10,22/Aug/08 21:13,01/Jul/25 07:49,22/Mar/08 22:36,0.1.0,0.2.0,,,,0.1.0,0.2.0,,Filters,,,,0,"If there are multiple column families, then creating a scanner with a RegexpRowFilter to match column values will mistakenly filter other columns.",,,,,,,,,,,,,,,,,,,,HBASE-476,,,,,HBASE-476,,,,"18/Mar/08 00:21;clint.morgan;hbase-527-v2.patch;https://issues.apache.org/jira/secure/attachment/12378093/hbase-527-v2.patch","18/Mar/08 00:20;clint.morgan;hbase-527.patch;https://issues.apache.org/jira/secure/attachment/12378092/hbase-527.patch","18/Mar/08 00:11;clint.morgan;hbase-527.patch;https://issues.apache.org/jira/secure/attachment/12378091/hbase-527.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25246,,,,,Sat Mar 22 22:36:18 UTC 2008,,,,,,,,,,"0|i0h7uf:",98533,,,,,,,,,,,,,,,,,,,,,"18/Mar/08 00:11;clint.morgan;Includes test which demonstrates the issue;;;","18/Mar/08 00:20;clint.morgan;Possible solution: hold off on full row examination by the filter (filterNotNull) until the row has been assembled.

Patch does this, and changes filter interface to use SortedMap (rather than TreeMap)

BTW, filterNotNull seems like an odd name here, perhaps I am missing something;;;","18/Mar/08 00:21;clint.morgan;last patch was same as the first one;;;","18/Mar/08 15:48;clint.morgan;The patch here also solves this related issue of a filter hitting columns spread across memcache and storeFile.;;;","22/Mar/08 22:36;jimk;Committed to 0.1.0 and trunk. Thanks Clint!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable.getRow(Text) does not work,HBASE-525,12391690,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,bryanduxbury,clint.morgan,clint.morgan,17/Mar/08 22:31,22/Aug/08 21:13,01/Jul/25 07:49,18/Mar/08 00:13,0.2.0,,,,,0.2.0,,,Client,,,,0,Updated from SVN to find that Hbase.getRow(Text) always return empty map.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/08 22:32;clint.morgan;hbase-525.patch;https://issues.apache.org/jira/secure/attachment/12378082/hbase-525.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25245,,,,,Tue Mar 18 00:13:59 UTC 2008,,,,,,,,,,"0|i0h7tz:",98531,,,,,,,,,,,,,,,,,,,,,"17/Mar/08 22:32;clint.morgan;this patch fixed it for me;;;","17/Mar/08 22:42;bryanduxbury;Wow, great catch. Looks like the tests only cover the region level, not the server level, which is why this was able to sneak through.;;;","17/Mar/08 23:47;bryanduxbury;+1 on supplied patch. Jim or Stack, can you weigh in and we'll commit?;;;","17/Mar/08 23:55;stack;+1 on patch (Fix indent on the line after '       if (columns != null) {' before committing).;;;","18/Mar/08 00:13;bryanduxbury;I just committed this patch. Thanks for the patch, Clint!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems with getFull,HBASE-524,12391680,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,bryanduxbury,bryanduxbury,bryanduxbury,17/Mar/08 20:14,22/Aug/08 21:13,01/Jul/25 07:49,18/Mar/08 19:34,0.1.0,0.2.0,,,,0.1.0,0.2.0,,regionserver,,,,0,"There are some issues with the implementation of getFull in HStore. 

 * If the loop encounters a deleted cell, it stops iterating. This correctly handles deletes, but then accidentally masks away any cells of different qualifiers that would come afterward
 * Since the mapfiles are search oldest to newest, and the results map is only updated when there isn't already a value in the results map for for the cell we're currently looking at, older values actually take precedence over newer ones. This may be fixed by simply reversing the order of mapfiles traversed to newest to oldest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/08 22:53;bryanduxbury;524-0.1.patch;https://issues.apache.org/jira/secure/attachment/12378085/524-0.1.patch","17/Mar/08 23:46;bryanduxbury;524-trunk.patch;https://issues.apache.org/jira/secure/attachment/12378089/524-trunk.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25244,,,,,Tue Mar 18 19:34:14 UTC 2008,,,,,,,,,,"0|i0h7tr:",98530,,,,,,,,,,,,,,,,,,,,,"17/Mar/08 22:28;bryanduxbury;The second problem above was invalid, it actually was scanning in the right order.

The second problem was a real issue, and in fact there was another problem: if the deletes weren't precisely matched to cells by timestamp, they would be ignored, resulting in an inaccurate view of the row.;;;","17/Mar/08 22:53;bryanduxbury;This patch introduces a new test case to make prove this stuff is actually broken, and then fixes to HStore to make it work as intended. All tests pass.;;;","17/Mar/08 22:53;bryanduxbury;Please review.;;;","17/Mar/08 23:46;bryanduxbury;Here's the same patch cut for trunk. Unit tests pass locally.;;;","18/Mar/08 18:54;stack;+1

Reviewed both patches.  Looks good.  In future, suggest using defines/constants for stuff used in 2 or more places such as this:

{code}
+      batchUpdate.put(COLUMNS[0], ""olderValue"".getBytes());
{code}

Later you test ""olderValue"" made it into the table.

I ran the 0.1 junit suite and all passed.;;;","18/Mar/08 19:34;bryanduxbury;I just committed this to 0.1 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table 'does not exist' when it does,HBASE-514,12390988,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,bryanduxbury,stack,stack,13/Mar/08 18:21,22/Aug/08 21:17,01/Jul/25 07:49,18/Mar/08 19:43,0.16.0,,,,,0.1.0,,,Client,,,,0,"This one I've seen a few times.  In hql, I do show tables and it shows my table.  I then try to do a select against the table and hql reports table does not exist.  Digging, whats happening is that the getClosest facility is failing to find the first table region in the .META. table.  I hacked up a region reading tool -- attached (for 0.1 branch) -- and tried it against but a copy and the actual instance of the region and it could do the getClosest fine.  I'm pretty sure I restarted the HRS and when it came up again, the master had given it again the .META. and again was failing to find the first region in the table (Looked around in server logs and it seemed 'healthy').",,,,,,,,,,,,,,,HBASE-528,,,,,,,,,,,,,,"14/Mar/08 18:21;bryanduxbury;514-0.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12377933/514-0.1-v2.patch","15/Mar/08 00:43;bryanduxbury;514-0.1-v3.patch;https://issues.apache.org/jira/secure/attachment/12377957/514-0.1-v3.patch","14/Mar/08 18:11;bryanduxbury;514-0.1.patch;https://issues.apache.org/jira/secure/attachment/12377932/514-0.1.patch","13/Mar/08 18:21;stack;region-v2.patch;https://issues.apache.org/jira/secure/attachment/12377818/region-v2.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25241,,,,,Fri May 30 06:11:46 UTC 2008,,,,,,,,,,"0|i0h7rj:",98520,,,,,,,,,,,,,,,,,,,,,"13/Mar/08 18:47;bryanduxbury;I'm not sure if this is the root or even part of the problem, but looking back at the code I wrote for getClosestRowBefore, the Memcache branch of the code isn't correct. It's pretty close, but I can envision some scenarios where it might produce bad results.

 * internalGetRowKeyAtOrBefore doesn't take into account potentially deleted cells. This probably isn't a big issue when dealing with region location stuff, but you never know. 
 * The check at the end of the tailMap branch of code in internalGetRowKeyAtOrBefore didn't make sure that the found row was still a match to the row we're searching for, meaning that in situations when the timestamp never matched for the found row but there were following rows, if the first following row's timestamp was less than the search timestamp, you'd get that row. Note that this means you'd actually get the first row AFTER the row you were looking for, which is the wrong behavior.
 * The handling of the headMap branch of internalGetRowKeyAtOrBefore is potentially incorrect. I assumed that the last cell in the headmap would contain the row we're looking for, but again don't check for possibly deleted cells. This means you could potentially get a row key for a region that had been deleted, say after a region split gets finalized in META. 

I'll whip up a patch to address these problems at least, so that we can feel a little more confident that it's doing the right thing.;;;","13/Mar/08 20:45;bryanduxbury;One additional issue i discovered - deletes in the memcache are not accounted for in the map files, either. So a flushed edit goes to disk, it could possibly be picked up by rowAtOrBeforeFromMapFile.;;;","13/Mar/08 21:56;bryanduxbury;I've been able to pretty easily fix the problem in the Memcache. However, I think I've hit something of an algorithmic snag when it comes to sorting out the closest row in the store files.

The problem lies in the fact that there's no guarantee newer store files are written with data that comes after data in older store files. For instance, a very recent store file could have data written from 10 minutes ago, but a store file written 10 minutes ago could have data timestamped an hour in the future. This means that you don't just need to check every store file for what it thinks is the closest matching row. You actually have to ask every store file what it thinks and then check it against *every other store file* to make sure it isn't deleted! Basically, figuring out the closest row with the current strategy is n^2 where n is the number of store files. 

Maybe there's a better strategy to investigate. Perhaps if we ask every store file what it thinks the best candidate is, then we sort the results by row, and then scan all the store files from the first candidate to the last candidate, noting any deletes as we go, and then ultimately choosing the maximum row remaining in that list. Would that actually even help?;;;","13/Mar/08 23:58;bryanduxbury;So perhaps the most palatable solution is to clearly specify that getClosestRowBefore is only meant to be used on the .META. table, since it has very simple characteristics. 

In particular, writes in .META. (or -ROOT-, used interchangeably) will always be written with the latest timestamp, meaning that older map files will never have future-dated information. Then, we can start with the memcache, then begin with the oldest map file and advance forward searching for better qualified rows and excluding rows that have been deleted in later map files. ;;;","14/Mar/08 00:02;bryanduxbury;In looking at the code, I've found one further problem with the way its original implemented. In HStore#getRowKeyAtOrBefore, if the Memcache returns a non-null key, then we will automatically use that without looking in the mapfiles at all. This means that until there is a flush, things are fine. However, as soon as the memcache flushes once, and there are some writes into memcache, then the Memcache will return incorrect answers about the closest row. This actually seems like it could explain difficulties with WREs because when your HBase gets reasonably large, there will be a .META. region flush, and then you're hosed for a while.

Instead we need to take the answer the memcache gives us and merge it with the results we get from the map files. Should be a fairly easy fix.;;;","14/Mar/08 18:11;bryanduxbury;Here's a go at it. The logic is much more complicated, though it shouldn't be too impossible to follow. The unit tests pass locally;;;","14/Mar/08 18:12;bryanduxbury;Please review the 0.1 patch. I will put together a trunk patch when I am convinced the 0.1 is sufficient.;;;","14/Mar/08 18:21;bryanduxbury;Jim couldn't get the patch to apply cleanly, here's a new version.;;;","14/Mar/08 20:01;stack;This complicated patch is missing commit message.

I like the way you removed it from HRegionInterface and removed ts.  Should it be package private in HRegion?

HashSet is not used in HStore

Why are we not allows searching w/ 'key'?  Why are we using the earliest key?

{code}
      // we want the earliest possible to start searching from                                                                                                |       }                                                                                                                                                     
      HStoreKey search_key = candidateKeys.isEmpty() ?                                                                                                        |     }                                                                                                                                                       
        new HStoreKey(key) : new HStoreKey(candidateKeys.first().getRow());     
{code}

Regards your strip timestamp method, creating a new HSK is safest but do you need to create a new instance?  (Could setVersion on current hsk).

Do you think candidateKeys size could be large or the aggregate of the values would blow memory?  Do we have to carry the values?  Can we get away with just yes no on whether a delete value or not?

Whas happening here?
{code}
          if (found_key.getRow().compareTo(key) <= 0) {
            if (HLogEdit.isDeleted(tailMap.get(found_key))) {
              candidateKeys.remove(stripTimestamp(found_key));
{code}

Deletes cells only delete cells of same r/c/ts combo.  Here you are deleting any key that has same r/c without regard for ts -- including cells newer than the found_key.  Is that intentional? (Happens in two places)

Incomplete comment: +        // empty. examine all the keys of the

In this code:

{code}
              // if the last row we found a candidate key for is different than
              // the row of the current candidate, we can stop looking.
              if (lastRowFound != null && !lastRowFound.equals(thisKey.getRow())) {
                break;
              }
{code}

Why break, why not keeping moving forward till we get even closer to the passed in key?

Remove commented out code.

Is the return premature in below or is rationale that because mapfiles are sorted, we look at first one and its last key is 'highest' for this store?
{code}
        if (finalKey.getRow().compareTo(row) < 0) {
          candidateKeys.add(stripTimestamp(finalKey));
          return;
        } 
{code}

Unit tests in branch all test.  I ran a randomWrite -- thinking randomWriting would exercise this patch --  PE with 4 clients and it succeeded (8 clients OOME'd though heap of 1.6G).  Means that you haven't broken anything it seems.




;;;","14/Mar/08 20:37;bryanduxbury;> Why are we not allows searching w/ 'key'? Why are we using the earliest key?
If there are any keys in the candidateKeys, then we want to start with the first one in there. If there are keys in candidateKeys, then they are guaranteed to be <= the search row's key. We need to start as early as possible because we also want to scan over regions where there might be deletes for candidates from earlier on. If we always just started at the search row, we wouldn't know if earlier candidate keys had been suppressed later on. 

> Regards your strip timestamp method, creating a new HSK is safest but do you need to create a new instance? (Could setVersion on current hsk).
I have no problem changing this method. Figuring cut down on object creations?

> Do you think candidateKeys size could be large or the aggregate of the values would blow memory? Do we have to carry the values? Can we get away with just yes no on whether a delete value or not?
We do not carry the values, just the keys. If the keys happen to be very large, then there is the possibility of using up lots of memory. Moreover, the idea is that when we encounter a delete, we'll use that event to remove the key that corresponds to it from the set of candidate keys. The candidate keys set should grow and shrink as we find and entertain more candidate keys. 

> In this code: ...
Actually, in this case, we're iterating through the map backwards, so once we've found one row that survives the whole way, then we've found the closest row already, and should stop looking.

> Is the return premature in below or is rationale that because mapfiles are sorted, we look at first one and its last key is 'highest' for this store?
Yes.
;;;","14/Mar/08 22:25;jimk;I found instances where we delete cells using TIMESTAMP_LATEST, hence
stripping timestamps is absolutely  the right thing to do.
- HMaster$ChangeTableState.postProcessMeta
- HRegion.offlineRegionInMETA

The following should call deleteAll:
- HMerge${OfflineMerger,OnlineMerger).updateMeta
- DisabledTestScanner2.removeRegionFromMETA

HStore:
- public method HStore$Memcache.getRowKeyAtOrBefore is missing javadoc for parameters and exceptions
- stripTimestamp should be package private and not private because it causes synthetic accessors to be generate so Memcache can access it. Will also make port to trunk easier
- public method HStore.getRowKeyAtOrBefore is missing javadoc for parameters, return value and exceptions thrown, also remove commented code.
- unnecessary else at line 1869
- comments should include examples to show why scanning from oldest to newest is important. We'll all have forgotten why 6 months from now.


> stack - 14/Mar/08 01:01 PM
> This complicated patch is missing commit message.
+1

> HashSet is not used in HStore

Neither is Set

> Regards your strip timestamp method, creating a new HSK is safest
> but do you need to create a new instance? (Could setVersion on current
> hsk). 

If you did setVersion on current HSK, then you would be invalidating
the map that it was being read from (memcache or snapshot)

;;;","15/Mar/08 00:43;bryanduxbury;Here's another patch. It incorporates a lot of the fixes suggested by Jim and Stack.

Before it gets committed, I will definitely take the time to write up examples and rationale.

Review please.;;;","18/Mar/08 19:02;stack;+1

Reviewed till my head hurt and then went and ran load test.  Completed w/o error (no empty cell stragglers in master scans of meta regions).  Unit tests also passed.;;;","18/Mar/08 19:43;bryanduxbury;I just committed this to 0.1 and cloned the issue for trunk.;;;","29/May/08 10:09;pratyushb;I have been using Hbase for some time now. We have set up a web-crawler and we are downloading the data into two tables in Hbase. I am using Hbase 0.1.1 for our purpose.

There is a particular table web_content having the following structure...

| Column Family Descriptor                                                                                |
| name: content, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, 
bloom filter: none                                                                 |
| name: content_length, max versions: 1, compression: BLOCK, in memory: false,max length: 32, 
bloom filter: none                                                                    |
| name: content_type, max versions: 1, compression: BLOCK, in memory: false, max length: 100, 
bloom filter: none                                                                       |
| name: crawl_date, max versions: 1, compression: BLOCK, in memory: false, max length: 1000,
bloom filter: none                                                                         |
| name: http_headers, max versions: 1, compression: BLOCK, in memory: false, max length: 10000,
bloom filter: none                                                                    |
| name: last_modified_date, max versions: 1, compression: BLOCK, in memory: false, max length: 100,
bloom filter: none                                                               |
| name: outlinks_count, max versions: 1, compression: BLOCK, in memory: false, max length: 100,
bloom filter: none                                                                    |
| name: parsed_text, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647,
bloom filter: none                                                              |
| name: title, max versions: 1, compression: BLOCK, in memory: false, max length: 1000
, bloom filter: none                                                                                 |

We are using Heritrix-2.0.1 as our crawling engine and we have created a Hbase writer which writes the contents of the downloaded pages into the above table.
Initially the crawler runs fine and i often query the web_conten table with ""Select count(*) from web_content"" to get the rate at which URLs are written in the table. However wehn the crawler runs for hours, and we have nearly 40K-50K urls in the table, the table suddenly seems to dissappear. Querying with the above query returns ""web_content"" is an non-existant table.

This has occured multiple times to me, and i found out that there was already a JIRA(HBASE-514) on this.  It says that this issue has been fixed in version 0.1.0. We are using 0.1.1 which is a later version but the problem seems to exist.

we have a small cluster of 6-10 machines wherein we are running hadoop-0.16.3 and hbase 0.1.1. One machine constitutes the NameNode, SecondaryNameNode, HMaster. While other machines form regionservers and datanodes.
I had run the system with DEBUG enabled and i am attaching the log files for help.

this is a screenshot of the error thati am facing...

hql > select count(*) from web_content;
08/05/29 09:38:13 INFO hbase.HTable: Creating scanner over web_content starting at key
08/05/29 09:38:13 DEBUG hbase.HTable: Advancing internal scanner to startKey
08/05/29 09:38:13 DEBUG hbase.HTable: New region: address: 10.178.87.27:60020, regioninfo: regionname: web_content,,1212056557578, startKey: <>, endKey: <>, encodedName: 1099787575, tableDesc: {name: web_content, families: {content:={name: content, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}, content_length:={name: content_length, max versions: 1, compression: BLOCK, in memory: false, max length: 32, bloom filter: none}, content_type:={name: content_type, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}, crawl_date:={name: crawl_date, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}, http_headers:={name: http_headers, max versions: 1, compression: BLOCK, in memory: false, max length: 10000, bloom filter: none}, last_modified_date:={name: last_modified_date, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}, outlinks_count:={name: outlinks_count, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}, parsed_text:={name: parsed_text, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}, title:={name: title, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}}}
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.UnknownScannerException: Name: -7311630080500504399
        at org.apache.hadoop.hbase.HRegionServer.next(HRegionServer.java:1425)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)

        at org.apache.hadoop.ipc.Client.call(Client.java:512)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:210)
        at $Proxy1.next(Unknown Source)
        at org.apache.hadoop.hbase.HTable$ClientScanner.next(HTable.java:914)
        at org.apache.hadoop.hbase.hql.SelectCommand.scanPrint(SelectCommand.java:233)
        at org.apache.hadoop.hbase.hql.SelectCommand.execute(SelectCommand.java:100)
        at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:50)
        at org.apache.hadoop.hbase.Shell.main(Shell.java:114)
8797 row(s) in set. (140.71 sec)
hql > select count(*) from web_content;
08/05/29 09:45:34 INFO hbase.HTable: Creating scanner over web_content starting at key
08/05/29 09:45:34 DEBUG hbase.HTable: Advancing internal scanner to startKey
08/05/29 09:45:34 DEBUG hbase.HTable: New region: address: 10.178.87.27:60020, regioninfo: regionname: web_content,,1212056557578, startKey: <>, endKey: <>, encodedName: 1099787575, tableDesc: {name: web_content, families: {content:={name: content, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}, content_length:={name: content_length, max versions: 1, compression: BLOCK, in memory: false, max length: 32, bloom filter: none}, content_type:={name: content_type, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}, crawl_date:={name: crawl_date, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}, http_headers:={name: http_headers, max versions: 1, compression: BLOCK, in memory: false, max length: 10000, bloom filter: none}, last_modified_date:={name: last_modified_date, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}, outlinks_count:={name: outlinks_count, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}, parsed_text:={name: parsed_text, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}, title:={name: title, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}}}
08/05/29 09:45:44 DEBUG hbase.HTable: reloading table servers because: org.apache.hadoop.hbase.NotServingRegionException: web_content,,1212056557578
        at org.apache.hadoop.hbase.HRegionServer.getRegion(HRegionServer.java:1639)
        at org.apache.hadoop.hbase.HRegionServer.getRegion(HRegionServer.java:1611)
        at org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1480)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)

08/05/29 09:45:44 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: region offline: web_content,,1212056557578
08/05/29 09:45:54 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: HRegionInfo was null or empty in .META.
08/05/29 09:46:04 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: HRegionInfo was null or empty in .META.
org.apache.hadoop.hbase.TableNotFoundException: Table 'web_content' does not exist.
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:418)
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:350)
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:318)
        at org.apache.hadoop.hbase.HTable.getRegionLocation(HTable.java:114)
        at org.apache.hadoop.hbase.HTable$ClientScanner.nextScanner(HTable.java:889)
        at org.apache.hadoop.hbase.HTable$ClientScanner.<init>(HTable.java:817)
        at org.apache.hadoop.hbase.HTable.obtainScanner(HTable.java:522)
        at org.apache.hadoop.hbase.HTable.obtainScanner(HTable.java:411)
        at org.apache.hadoop.hbase.hql.SelectCommand.scanPrint(SelectCommand.java:219)
        at org.apache.hadoop.hbase.hql.SelectCommand.execute(SelectCommand.java:100)
        at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:50)
        at org.apache.hadoop.hbase.Shell.main(Shell.java:114)
0 row(s) in set. (40.37 sec)

Interestingly after this if I query the Hbase using show tables, it still shows me two tables.

If anybody can tell me what exactly is going wrong or what is the intended fix, it will be of great help.

thanks

Pratyush;;;","29/May/08 16:59;stack;Would suggest you update to 0.1.2.  Lots of critical fixes including fixes for what you are seeing above, I'd guess.  No migration needed going between 0.1.1 and 0.1.2 so upgrade is just matter of replacing the software.

Regards your schema, why have so many families?  Would suggest you put the content into one family and then all other attributes into another family.

Good to hear you have a writer for Heritrix (Is it based off the work by Ankur Goel -- and the code I passed him)?

FYI, rather than stick your question into a closed issue, you might do better next time posting to the list.







;;;","30/May/08 06:11;pratyushb;
 Dear Stack,

Sorry about the inconvenience caused. Actually putting the log data in the mail made my office server detect it as a spam and hence i was unable to send a mail to the group.? 

Thanks for your suggestions. I will definitely upgrade to 0.1.2. Your suggestions regrading the schema is also very helpful, we would look into it to make necessary adjustments. 

Yes this is indeed the code that was worked upon by Ankur Goel, he happens to be my teammate !!! 

thanks and regards

Pratyush


 


 

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HConnectionManger.listTables returns empty list if exception (though there may be many tables present),HBASE-510,12390973,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,bryanduxbury,stack,stack,13/Mar/08 16:41,22/Aug/08 21:13,01/Jul/25 07:49,14/Mar/08 03:43,0.1.0,0.16.0,0.2.0,,,0.1.0,0.2.0,,Client,,,,0,"Its a problem because commonly a check for existence will get list of current tables.

Yesterday saw problem when .META. went off line.  A piece of client code was asking for list of tables when .META. was offline, it was getting back an empty list because listTables do while was seeing 'org.apache.hadoop.hbase.NotServingRegionException: .META.,,1'

Problem is the do while in HCM.listTables goes as long as startRow does not equal LAST_ROW but startRow is initialized with EMPTY_START_ROW which is equal to LAST_ROW.



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/08 00:15;bryanduxbury;510-0.1.patch;https://issues.apache.org/jira/secure/attachment/12377851/510-0.1.patch","14/Mar/08 01:16;bryanduxbury;510-trunk.patch;https://issues.apache.org/jira/secure/attachment/12377857/510-trunk.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25240,,,,,Fri Mar 14 03:43:24 UTC 2008,,,,,,,,,,"0|i0h7qn:",98516,,,,,,,,,,,,,,,,,,,,,"13/Mar/08 16:53;bryanduxbury;Ah, interesting. The first step is to add retry logic to that section of code, I would say. Currently it assumes a successful passage straight through.;;;","14/Mar/08 00:15;bryanduxbury;Patch for 0.1 that adds retry logic around the listTables scan code. Passes unit tests.;;;","14/Mar/08 00:15;bryanduxbury;Please review.;;;","14/Mar/08 01:16;bryanduxbury;Patch for trunk.;;;","14/Mar/08 03:31;stack;+1 Ran tests on branch and passed.  Reviewed both patches.  Look fine.;;;","14/Mar/08 03:43;bryanduxbury;Committed to 0.1 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In master, there are a load of places where no sleep between retries",HBASE-507,12390913,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,stack,stack,13/Mar/08 06:26,22/Aug/08 21:13,01/Jul/25 07:49,05/Apr/08 02:13,0.2.0,,,,,0.2.0,,,,,,,0,"Here is an example:

{code}
 270308 2008-03-12 14:10:02,054 DEBUG org.apache.hadoop.hbase.HMaster: numberOfMetaRegions: 1, onlineMetaRegions.size(): 1                                                                                                                                             
270309 2008-03-12 14:10:02,054 DEBUG org.apache.hadoop.hbase.HMaster: process server shutdown scanning .META.,,1 on XX.XX.XX.184:60020 HMaster                                                                                                                        
270310 2008-03-12 14:10:02,056 DEBUG org.apache.hadoop.hbase.HMaster: process server shutdown scanning .META.,,1 on XX.XX.XX.184:60020 HMaster                                                                                                                        
270311 2008-03-12 14:10:02,057 DEBUG org.apache.hadoop.hbase.HMaster: process server shutdown scanning .META.,,1 on XX.XX.XX.184:60020 HMaster                                                                                                                        
270312 2008-03-12 14:10:02,059 DEBUG org.apache.hadoop.hbase.HMaster: process server shutdown scanning .META.,,1 on XX.XX.XX.184:60020 HMaster
270313 2008-03-12 14:10:02,060 DEBUG org.apache.hadoop.hbase.HMaster: process server shutdown scanning .META.,,1 on XX.XX.XX.184:60020 HMaster
270314 2008-03-12 14:10:02,062 WARN org.apache.hadoop.hbase.HMaster: Processing pending operations: ProcessServerShutdown of XX.XX.XX.180:60020                                                                                                                       
270315 org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException .META.,,1                                                                                                                                                 
270316         at org.apache.hadoop.hbase.HRegionServer.getRegion(HRegionServer.java:1606) 
...
{code}

Whats actually going on here is 5 retries without a wait in between (logging should include index numbering retry.  Seems to be a bunch of duplicated code around retrying that we might be able to fix with a Callable.  Jim Firby today suggested we do expotential backoffs in our retries. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/08 18:45;jimk;507-0.1.patch;https://issues.apache.org/jira/secure/attachment/12379049/507-0.1.patch","04/Apr/08 22:51;jimk;507-trunk.patch;https://issues.apache.org/jira/secure/attachment/12379438/507-trunk.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25237,,,,,Sat Apr 05 02:13:55 UTC 2008,,,,,,,,,,"0|i0h7pz:",98513,,,,,,,,,,,,,,,,,,,,,"13/Mar/08 06:47;stack;Above looping is responsible for 20k+ lines within about 6 seconds in the log I'm currently looking at.;;;","23/Mar/08 03:52;bryanduxbury;We should make (or extend) tools for doing timed retry operations against regionservers, and then use them in the master and possibly HTable. The ServerCallable stuff in HTable might be a good starting point. 

Perhaps this should also incorporate the exponential backoff stuff? (HBASE-511);;;","23/Mar/08 17:43;jimk;Is this a blocker for 0.1.0 or should it be incorporated in 0.1.1?;;;","23/Mar/08 18:42;bryanduxbury;I think we should do a simple retry scheme in 0.1 and plan for something more complex in 0.2. It's pretty much unacceptable that we don't pause between retries in the master - makes the logs harder to debug and puts undue stress on the whole system. ;;;","23/Mar/08 22:09;stack;I'm fine if this is not fixed in 0.1.  Doesn't seem critical.

If we do fix it, please lets not do it duplicating code if possilble.  Can we use the Callable pattern from the Client here too?;;;","01/Apr/08 18:45;jimk;patch for 0.1;;;","01/Apr/08 18:45;jimk;Patch for 0.1 available. Please review.;;;","01/Apr/08 18:57;stack;+1 as patch for 0.1 branch.  Lets do real fix in 0.2.;;;","01/Apr/08 20:30;jimk;Committed patch for 0.1 branch.;;;","01/Apr/08 20:31;jimk;Unmarking for 0.1 because patch has been committed.;;;","02/Apr/08 21:14;bryanduxbury;Should we clone this issue to 0.2 and remark this one as 0.1 so that it's clear it both got done in 0.1 and isn't actually patch available in 0.2?;;;","02/Apr/08 21:29;jimk;If you want to do the extra work, go ahead. I almost have changes ready for 0.2.;;;","02/Apr/08 21:31;jimk;Patch was for 0.1;;;","02/Apr/08 21:32;jimk;Started work on 0.2;;;","04/Apr/08 22:51;jimk;The ""real"" fix for trunk. Uses Callable pattern.;;;","04/Apr/08 22:52;jimk;507-trunk.patch is the ""real"" fix for this problem, not the band-aid we applied to 0.1 branch.

Please review.;;;","04/Apr/08 23:41;bryanduxbury;RetryableOperation 
 * Needs a class comment.
 * It's not a generically retryable operation, so the name is a tad misleading. Maybe something like RetryableMetaOperation?

TableOperation
 * process() - if you need to tag the end of your loops with comments, you should consider splitting the method into a few different methods. How about everything inside the Retryable becomes processSingleMetaRegion?

Nice to haves:

ProcessRegionServerShutdown:
 * process() is a very long method (100 lines). If it were broken up a little, might make the indentation in the anonymous Retryable classes a little less ugly.

Generally nice patch. Running unit tests now.;;;","05/Apr/08 00:07;bryanduxbury;Tests pass. With some of above suggestions, +1.;;;","05/Apr/08 02:03;jimk;> Bryan Duxbury - 04/Apr/08 04:41 PM
> RetryableOperation
>
>    * Needs a class comment.

Done.

>    * It's not a generically retryable operation, so the name is a tad misleading. Maybe something like 
> RetryableMetaOperation?

Changed.

> TableOperation
>
>    * process() - if you need to tag the end of your loops with comments, you should consider splitting
>  the method into a few different methods. How about everything inside the Retryable becomes
>  processSingleMetaRegion?

Well, those comments were from before when the retry logic was in place. It was not possible to factor
it out into a separate method, but I did create a private internal class that cleaned it up nicely.

> Nice to haves:
>
> ProcessRegionServerShutdown:
>
>    * process() is a very long method (100 lines). If it were broken up a little, might make the indentation
>  in the anonymous Retryable classes a little less ugly.

Again, couldn't do it as a separate method, but did create two inner classes that cleans it up.
;;;","05/Apr/08 02:13;jimk;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Region assignments should never time out so long as the region server reports that it is processing the open request,HBASE-505,12390793,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,jimk,jimk,11/Mar/08 21:52,12/Apr/08 16:12,01/Jul/25 07:49,01/Apr/08 00:07,0.1.0,0.2.0,,,,0.1.1,0.2.0,,,,,,0,"Currently, when the master assigns a region to a region server, it extends the reassignment timeout when the region server reports that it is processing the open. This only happens once, and so if the region takes a long time to come on line due to a large set of transactions in the redo log or because the initial compaction takes a long time, the master will assign the region to another server when the reassignment timeout occurs.

Assigning a region to multiple region servers can easily corrupt the region. For example:

region server 1 is processing the redo log creating a new mapfile. It takes more than one interval to do so so the master assigns the region to region server 2. region server 2 starts processing the redo log creating essentially the same mapFile as region server 1, but with a different name. 

region server 2 can fail to open the region if region server 1 deletes the old log file or if it tries to open the new mapFile that region server 1 is creating.

region server 1 can fail to open the region if it tries to open the mapFile that region server 2 is creating.

Often region server 1 eventually succeeds and reports to the master that it has finished opening the region, but the master tells it to close that region because it has assigned it to another server. Region server 2 often fails to open the region, because the old log file has been deleted, or it fails to process the new map file created by region server 1.

Proposed solution:

During the open process the region server should send a MSG_PROCESS_OPEN with each heartbeat until the region is opened (when it sends MSG_REGION_OPEN). The master will extend the reassignment timeout with each MSG_PROCESS_OPEN it receives and will not assign the region to another server so long as it continues to receive heart beat messages from the region server processing the open.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/08 20:24;stack;505.patch;https://issues.apache.org/jira/secure/attachment/12378740/505.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25236,,,,,Tue Apr 01 00:07:32 UTC 2008,,,,,,,,,,"0|i0h7pj:",98511,,,,,,,,,,,,,,,,,,,,,"11/Mar/08 22:03;stack;Sounds fine as long as the regionserver doesn't just robotically send MSG_PROCESS_OPEN while a region is being opened.  If regionserver gets stuck replaying edits or is hung as we've been seeing from time to time, etc., it shouldn't send MSG_PROCESS_OPEN;;;","11/Mar/08 22:07;bryanduxbury;+1 to Stack's comment. Perhaps if it's making progress in opening the region, it can increment some progress counter that keeps the region server sure the open is still going on and not just screwed up.;;;","11/Mar/08 22:14;jimk;Not a blocker.;;;","26/Mar/08 22:29;stack;Just came across an instance of this.  Here is where we are doing the open:

2008-03-25 19:16:43,720 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482

Here is where we failed the open because region was allocated elsewhere even though this is the server that replayed the edits:

2008-03-25 19:19:56,472 ERROR org.apache.hadoop.hbase.HRegionServer: error opening region enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482

It took 3 minutes to replay 60k edits.

RegionServer should send a ping back to the master every 10K edits applied.;;;","26/Mar/08 22:29;stack;Marking this a blocker for 0.2.;;;","26/Mar/08 22:31;stack;Maybe do this for 0.1.1 too.  Its bad when it happens.;;;","27/Mar/08 20:24;stack;Here's a first cut.

HBASE-505 Region assignments should never time out so long as the region
server reports that it is processing the open request

Have the HRegionServer pass down a Progressable implementation down into
Region and then down int Store where edits are replayed.  Call progress
after every couple of thousand edits.

M  src/java/org/apache/hadoop/hbase/HStore.java
    Take a Progessable in the constructor.  Call it when applying
    edits.
M  src/java/org/apache/hadoop/hbase/HMaster.java
    Update commment around MSG_REPORT_PROCESS_OPEN so its expected
    that we can get more than one of these messages during a region
    open.
M  src/java/org/apache/hadoop/hbase/HRegion.java
    New constructor that takes a Progressable.  Pass it to Stores on
    construction.
M  src/java/org/apache/hadoop/hbase/HRegionServer.java
    On open of a region, pass in a Progressable that adds a
    MSG_REPORT_PROCESS_OPEN every time its called.;;;","27/Mar/08 20:24;stack;Making a blocker against 0.1.1.;;;","30/Mar/08 23:07;jimk;Patch available for 0.1.1;;;","30/Mar/08 23:15;jimk;Reviewed patch +1. I especially like the idea of using a progressable to indicate that something is really happening.;;;","01/Apr/08 00:07;stack;Applied to branch and TRUNK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty region server address in info:server entry and a startcode of -1 in .META.,HBASE-501,12390518,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,stack,stack,08/Mar/08 02:32,22/Aug/08 21:13,01/Jul/25 07:49,13/Mar/08 19:37,0.1.0,0.16.0,0.2.0,,,0.1.0,0.2.0,,,,,,0,"Manufactured a region empty server address and a startcode of -1 when a regionserver was slow to open a region and the alternative regionserver that had been asked open the region fails and reports CLOSE to the master.

Here's long version of story:

Region is enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==.  Was originally on XX.XX.XX.184:60020 but this node ran out of memory (though it had 2G).

{code}
2008-03-08 00:29:39,472 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 60020, call batchUpdate(enwiki_071018,6q_ORe3mPzBTOnenVGS6zk==,1204860472398, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@126d2380) from XX.XX.XX.233:54292: error: java.io.IOException: java.lang.OutOfMemoryError: Java heap space
java.io.IOException: java.lang.OutOfMemoryError: Java heap space
        at java.lang.Object.clone(Native Method)
        at java.lang.reflect.Method.getParameterTypes(Unknown Source)
        at java.lang.Class.searchMethods(Unknown Source)
        at java.lang.Class.getMethod0(Unknown Source)
        at java.lang.Class.getMethod(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:408)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
2008-03-08 00:29:39,472 WARN org.apache.hadoop.ipc.Server: Out of Memory in server select
java.lang.OutOfMemoryError: Java heap space
        at java.util.HashMap.newKeyIterator(Unknown Source)
        at java.util.HashMap$KeySet.iterator(Unknown Source)
        at java.util.HashSet.iterator(Unknown Source)
        at sun.nio.ch.SelectorImpl.processDeregisterQueue(Unknown Source)
        at sun.nio.ch.PollSelectorImpl.doSelect(Unknown Source)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        at sun.nio.ch.SelectorImpl.select(Unknown Source)
        at sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:323)
2008-03-08 00:31:15,300 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 60020, call batchUpdate(enwiki_080103_meta,,1204867086244, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@2d13981b) from XX.XX.XX.233:54810: error: java.io.IOException: java.lang.OutOfMemoryError: Java heap space
java.io.IOException: java.lang.OutOfMemoryError: Java heap space
        at java.lang.String.<init>(Unknown Source)
        at java.lang.StringBuilder.toString(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:415)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
{code}

Was given to XX.XX.XX.227 at 00:36:20 but this server is crazy replaying a bunch of edits (Need to stop emitting edits in HStore -- 496 removed outputting skipped edits).  It can't put the region up immediately.  Takes a long time. 

Then given to XX.XX.XX.183 at 00:37:26. It fails to open with:
{code}
2008-03-08 00:37:29,827 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region enwiki_071018,AYtsfKtThdIJkVLUSKipA-==,1204860383810. Took 5sec
2008-03-08 00:37:29,943 ERROR org.apache.hadoop.hbase.HRegionServer: error opening region enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985
org.apache.hadoop.ipc.RemoteException: java.io.IOException: Could not complete write to file /hbase/aa0-005-2.u.powerset.com/enwiki_080103/1578810967/page/mapfiles/5679937491167886060/data by DFSClient_-540201177
        at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:341)
        at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
...
{code}

Sends a CLOSE to the master.

Then 227 says its successfully opened region.

Master says region server XX.XX.XX.227:60020 should not have opened region enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985

Now the server field in META is empty.

{code}
 59 2008-03-08 00:38:09,167 DEBUG org.apache.hadoop.hbase.HMaster: HMaster.metaScanner regioninfo: {regionname: enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985, startKey: <CzQ7UPCw-AoIn2JzSEN_pV==>, endKey: <DUwzKe-niVjzlXs1SvrvVk==>, encodedName: 1578810967, offline: true, tableDesc: {name: enwiki_080103,         families: {anchor:={name: anchor, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, misc:={name: misc, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, page:={name: page, max versions: 3, compression: NONE, i        n memory: false, max length: 2147483647, bloom filter: none}, redirect:={name: redirect, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}}}, server: , startCode: -1
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/08 20:28;stack;501-v5.patch;https://issues.apache.org/jira/secure/attachment/12377638/501-v5.patch","13/Mar/08 18:06;stack;501-v7.patch;https://issues.apache.org/jira/secure/attachment/12377816/501-v7.patch","08/Mar/08 02:34;stack;master.log;https://issues.apache.org/jira/secure/attachment/12377418/master.log","08/Mar/08 22:28;stack;noise-and-logging-0.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12377459/noise-and-logging-0.1-v2.patch","08/Mar/08 23:31;stack;noise-and-logging-and-coarse-fix-0.1-v3.patch;https://issues.apache.org/jira/secure/attachment/12377463/noise-and-logging-and-coarse-fix-0.1-v3.patch","11/Mar/08 00:01;stack;noise-and-logging-fix-0.1-v4.patch~;https://issues.apache.org/jira/secure/attachment/12377570/noise-and-logging-fix-0.1-v4.patch%7E","08/Mar/08 07:09;stack;noise.patch;https://issues.apache.org/jira/secure/attachment/12377424/noise.patch",,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25233,,,,,Thu Mar 13 19:37:24 UTC 2008,,,,,,,,,,"0|i0h7on:",98507,,,,,,,,,,,,,,,,,,,,,"08/Mar/08 02:34;stack;Edited master log showing sequence of events for pertinent region.;;;","08/Mar/08 06:13;stack;Saw this sequence happen again.  Here is relevant master log:

{code}
2008-03-08 00:37:32,349 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_CLOSE : enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985 from XX.XX.XX.183:60020
2008-03-08 00:37:32,349 INFO org.apache.hadoop.hbase.HMaster: XX.XX.XX.183:60020 no longer serving enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985
2008-03-08 00:37:32,350 DEBUG org.apache.hadoop.hbase.HMaster: Main processing loop: ProcessRegionClose of enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985
2008-03-08 00:37:32,350 INFO org.apache.hadoop.hbase.HMaster: region closed: enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985
2008-03-08 00:37:32,350 DEBUG org.apache.hadoop.hbase.HMaster: numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
2008-03-08 00:37:41,845 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_OPEN : enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985 from XX.XX.XX.227:60020
2008-03-08 00:37:41,845 DEBUG org.apache.hadoop.hbase.HMaster: region server XX.XX.XX.227:60020 should not have opened region enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985
{code}

This comment from HMaster is a little confusing:
{code}
          // NOTE: we cannot put the region into unassignedRegions as that
          //       could create a race with the pending close if it gets 
          //       reassigned before the close is processed.

          unassignedRegions.remove(region);

          try {
            toDoQueue.put(new ProcessRegionClose(region, reassignRegion,
                deleteRegion));
{code}

We add the region to the unassignedRegions in spite of the comment... I notice then that when the MSG_REPORT_OPEN comes in, the first thing we do is remove the region from unassigned regions.  Is this removal what is making it so the region never gets assigned again?  Jim?;;;","08/Mar/08 06:13;stack;Making it a blocker because happened twice in one night and when it happens, table is horked.;;;","08/Mar/08 06:27;stack;Regions when they get into this state are offline and client accesses get an ISA 'offline region' exception.;;;","08/Mar/08 07:09;stack;I understand the above comment now but I do not see this log from shutdown message so the reassignRegion flag is false for some reason and it shouldn't be:

LOG.info(""reassign region: "" + regionInfo.getRegionName());

Attached is a small patch for 0.1 that cleans up some of the noise in logs and that adds better logging on whats happening here around this close.

I seem to have 'fixed' this broken region by running 'enable TABLE_NAME;' even though this table is technically 'online'.  Running this command changed the offline flag on this region.  It then picked up startcode and a server.;;;","08/Mar/08 21:13;jimk;How is this different from HBASE-27?;;;","08/Mar/08 22:28;stack;hbase-27 is just about some of the cells in meta being empty, usually the regioninfo, server, and startcode leaving split cell vestiges.  Nothings 'broken'. 

This is about a scenario that manufactures an offlined region in an online table.   The region has a regioninfo in META but its offlined, there is no startcode or server (shows as -1 and empty in master log respectively) and we never get around to reassigning this region; it just sits there in the middle of the table breaking it.

When we have one of these, we get ISAs in client -- seen by a few folks, Lars for one -- and the offlined regions have also been noticed by others.

Looks like its the CLOSE that offlines the region.  Should it?  I suppose thats fair but we don't put it back on to the unassigned list.  We don't see the 'reassign region' log message?  Its as though the region was on the kill or delete list when CLOSE is running but I can't see how it got there.

Adding new noise patch.  Adds in logging around CLOSE.

This is sample of 'noise' patch cleans up in HLog (Log just keeps outputting this map everytime new element is added):

2008-03-08 00:35:21,886 DEBUG org.apache.hadoop.hbase.HLog: Creating new log file writer for path hdfs://coral-dfs.cluster.powerset.com:10000/hbase/aa0-005-2.u.powerset.com/enwiki_080103/2055991669/oldlogfile.log; map content {enwiki_080103_meta,dy9fcHV_BBQzozASgqoQdk==,1204867224716=org.apache.hadoop.io.SequenceFile$Writer@5e956133, enwiki_071018,65DdQqrq_BtbmFFhMTmyqF==,1199838155829=org.apache.hadoop.io.SequenceFile$Writer@66e9a2c4, enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985=org.apache.hadoop.io.SequenceFile$Writer@1c0fdfec, enwiki_071018,2ZRmAYSs97F__sLy5-ADMV==,1199837444576=org.apache.hadoop.io.SequenceFile$Writer@3456dafb, enwiki_080103,,1204865363666=org.apache.hadoop.io.SequenceFile$Writer@466668f9, enwiki_071018,AYtsfKtThdIJkVLUSKipA-==,1204860383810=org.apache.hadoop.io.SequenceFile$Writer@7bac9f86, enwiki_080103_meta,AyBTC9HXLLw5rxUy675O0-==,1204867122628=org.apache.hadoop.io.SequenceFile$Writer@5fb90875, enwiki_071018,CeAIzFFdqzhyiJQOseL_n-==,1204883192108=org.apache.hadoop.io.SequenceFile$Writer@3eb2492, enwiki_071018,avwxS5T7D4_OCPeI4F17Fk==,1199837870994=org.apache.hadoop.io.SequenceFile$Writer@381578fa, enwiki_080103_meta,4zByyv3_p2JHDADZdbXD4-==,1204867058338=org.apache.hadoop.io.SequenceFile$Writer@7780cea1, enwiki_071018,PperfeKM7-cS9psy-tzmSk==,1199837513465=org.apache.hadoop.io.SequenceFile$Writer@71fc1432, enwiki_071018,nBPrcMh5Sfi2H7KSrsSgkV==,1199838451040=org.apache.hadoop.io.SequenceFile$Writer@5c921914, enwiki_080103_meta,KyceV9ER2CAm3B-qYaoASF==,1204867238538=org.apache.hadoop.io.SequenceFile$Writer@6d75d78a, enwiki_080103,8-CxZA7pnVnBSrjlmICoq-==,1204863457723=org.apache.hadoop.io.SequenceFile$Writer@5399dd2a, enwiki_071018,y0WpQcE85bGuHBk5NbhDtV==,1197675580338=org.apache.hadoop.io.SequenceFile$Writer@48f093f8, enwiki_071018_meta,nxL_BrEZ6q2ooIo06-2g1F==,1199839770536=org.apache.hadoop.io.SequenceFile$Writer@1bd5211, enwiki_071018_meta,jx6p_Uek6ral-F0X3rZnoV==,1199839770535=org.apache.hadoop.io.SequenceFile$Writer@2e3414dc, enwiki_080103_meta,,1204867086244=org.apache.hadoop.io.SequenceFile$Writer@297de952, enwiki_071018,4b-F0-foVXo03XKFzZHeh-==,1204860659625=org.apache.hadoop.io.SequenceFile$Writer@3242af95, enwiki_080103,EWNE3bg-K_93e7xaUTGNmk==,1204863572529=org.apache.hadoop.io.SequenceFile$Writer@1a871b47, enwiki_071018,aR_SC5R5QuEDFSdmlRsk5V==,1199837870994=org.apache.hadoop.io.SequenceFile$Writer@74bd26a4, enwiki_071018,D4cfNPUvLY9KG8OEZOag0F==,1197675115854=org.apache.hadoop.io.SequenceFile$Writer@5d458f36, enwiki_080103_meta,Oz2-mF71uv4gsfgrX4-bZ-==,1204866969564=org.apache.hadoop.io.SequenceFile$Writer@9611bc6, enwiki_071018,6q_ORe3mPzBTOnenVGS6zk==,1204860472398=org.apache.hadoop.io.SequenceFile$Writer@26a3a6f4, enwiki_080103_meta,Ex3rZAu2sKFj2Bij8bVoZF==,1204867006209=org.apache.hadoop.io.SequenceFile$Writer@21208bc8, enwiki_071018_meta,Ey13Xkhj5QxDjHcAzaoEkk==,1197679241539=org.apache.hadoop.io.SequenceFile$Writer@4225f0fd};;;","08/Mar/08 23:28;stack;Ok.  Figured it out.  The exception on the regionserver side of the attached log that provokes the CLOSE is a HDFS error. Here it is:

{code}
2008-03-08 00:37:28,096 DEBUG org.apache.hadoop.hbase.HStore: Applying edit <D496DdeK5YmdoXDJYKeCLF==/page:url/1204889856000=(page:url/1204889856000/http://en.wikipedia.org/wiki/Caesar_Takeshi)>
2008-03-08 00:37:28,096 DEBUG org.apache.hadoop.hbase.HStore: flushing reconstructionCache
2008-03-08 00:37:29,752 DEBUG org.apache.hadoop.hbase.HStore: moving 67943686/page/7182609840669926539 in hdfs://coral-dfs.cluster.powerset.com:10000/hbase/aa0-005-2.u.powerset.com/enwiki_071018/compaction.dir to 67943686/page/4327824140768830867 in hdfs://coral-dfs.cluster.powerset.com:10000/hbase/aa0-005-2.u.powerset.com/enwiki_071018 for 67943686/page
2008-03-08 00:37:29,827 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region enwiki_071018,AYtsfKtThdIJkVLUSKipA-==,1204860383810. Took 5sec
2008-03-08 00:37:29,943 ERROR org.apache.hadoop.hbase.HRegionServer: error opening region enwiki_080103,CzQ7UPCw-AoIn2JzSEN_pV==,1204865434985
org.apache.hadoop.ipc.RemoteException: java.io.IOException: Could not complete write to file /hbase/aa0-005-2.u.powerset.com/enwiki_080103/1578810967/page/mapfiles/5679937491167886060/data by DFSClient_-540201177
        at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:341)
        at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)

        at org.apache.hadoop.ipc.Client.call(Client.java:512)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
        at org.apache.hadoop.dfs.$Proxy1.complete(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy1.complete(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:2292)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:51)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:67)
        at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:932)
        at org.apache.hadoop.io.MapFile$Writer.close(MapFile.java:172)
        at org.apache.hadoop.hbase.HStore.internalFlushCache(HStore.java:1103)
        at org.apache.hadoop.hbase.HStore.doReconstructionLog(HStore.java:826)
        at org.apache.hadoop.hbase.HStore.<init>(HStore.java:720)
        at org.apache.hadoop.hbase.HRegion.<init>(HRegion.java:289)
        at org.apache.hadoop.hbase.HRegionServer.openRegion(HRegionServer.java:1154)
        at org.apache.hadoop.hbase.HRegionServer$Worker.run(HRegionServer.java:1100)
        at java.lang.Thread.run(Unknown Source)
{code}

We're doing the reconstruction log replay.

An exception in the HRegion constructor at this stage falls in here over on the HRS side:

{code}
Index: HRegionServer.java
===================================================================
--- HRegionServer.java  (revision 634789)
+++ HRegionServer.java  (working copy)
@@ -1168,12 +1168,9 @@
       } catch (IOException e) {
         LOG.error(""error opening region "" + regionInfo.getRegionName(), e);
         // Mark the region offline.
         // TODO: add an extra field in HRegionInfo to indicate that there is
         // an error. We can't do that now because that would be an incompatible
         // change that would require a migration        
         regionInfo.setOffline(true);
         reportClose(regionInfo);
         return;
       }
{code}

We're setting the HRegionInfo instance offline flag to true.  We send that over to the HMaster.   It, when processing the MSG_REPORT_CLOSE, if the passed HRegionInfo has an offline flag set, intentionally sets the do-not-reassign flag:

{code}
          boolean reassignRegion = !region.isOffline();
{code}

So, edits or compaction messes up on HRS side, it breaks our table.  Any ongoing uploads break to this region with ISEs.;;;","08/Mar/08 23:31;stack;v3.  Removes marking region offline on HRS side if error opening region.  Means we'll attempt opening region on another server.  It'll probably fail for same reason.  So need to come up with something better.;;;","09/Mar/08 01:02;jimk;The reason for offlining the region when the construction failed was that before, the master would keep trying to assign the region and it would fail again. The assumption is that the region is broken somehow. To do something more elegant would require HBase-fsck which I thought was targeted for 0.2 and not 0.1

Yes it causes ISEs but without knowing more about the failure, it is hard to know where to look to fix the broken region.;;;","09/Mar/08 02:28;stack;I'm thinking that ruling that an IOE in HRegion init fatal may be an overreaction.  In this case at least, a manual onlining of the table brought this region back online  (so it must have deployed successfully elsewhere) and I was able to complete the full upload on restart.  Will look into it more.
;;;","10/Mar/08 21:07;stack;Chatting w/ Jim and looking more in logs, IOE in HRegion is probably an overreaction so should fix but the root cause would look to be two servers concurrently replaying edits and independently cleaning up edit files which makes one of the regionservers get an exception out of HDFS and fail its deploy.

TODO: Confirm this is whats actually happening, then make it so just one regionserver replays edits at a time (Part of the problem is we were logging every edit so their replay were taking a long time).;;;","11/Mar/08 20:28;stack;v4 was incomplete; region was being offlined on close regardless.  v5 adds one line fix (noticed by JimK).;;;","13/Mar/08 18:06;stack;Here is v7.  Cleans up logs and stops our offlining of regions when an IOE on construction of HRegion.  Adds enhanced logging to help us figure some anomalies such as the 'hung regionserver'.

M  conf/hbase-default.xml
    Add hbase.hbasemaster.maxregionopen property.
M  src/java/org/apache/hadoop/hbase/HStore.java
    Change way we log.  Do way less.  Just emit sums of edits applied
    and skipped rather than individual edits.
M  src/java/org/apache/hadoop/hbase/HRegionServer.java
    Make sleeper instance a local rather than data member.
    (reportForDuty): Take a sleeper instance.
    (run): Removed redundant wrap of a 'for' by a 'while'.
    (constructor): If IOE, do not offline the region.  Seen to be
    an overreaction.
M  src/java/org/apache/hadoop/hbase/HLog.java
    Don't output map of all files being cleaned everytime a new
    entry is added; instead just log new entry.  Remove emission
    of every 10k edits.
M src/java/org/apache/hadoop/hbase/HMaster.java
    Up default for maxregionopen.  Was seeing that playing edits
    could take a long time (mostly because we used log every
    edit) but no harm in this being longer.  On REPORT_CLOSE,
    emit region info, not just region so can see the properties
    (W/o, made it hard to figure who was responsible for offlining).
    Add logging of attempt # in shutdown processing.
    Add logging of state flags passed to the close region.  Helps
    debugging.  Also in close offline ONLY if we are NOT reassigning
    the region (jimk find).
M  src/java/org/apache/hadoop/hbase/util/Sleeper.java
    Add logging of extraordinary sleeps or calculated periods 
    (suspicion is that we're sleeping way longer on loaded machies
    and the regionserver appears hung).;;;","13/Mar/08 18:25;jimk;Reviewed changes. +1;;;","13/Mar/08 19:37;stack;Committed to TRUNK and BRANCH;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regionserver stuck on exit,HBASE-500,12390515,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,08/Mar/08 02:17,11/Jun/22 20:13,01/Jul/25 07:49,29/Oct/08 18:19,0.1.0,0.16.0,0.2.0,,,,,,regionserver,,,,0,Found in 0.16.0 cluster.  We're rolling a log and trying to split it too.  Looks like hung up locks.,,,,,,,,,,,,,,,,,,,,,,,HBASE-617,HBASE-46,,,,,"15/Apr/08 22:57;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380224/patch.txt","08/Mar/08 02:17;stack;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12377414/stacktrace.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25232,,,,,Wed Oct 29 18:19:35 UTC 2008,,,,,,,,,,"0|i0h7of:",98506,,,,,,,,,,,,,,,,,,,,,"08/Mar/08 02:18;stack;Attached threaddump;;;","15/Apr/08 22:48;jimk;In the thread dump, the cacheFlusher thread is not present, so
HRegionServer.run has interrupted it.

HRegionServer.run is now waiting to enter the synchronized block
around logRollerLock which is held by the logRoller thread. This means
that HRegionServer.run has interrupted the compactSplit thread (which
does not protect itself using compactSplitLock as it should while it
is working.

The logRoller, HRegionServer$Worker and compactSplit threads are
waiting to acquire HLog.cacheFlushLock It is not clear why one of the
threads cannot proceed as it appears that there is no thread that
could currently be holding that lock.

Certainly, interrupting the compactSplit thread while it is working is
an error. Creating a patch that will fix this error and see if this
lock up occurs after it is committed.;;;","15/Apr/08 22:57;jimk;Addresses problem with interrupting compactSplit thread. But do not know if this is root cause of problem.;;;","15/Apr/08 22:59;jimk;Although this patch may not address the root cause of this issue, it does fix an obvious error. Please review.;;;","16/Apr/08 16:39;stack;+1 on patch.

This issue actually occurs pretty often.;;;","16/Apr/08 17:21;jimk;Committed to 0.1 branch.;;;","16/Apr/08 17:23;jimk;Marking as unassigned since there is no more work to do on this issue unless it re-occurs after the patch.;;;","16/Apr/08 17:43;jimk;It should be noted that trunk does not have the error that was patched for 0.1;;;","29/Oct/08 18:19;stack;We don't see this issue any more.   Resolvling.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegionServer needs to recover if datanode goes down,HBASE-497,12390392,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,bryanduxbury,bien,bien,06/Mar/08 21:45,22/Aug/08 21:17,01/Jul/25 07:49,18/Mar/08 19:48,0.1.0,0.16.0,0.2.0,,,0.1.0,,,,,,,0,"If I take down a datanode, the regionserver will repeatedly return this error:

java.io.IOException: Stream closed.
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.isClosed(DFSClient.java:1875)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:2096)
        at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:141)
        at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:124)
        at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:41)
        at java.io.DataOutputStream.write(Unknown Source)
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)
        at org.apache.hadoop.hbase.HLog.append(HLog.java:377)
        at org.apache.hadoop.hbase.HRegion.update(HRegion.java:1455)
        at org.apache.hadoop.hbase.HRegion.batchUpdate(HRegion.java:1259)
        at org.apache.hadoop.hbase.HRegionServer.batchUpdate(HRegionServer.java:1433)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)

It appears that hbase/dfsclient does not attempt to reopen the stream.",,,,,,,,,,,,,,,HBASE-529,,,,,,,,,,,,,,"18/Mar/08 01:45;bryanduxbury;497-0.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12378095/497-0.1-v2.patch","08/Mar/08 17:36;bryanduxbury;497_0.1.patch;https://issues.apache.org/jira/secure/attachment/12377449/497_0.1.patch",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25230,,,,,Tue Mar 18 19:48:30 UTC 2008,,,,,,,,,,"0|i0h7nr:",98503,,,,,,,,,,,,,,,,,,,,,"06/Mar/08 22:10;bryanduxbury;This is a big deal. We need to be resilient to datanode failures.;;;","08/Mar/08 02:02;bryanduxbury;At least in 0.1, where we don't have appends to logs, when we get an error trying to append to a log, there's no real way to recover the lost log data. This is because HDFS files don't exist until they're closed. (See HADOOP-1700)

Our options are:
 * Bail the regionserver. There's been an exception we shouldn't really ever get, and it's bad. Let it get worked out by restarting.
 * Bail the regionserver, but also try to flush the caches first. This has the advantage of saving the data already written to caches, if possible. Might end up with a convoluted flow to make it happen.
 * Open a new log like nothing ever happened. We'll have lost the updates since the last log roll, but who cares, since there's nothing we can do to recover it, period.
 * Change logging to log to a local file as well as the HDFS file. Then, if there's an exception at any point writing to the HDFS log, we can copy the local version of the log up to HDFS and keep appending. This gives us some resilience to datanode failures, but doesn't really make our logs any more useful in the case of dying machines or network partitions. It's also a lot of new functionality, which doesn't exactly fit with the goals of 0.1 (bugfixes only). 

Of these options, I think the best one is to just open a new log. This will keep our regionserver online and let us carry on with the minimum of difficulty. Does this seem like enough of a fix to satisfy the 0.1 release block?;;;","08/Mar/08 17:36;bryanduxbury;Here's a shot at the ""reopen the log"" approach for 0.1. It passes unit tests, but no test exercises the dying datanode functionality at the moment, so I don't put too much stock in that. Will have to try killing a datanode intentionally. ;;;","17/Mar/08 20:17;stack;Make one emission rather than the two that are in your patch:

{code}
+          LOG.error(""Could not append to log because "" + e.toString());
+          LOG.error(""Opening a new writer."");
{code}

Also change it to LOG.error(""Could not append to log... opening a new writer"", e); i.e. pass the actual exception... 

Otherwise, I'm good with just applying this patch and trying this tactic.  I like the way that the new append is done within the IOE block and it in turn catches an IOE.  But on the second throw, shouldn't we throw something else, something that will break the eternal looping that Michael B reports -- IIUC.  It seems like an IOE won't do?
;;;","18/Mar/08 01:43;bryanduxbury;If an IOException gets thrown up the stack all the way back to HRegionServer#batchUpdate, then the HRS will call checkFileSystem. If there is actually an FS error (which there would have to be in order for two IOExceptions to occur in a row inside rollWriter - I think), then checkFileSystem sets abortRequested and stopRequested, which should kill the main HRS thread.

In this issue, the reason we are not seeing the HRS go down is that there actually isn't an FS problem - it's totally our fault for not trying to reopen the log writer. I suspect that we will be able to recover from this kind of error with the code previously posted.

I will put up a new patch with the logging improvements requested.;;;","18/Mar/08 01:45;bryanduxbury;Passes unit tests locally.;;;","18/Mar/08 01:45;bryanduxbury;Please review patch.;;;","18/Mar/08 17:33;stack;+1 on v2 of patch;;;","18/Mar/08 19:48;bryanduxbury;I just committed this to 0.1 and opened a new issue for porting to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
impossible state for createLease writes 400k lines in about 15seconds,HBASE-496,12390384,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,06/Mar/08 19:25,22/Aug/08 21:13,01/Jul/25 07:49,06/Mar/08 21:54,0.16.0,,,,,0.1.0,0.2.0,,,,,,0,"Saw this in 0.16.0:

{code}
2008-03-06 01:12:03,861 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 60000, call regionServerStartup(address: XX.XX.XX.221:60020, startcode: 1204765922029, load: (requests: 0 regions: 0)) from XX.XX.XX.221:41140: error: java.io.IOException: java.lang.AssertionError: Impossible state for createLease(): Lease 277528057/277528057 is still held.
java.io.IOException: java.lang.AssertionError: Impossible state for createLease(): Lease 277528057/277528057 is still held.
        at org.apache.hadoop.hbase.Leases.createLease(Leases.java:145)
        at org.apache.hadoop.hbase.HMaster.regionServerStartup(HMaster.java:1307)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)                                                                                                                                                                                                                                                              at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
{code}

... over and over for hours.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/08 19:26;stack;12-0.16.0.patch;https://issues.apache.org/jira/secure/attachment/12377276/12-0.16.0.patch","06/Mar/08 19:34;stack;496-0.1.patch;https://issues.apache.org/jira/secure/attachment/12377278/496-0.1.patch","06/Mar/08 19:33;stack;496-TRUNK.patch;https://issues.apache.org/jira/secure/attachment/12377277/496-TRUNK.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25229,,,,,Thu Mar 06 21:54:02 UTC 2008,,,,,,,,,,"0|i0h7nj:",98502,,,,,,,,,,,,,,,,,,,,,"06/Mar/08 19:26;stack;This is like HBASE-12 but let this issue just address the fact that we retry w/o pause.  The retries are putting such a load on the master, its not able to do much else.

This patch should fix the issue.  We were not pausing between retries.;;;","06/Mar/08 19:33;stack;For TRUNK;;;","06/Mar/08 19:34;stack;For branch;;;","06/Mar/08 19:36;stack;Of note, during course of about a minute, we write ~400k lines to the log.;;;","06/Mar/08 19:37;jimk;Reviewed patch. +1;;;","06/Mar/08 21:54;stack;Committed to TRUNK and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No server address listed in .META.,HBASE-495,12390381,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,06/Mar/08 18:46,22/Aug/08 21:13,01/Jul/25 07:49,07/Mar/08 20:00,0.16.0,,,,,0.1.0,0.2.0,,master,,,,0,"Michael Bieniosek manufactured the following in a 0.16.0 install:

{code}
08/03/06 17:52:02 DEBUG hbase.HTable: Advancing internal scanner to startKey g80Fi5WZHlzLqGzErrAd7V==
08/03/06 17:52:02 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: No server address listed in .META. for region enwiki_080103,g80Fi5WZHlzLqGzErrAd7V==,1204768636421
08/03/06 17:52:12 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: No server address listed in .META. for region enwiki_080103,g80Fi5WZHlzLqGzErrAd7V==,1204768636421
08/03/06 17:52:22 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: No server address listed in .META. for region enwiki_080103,g80Fi5WZHlzLqGzErrAd7V==,1204768636421
org.apache.hadoop.hbase.NoServerForRegionException: No server address listed in .META. for region enwiki_080103,g80Fi5WZHlzLqGzErrAd7V==,1204768636421
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:449)
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:346)
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:309)
        at org.apache.hadoop.hbase.HTable.getRegionLocation(HTable.java:103)
        at org.apache.hadoop.hbase.HTable$ClientScanner.nextScanner(HTable.java:854)
        at org.apache.hadoop.hbase.HTable$ClientScanner.next(HTable.java:915)
        at org.apache.hadoop.hbase.hql.SelectCommand.scanPrint(SelectCommand.java:233)
        at org.apache.hadoop.hbase.hql.SelectCommand.execute(SelectCommand.java:100)
        at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:50)
        at org.apache.hadoop.hbase.Shell.main(Shell.java:114)
{code}

When I look in the .META., I see that the above region range has multiple mentions... : one offlined, two that have startcodes and servers associated and about 5 others that are just HRIs.  Table is broke.  At least need the merge of overlapping regions tool to fix.  Digging more....",,,,,,,,,,,,,,,,,,,,,,,,,HBASE-12,,,,"07/Mar/08 06:23;stack;495-0.1.patch;https://issues.apache.org/jira/secure/attachment/12377316/495-0.1.patch","07/Mar/08 19:56;stack;495-0.16.0.patch;https://issues.apache.org/jira/secure/attachment/12377384/495-0.16.0.patch","07/Mar/08 21:29;stack;495-v2-0.16.0.patch;https://issues.apache.org/jira/secure/attachment/12377391/495-v2-0.16.0.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25228,,,,,Fri Mar 07 21:29:27 UTC 2008,,,,,,,,,,"0|i0h7nb:",98501,,,,,,,,,,,,,,,,,,,,,"07/Mar/08 00:36;bryanduxbury;Please try to get the newlines into text you'll put inside of a preserved spacing block. Otherwise, there's not much value in having it in the first place.;;;","07/Mar/08 02:03;stack;Here is story that I have so far.

RegionServer gets hung on DFS ('Call queue overflow discarding oldest call batchUpdate').
Michael B notices it and shutsdown the regionserver (44.221).
The server is restarted.
Tries to check in w/ master but the master says lease still exists
HRS has no pause facility so in a tight loop writes 400k lines in 15seconds about the master's saying the lease exists when it tries to check in w/ master (HBASE-496)
Eventually the old HRS lease expires.
Master gives new HRS a region.
HRS tries to deploy the region.  Skips 2M lines worth of edits (HBASE-472)
Region eventually opens.
Master gives the HRS more regions to open.
Meantime the region w/ all the skipped edits tries to do a compaction and runs into DFS issue: NotReplicatedYetException. Compaction is aborted.
Others of the new regions try to compact.  Fail again in DFS.  Here are what the fails are like:
{code}
2464394 2008-03-06 01:13:59,299 WARN org.apache.hadoop.fs.DFSClient: NotReplicatedYetException sleeping /hbase/aa0-005-2.u.powerset.com/enwiki_080103/compaction.dir/123835725/page/mapfiles/7474986258048984189/data retries left 2
2464395 2008-03-06 01:14:00,902 INFO org.apache.hadoop.fs.DFSClient: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.LeaseExpiredException: No lease on /hbase/aa0-005-2.u.powerset.com/enwiki_080103/compaction.dir/123835725/page/mapfiles/7474986258048984189/data
2464396         at org.apache.hadoop.dfs.FSNamesystem.checkLease(FSNamesystem.java:1157)
2464397         at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1095)
2464398         at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:310)
2464399         at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2464400         at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
2464401         at java.lang.reflect.Method.invoke(Unknown Source)
2464402         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
2464403         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
2464404 
2464405         at org.apache.hadoop.ipc.Client.call(Client.java:512)
2464406         at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
2464407         at org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)
2464408         at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
2464409         at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
2464410         at java.lang.reflect.Method.invoke(Unknown Source)
2464411         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
2464412         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
2464413         at org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)
2464414         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2065)
2464415         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:1958)
2464416         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:1479)
2464417         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1593)
2464418 
2464419 2008-03-06 01:14:01,029 WARN org.apache.hadoop.fs.DFSClient: NotReplicatedYetException sleeping /hbase/aa0-005-2.u.powerset.com/enwiki_080103/compaction.dir/123835725/page/mapfiles/7474986258048984189/data retries left 1
2464420 2008-03-06 01:14:04,231 WARN org.apache.hadoop.fs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.LeaseExpiredException: No lease on /hbase/aa0-005-2.u.powerset.com/enwiki_080103/compaction.dir/123835725/page/mapfiles/7474986258048984189/data
2464421         at org.apache.hadoop.dfs.FSNamesystem.checkLease(FSNamesystem.java:1157)
2464422         at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1095)
2464423         at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:310)
2464424         at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2464425         at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
2464426         at java.lang.reflect.Method.invoke(Unknown Source)
2464427         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
2464428         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
2464429 
2464430 2008-03-06 01:14:04,231 WARN org.apache.hadoop.fs.DFSClient: Error Recovery for block blk_1794752555243844791 bad datanode[0]
2464431 2008-03-06 01:14:04,232 ERROR org.apache.hadoop.hbase.HRegionServer: Compaction failed for region enwiki_080103,g80Fi5WZHlzLqGzErrAd7V==,1204766010394
2464432 java.io.IOException: Could not get block locations. Aborting...
2464433         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:1824)
2464434         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1479)
2464435         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1571)
{code}

More regions to open from master.  Now the open messages are for the same region... here is illustration:

{code}
...
2464453 2008-03-06 01:16:41,090 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_080103,cD-17MphmZfwXnZVdtKy1k==,1199852162634
2464454 2008-03-06 01:28:32,670 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
2464455 2008-03-06 01:29:29,718 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
2464456 2008-03-06 01:29:35,722 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
2464457 2008-03-06 01:29:35,722 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_080103,g80Fi5WZHlzLqGzErrAd7V==,1204766010394
2464458 2008-03-06 01:29:41,728 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
2464459 2008-03-06 01:29:47,734 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
2464460 2008-03-06 01:29:53,740 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
2464461 2008-03-06 01:29:59,746 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
2464462 2008-03-06 01:30:05,752 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : enwiki_071018,75WX3Q0b857NBV8HfO7PC-==,1197675176778
...
{code}

Regionserver should shut itself down if its failing to open a region because of DFS issues -- if it can recognize them as that.

Meantime, on the server, its stuck in the shutdown loop:

{code}
...
4981501 2008-03-06 01:28:00,016 DEBUG org.apache.hadoop.hbase.HMaster: process server shutdown scanning root region on XX.XX.XX.92 finished HMaster
4981502 2008-03-06 01:28:00,016 DEBUG org.apache.hadoop.hbase.HMaster: numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
4981503 2008-03-06 01:28:00,016 DEBUG org.apache.hadoop.hbase.HMaster: process server shutdown scanning .META.,,1 on XX.XX.XX.96:60020 HMaster
4981504 2008-03-06 01:28:00,021 DEBUG org.apache.hadoop.hbase.HMaster: shutdown scanner looking at enwiki_071018,,1199837878882
4981505 2008-03-06 01:28:00,021 DEBUG org.apache.hadoop.hbase.HMaster: Server name XX.XX.XX.226:60020 is not same as XX.XX.XX.221:60020: Passing
...
{code}

Above goes on for 10M lines over about 30 minutes.  Problem is this bit of code in regionServerStartup:

{code}
    HServerInfo storedInfo = serversToServerInfo.remove(s);
    if (storedInfo != null && !closed.get()) {
      // The startup message was from a known server with the same name.
      // Timeout the old one right away.
      HServerAddress root = rootRegionLocation.get();
      if (root != null && root.equals(storedInfo.getServerAddress())) {
        unassignRootRegion();
      } 
      delayedToDoQueue.put(new ProcessServerShutdown(storedInfo));
    } 
{code}

Don't put if server already has a shutdown queued.

OK.  Two fixes needed for this issue (at least): Regionservers should shut down if DFS probs. and don't queue a shutdown if one already queued.;;;","07/Mar/08 06:23;stack;Here is a patch against 0.1.  Will make others if this passes muster.

My thought on this issue is that the cluster is so messy w/ millions of log lines, its hard to debug.  Suggest that we commit this patch against this issue and open another when we see duplicate regions next time.

What seems to be happening is regions are failing to open out on the regionservers because dfs is corrupt.  Was thinking could shutdown if IOE out of HDFS but looking at where the exception is coming up, we actually do do a filesystem check and it must be succeeding.  Also, a failed compaction may not always be worthy of our shutting down regionserver -- in this case on region startup it probably is but later as part of normal operation it probably is not.  DFS health seems to be a tad more involved.

HBASE-495 No server address listed in .META.
M src/java/org/apache/hadoop/hbase/HMaster.java
  (regionServerStartup): Refactor.  Create lease BEFORE scheduling shutdown
  process.  We used do things other way round; meant that we'd shedule a
  shutdown process for every report the regionserver made.  Could be many
  if old lease hanging around.
  (registerRegionServer): Added.  This is body of what used to be in
  regionServerStartup moved here so easy to have a finally in the calling
  method (Should never be an exception out of this method so finally should
  never have to run).

  Removed some useless DEBUG level logs; If thousands of rows in .META.,
  then at least a DEBUG per row multiplied by the shutdown processes
  queued.;;;","07/Mar/08 18:45;jimk;Reviewed patch.

There is a one line change in HRegionServer which only removes a blank line.

+1 on changes to HMaster;;;","07/Mar/08 19:56;stack;Version for 0.16.0.;;;","07/Mar/08 20:00;stack;Committed to branch and TRUNK.

Closing this issue though I don't think we've seen the last of overlapping regions. I think we'll see overlapping instances again sometime down the road but will be easier to figure out whats going on now this and other recent commits reduce the noise significantly around regionserver checking in w/ server.;;;","07/Mar/08 21:29;stack;Version 2 of 495 patch for 0.16.0; this one actually passes tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Doubly-assigned .META.; master uses one and clients another",HBASE-490,12390106,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,stack,stack,04/Mar/08 01:25,06/Mar/08 21:52,01/Jul/25 07:49,04/Mar/08 19:35,0.1.0,0.2.0,,,,0.1.0,0.2.0,,master,,,,0,"Internal cluster has two .META.,,1 regions up (Its possible for a region to be added twice to the unassigned map if meta scans run close together).  Worse is that the master is working with one .META. but when clients come in, they're being give the other.  Makes for odd results.

Made it a blocker.  Still trying to track down how master doesn't see subsequent update of .META. info in -ROOT-.....",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/08 06:35;stack;490-0.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12377036/490-0.1-v2.patch","04/Mar/08 19:34;stack;490-0.16.0.patch;https://issues.apache.org/jira/secure/attachment/12377110/490-0.16.0.patch","04/Mar/08 06:35;stack;490-TRUNK-v2.patch;https://issues.apache.org/jira/secure/attachment/12377037/490-TRUNK-v2.patch","04/Mar/08 06:00;stack;490-trunk.patch;https://issues.apache.org/jira/secure/attachment/12377034/490-trunk.patch","04/Mar/08 05:37;stack;490.patch;https://issues.apache.org/jira/secure/attachment/12377032/490.patch",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25226,,,,,Thu Mar 06 21:52:42 UTC 2008,,,,,,,,,,"0|i0h7m7:",98496,,,,,,,,,,,,,,,,,,,,,"04/Mar/08 05:37;stack;Patch for 0.1.  Here is commit message:

HBASE-490  Doubly-assigned .META.; master uses one and clients another

On start, the first two scans run back-to-back.  The first scan of -ROOT-
on restart of a decent size hbase install of more than a single server
will mark the .META. assignment as bad and assign the .META. to a
regionserver to deploy.  The logic in checkingAssignment fixed in the below
was allowing that the second scan would also assign the .META.,
usually to adifferent region server.  Both assignments would report back
in as successful opens.  Both would be added to metaRegionsToScan but only
the first addition would be acted on (I changed code here so we exhaust
metaRegionsToScan before we move out of initialScan).  The second would
never addition would never get promoted to being an online region even
though it being the second open, it would be the .META. that was written
into -ROOT-; it would be what clients would get while the onlineRegions
would have the first .META. in it (Master was getting its .META. info
from this deploy)..

M  src/java/org/apache/hadoop/hbase/HMaster.java
  Mild formatting. Javadoc.  Above described bug fixes.;;;","04/Mar/08 05:46;stack;Passes tests locally on 0.1.;;;","04/Mar/08 06:00;stack;For TRUNK;;;","04/Mar/08 06:35;stack;Add fix for fact that scans of meta regions run back to back on start up (Fix in Sleeper class).;;;","04/Mar/08 06:35;stack;Fix for TRUNK;;;","04/Mar/08 18:29;jimk;Patches apply: +1
Patches reviewed: +1
;;;","04/Mar/08 19:34;stack;Patch for 0.16.0;;;","04/Mar/08 19:35;stack;Committed to branch and TRUNK;;;","06/Mar/08 21:52;stack;Applied to TRUNK and branch.  Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tool to manually merge two regions,HBASE-480,12389931,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,stack,stack,29/Feb/08 22:38,01/Feb/11 10:49,01/Jul/25 07:49,01/Mar/08 22:06,0.1.0,0.16.0,0.2.0,,,0.1.0,0.2.0,,,,,,0,hbase-471 needs a tool to merge two regions that have same start key.  This tool may be of use elsewhere making repairs.,,,,,,,,,,,,,,,,,,,,,,,HBASE-285,,,,,,"01/Mar/08 16:30;stack;merge-0.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12376905/merge-0.1-v2.patch","01/Mar/08 20:06;stack;merge-0.1-v3.patch;https://issues.apache.org/jira/secure/attachment/12376910/merge-0.1-v3.patch","01/Mar/08 16:28;stack;merge-v2.patch;https://issues.apache.org/jira/secure/attachment/12376904/merge-v2.patch","01/Mar/08 19:56;stack;merge-v3.patch;https://issues.apache.org/jira/secure/attachment/12376909/merge-v3.patch","29/Feb/08 22:44;stack;merge.patch;https://issues.apache.org/jira/secure/attachment/12376876/merge.patch",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25223,,,,,Thu Mar 06 21:50:41 UTC 2008,,,,,,,,,,"0|i0h7k7:",98487,,,,,,,,,,,,,,,,,,,,,"29/Feb/08 22:44;stack;HBASE-480 Tool to manually merge two regions
M  src/java/org/apache/hadoop/hbase/HMerge.java
   (main) Added.  Made this class implement Tool interface
   so can source alternate configuration.
   (merge): Made it protected instead of private so can
   call it explicitly from subclass that does a different
   'process' implementation.  Added a 'force' flag that
   overrides the ""won't merge if makes a too-big region""
   check.
   (getMetaRow): Added.  Add protection against null
   HRegionInfo seen commonly in the past.
   (checkOffLine): Added.  Usually merge will not work
   if table is online but offlining is not working
   reliably and shouldn't get in the way of this tool
   working anyways.  Let subclasses choose whether this
   is necessary or not.
M  src/java/org/apache/hadoop/hbase/hql/DisableCommand.java
    Just some clean up and added a main so can invoke this
    class w/o having to go via hql;;;","01/Mar/08 16:28;stack;Removed DisableCommand changes from the patch.  Any chance of a review?;;;","01/Mar/08 16:30;stack;Patch for 0.1 branch.;;;","01/Mar/08 18:16;bryanduxbury;I applied and ran tests for v2, got:

{code}
Testcase: testMergeTable took 33.626 sec
	Caused an ERROR
null
java.lang.NullPointerException
	at org.apache.hadoop.hbase.HMerge$OnlineMerger.nextRegion(HMerge.java:218)
	at org.apache.hadoop.hbase.HMerge$OnlineMerger.next(HMerge.java:275)
	at org.apache.hadoop.hbase.HMerge$Merger.process(HMerge.java:127)
	at org.apache.hadoop.hbase.HMerge.merge(HMerge.java:93)
	at org.apache.hadoop.hbase.TestMergeTable.testMergeTable(TestMergeTable.java:37)
{code};;;","01/Mar/08 19:56;stack;My fault.  Had fixed that in code but hadn't updated the patch.  Here's v3.  Thanks for reviewing.
{code}
durruti:~/Documents/checkouts/hbase/trunk stack$ svn diff src/java/org/apache/hadoop/hbase/HMerge.java > merge-v3.patch
durruti:~/Documents/checkouts/hbase/trunk stack$ ~/bin/ant/bin/ant clean compile jar test -Dtestcase=TestMergeTable
Buildfile: build.xml

clean:
   [delete] Deleting directory /Users/stack/Documents/checkouts/hbase/trunk/build
...
test:
    [mkdir] Created dir: /Users/stack/Documents/checkouts/hbase/trunk/build/test/logs
    [junit] Running org.apache.hadoop.hbase.TestMergeTable
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 34.91 sec

BUILD SUCCESSFUL
Total time: 42 seconds
{code};;;","01/Mar/08 20:06;stack;Patch for 0.1;;;","01/Mar/08 21:55;bryanduxbury;Latest 0.2 patch works now, tests pass. ;;;","01/Mar/08 22:06;stack;Thanks for review Bryan.  Committed to TRUNK and branch.;;;","06/Mar/08 21:50;stack;Already backported. Marking as fixed in 0.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
offlining of table does not run reliably,HBASE-478,12389927,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jimk,stack,stack,29/Feb/08 22:10,22/Aug/08 21:13,01/Jul/25 07:49,07/May/08 17:57,0.1.1,0.1.2,0.2.0,,,0.1.2,0.2.0,,,,,,0,"I have a table of 4 regions made w/ PE.  I cannot reliably offline it.  I'm using 'disable TestTable' and have traced it to ensure its not a problem in hql.    What I see is that one region will get the offlined mark or maybe two.. but never all.

Jim in IRC suggested that if we did the .TABLE. catalog table, offlining the entry there might be more reliable than trying to offline all regions in a table.",,,,,,,,,,,,,,,,,,,,HBASE-599,,,HBASE-627,,,,,,"04/May/08 21:31;jimk;478-0.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12381396/478-0.1-v2.patch","02/May/08 23:47;jimk;478-0.1.patch;https://issues.apache.org/jira/secure/attachment/12381355/478-0.1.patch","06/May/08 03:27;jimk;478-trunk.patch;https://issues.apache.org/jira/secure/attachment/12381476/478-trunk.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25222,,,,,Wed May 07 17:57:05 UTC 2008,,,,,,,,,,"0|i0h7jr:",98485,,,,,,,,,,,,,,,,,,,,,"11/Mar/08 20:43;stack;Made critical issue.  Can't admin hbase -- add columns after creation, etc. -- without being able to offline reliably.;;;","23/Apr/08 22:32;stack;Needs fixing for 0.2.0; offlining/onlining needs to be reliable.;;;","30/Apr/08 02:59;bryanduxbury;It seems whatever strategy we take for this issue it'll be easier if we do HBASE-451. Maybe we should do that first?

In general, the problem is that its impossible (or at least impractical) for the client to block while a table is being offlined. So, instead, I think HTableDescriptor should get a new flag, something like isGoingDown that indicates it's not offline yet but it's supposed to be coming down. Then, whenever the client tries to do a schema operation, if the table is going down but isn't offline yet, we know to reject attempts. Regions for tables that are offline or going down won't get assigned. 

I'd go one step further and say that we should require explicit disable of table before allowing schema operations, rather than doing it automatically. It'd be an extra step but I think it would set expectations a little better about what should happen during schema operations.;;;","01/May/08 05:32;stack;I thought HBASE-609 also fixed this issue but I just tested with a 40 region table and there are laggards... 3 regions that wouldn't offline.  Then enabling, only about 10 or 12 came back online.  Deleting, all but the 3 that wouldn't offline went away.  Once it gets into the half-online/half-offline state, its hosed (which is odd -- a second run of disable should just offline whatever is still online).;;;","02/May/08 01:30;jimk;With a couple of changes, I disable table now appears to work, even if the table is only partially enabled.

(49 table regions)

enable is still problematic. It still only on-lines part of the table, and repeated attempts to online the remainder has only worked in one of 5 disable/enable cycles.

I'll pursue this further tomorrow.
;;;","02/May/08 23:47;jimk;HBASE-478 offlining of table does not run reliably

M HMaster

  Require tables to be offline before they can be deleted. This
  enables the Set regionsToBeDeleted to be removed. Also we need no
  longer worry about regionsToBe deleted in processMsgs,
  ProcessServerShutdown, ProcessServerShutdown.scanMetaRegion,
  ProcessRegionClose, ProcessRegionClose.process

  If a region server exits cleanly, do not put any region up for
  reassignment if it was in the killedRegions Set.

  Fix potential synchronization problem in
  ProcessServerShutdown.scanMetaRegion 

  Remove regions from killList when ProcessServerShutdown finishes.

  Don't throw an error from TableOperation.process if info:regioninfo
  is empty. Log an error and continue.

  Change IOException to TableNotFoundException in TableOperation.process

  Don't retry in TableOperation.process if exception caught is
  TableNotFoundException, TableNotDisabledException, or
  InvalidColumnNameException. Just rethrow the exception.

  In ChangeTableState.postProcessMeta, if onlining a region, remove
  the region from killedRegions. Fix a couple of potential
  synchronization problems. Put regions being offlined into
  localKillList before adding those already queued for that server.

  In TableDelete, add method processScanItem, which throws
  TableNotDisabledException if the table is still online.

  TableDelete.postProcessMeta need no longer worry about regions that
  are online. If it is called, it can assert that every region in the
  table is offline, remove them from the meta and delete the region
  directories. 

  ModifyColumn.postProcessMeta now throws InvalidColumnNameException
  if the column does not exist instead of generic IOException

M HStore

  Add deleted columns to results of MemcacheScanner

  Added @SuppressWarnings(""hiding"") annotations to StoreFileScanner as
  needed.

  Add javadoc for public methods and @Override annotations where needed. 

  Fix some indentation and line too long problems.

M TestHBaseCluster

  Remove private method cleanup which deleted table.
;;;","02/May/08 23:49;jimk;Patch for 0.1.2. Passes local tests and enable/disable now work reliably.

Please review.;;;","02/May/08 23:53;jimk;Once patch for 0.1 branch is approved, I will submit patch for 0.2.0;;;","04/May/08 06:22;stack;Jim: Can you fix the patch.  Seems like most of it is you adding ^M.  For example:

{code}
Index: src/java/org/apache/hadoop/hbase/HStore.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HStore.java    (revision 652949)
+++ src/java/org/apache/hadoop/hbase/HStore.java    (working copy)
@@ -85,7 +85,7 @@
     // The currently active sorted map of edits.
     private volatile SortedMap<HStoreKey, byte[]> mc =
       createSynchronizedSortedMap();
- 
+
     // Snapshot of memcache.  Made for flusher.
     private volatile SortedMap<HStoreKey, byte[]> snapshot =
       createSynchronizedSortedMap();
@@ -126,43 +126,43 @@
       }
     }

-   /**
-    * Return the current snapshot.
-    * Called by flusher when it wants to clean up snapshot made by a previous
+    /**
+     * Return the current snapshot.
+     * Called by flusher when it wants to clean up snapshot made by a previous
      * call to {@link snapshot}
-    * @return Return snapshot.
-    * @see {@link #snapshot()}
-    * @see {@link #clearSnapshot(SortedMap)}
-    */
-   SortedMap<HStoreKey, byte[]> getSnapshot() {
-     return this.snapshot;
-   }
+     * @return Return snapshot.
+     * @see {@link #snapshot()}
+     * @see {@link #clearSnapshot(SortedMap)}
+     */
+    SortedMap<HStoreKey, byte[]> getSnapshot() {
+      return this.snapshot;
+    }
{code}

Above is from the head of the patch and nothing of substance has been changed.  Makes it hard to review.  Thanks.;;;","04/May/08 21:31;jimk;This patch removes most of the whitespace changes, so it should be easier to read.

The whitespace changes were made originally because several sections of code are indented 1 space instead of 2.;;;","04/May/08 21:32;jimk;new patch that removes whitespace changes. please review.;;;","05/May/08 17:00;stack;Thanks for removing white-space differences.

Following is not important.

To avoid confusion, I'd suggest in future that we act on the IDE's suggestion and rename these fields in the subclass.  My sense is that subclasses with data members of the same name as members of their parent class will for sure burn someone refactoring or debugging.

{code}
+    @SuppressWarnings(""hiding"")
     private MapFile.Reader[] readers;

     // Used around replacement of Readers if they change while we're scanning.
+    @SuppressWarnings(""hiding"")
     private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
{code}

You removed table delete from TestHBaseCluster.  It'd be too hard to add enable/delete?  It'd be hard asserting enable state?   Disabled state?   If you could assert disabled state, maybe add back delete?  I just suggest this because this facility -- enabling/disabling table is critical to hbase admin and can't be broken going forward (I can't believe we got this far w/o clammer for this fix -- smile).

Will be back after test has finished.;;;","05/May/08 21:52;jimk;I removed the table delete because enable/disable are asychronous and might not be deterministic for a test case. Given that this table is so small, that may not be an issue, on the other hand, it doesn't test much either since we generally needed a table of 30-40 regions to get enable/disable to fail.

I can put back the table delete if you want, preceded by a disable. I don't have a strong opinion either way. Your call.;;;","05/May/08 23:11;stack;+1 on this patch.

I tested with a big table.  I was able to disable, then enable, and then scan the whole thing.  I then did a remove and all regions were cleared from the .META. (dropping a table that has not been first disabled comes back with success though nothing happened but this is a HQL bug and HQL is going bye-byes so not a worry).  Its not removing the table directory but thats another issue.

Regards tests for enable/disable, maybe add them to TRUNK if can figure an easy means of making assertions (Scan of .META. and all regions are on or offline?) otherwise, lets worry about it next time we break this facility (smile).  Lets not tests get in way of adding this patch to branch.;;;","05/May/08 23:12;jimk;The patch to fix this issue for trunk, demonstrates the CME exception described in HBASE-599;;;","05/May/08 23:57;jimk;Committed patch for 0.1 branch. Working on patch for trunk;;;","06/May/08 00:00;jimk;This patch is so intertwined with HBASE-599, incorporating fix for HBASE-599 in this issue for 0.2.0;;;","06/May/08 03:27;jimk;HBASE-478 offlining of table does not run reliably, HBASE-599 CME iterating return from ServerManager.getMarkedToClose

M BaseScanner, ProcessRegionClose, ProcessServerShutdown, RegionManager, ServerManager, TableDelete

Require tables to be offline before they can be deleted. This enables the Set regionsToBeDeleted to be removed.

M In ChangeTableState.postProcessMeta

If onlining a region, mark it as noLongerClosing.  Fix a couple of potential synchronization problems. Put regions being offlined into localKillList before adding those already queued for that server.

M ModifyColumn

In postProcessMeta, throw InvalidColumnNameException if the column does not exist instead of generic IOException

M ProcessServerShutdown

Remove regions from closing regions when ProcessServerShutdown finishes.

M RegionManager

Fix potential synchronization problem in regionsAwaitingAssignment, markToCloseBulk

Eliminate potential CME's by returning a copy of map in onlineMetaRegions

Eliminate CME by changing getMarkedToClose to removeMarkedToClose

Add method allRegionsClosed.

M RetryableMetaOperation

Don't retry in doWithRetries if exception caught is TableNotFoundException, TableNotDisabledException, or InvalidColumnNameException. Just rethrow the exception.

M ServerManager

Fix potential synchronization problems in regionServerStartup, processRegionServerAllsWell, cancelLease, leaseExpired

If a region server exits cleanly, do not put any region up for reassignment if it was marked to close.

In processMsgs, remove regions to close, rather than getting, iterating and removing. Caused a CME.

Don't need to iterate over entrySet in getAverageLoad. Iterating over values is sufficient.

Eliminate potential CME's by returning a copy of map in getServersToServerInfo, getServersToLoad, getLoadToServers

M TableDelete

Add method processScanItem, which throws TableNotDisabledException if the table is still online.

M TableOperation

Don't throw an error from TableOperation.process if info:regioninfo is empty. Log an error and continue.

Change IOException to TableNotFoundException in TableOperation.process

M Memcache

Add deleted columns to results of MemcacheScanner

M TestHBaseCluster

Remove private method cleanup which deleted table.
;;;","06/May/08 03:29;jimk;Patch for trunk available. Includes fix for HBASE-599. Please review.;;;","07/May/08 17:11;stack;+1 on patch.

I did not test (Patch is stale for TRUNK I believe).

The 599 fixes all look reasonable to me.

I like changes where IOEs are converted to TableNotFoundExceptions where it makes sense.

Going forward, doesn't have to be in this patch, we should consider new exception superclass, one we don't retry so we don't have to do this kinda thing in multiple places:

{code}
+        if (e instanceof TableNotFoundException ||
+            e instanceof TableNotDisabledException ||
+            e instanceof InvalidColumnNameException) {
+          throw e;
{code}

;;;","07/May/08 17:57;jimk;Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for an HBASE_CLASSPATH,HBASE-477,12389922,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,stack,stack,29/Feb/08 21:23,22/Aug/08 21:13,01/Jul/25 07:49,03/Mar/08 19:18,0.1.0,0.16.0,0.2.0,,,0.1.0,0.2.0,,,,,,0,"We have mention of HBASE_CLASSPATH in hbase-env.sh but its not actually read anywhere.  Make it work like HADOOP_CLASSPATH.  See classpath discussion on this page, http://wiki.apache.org/hadoop/Hbase/Jython, for a use case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/08 04:58;stack;cp.patch;https://issues.apache.org/jira/secure/attachment/12376920/cp.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25221,,,,,Thu Mar 06 21:51:08 UTC 2008,,,,,,,,,,"0|i0h7jj:",98484,,,,,,,,,,,,,,,,,,,,,"02/Mar/08 04:58;stack;Add to bin/hbase HBASE_CLASSPATH.  Review please lads.;;;","02/Mar/08 05:02;stack;With this patch, I can do following:

{code}
durruti:~/Documents/checkouts/hbase/trunk stack$ HBASE_OPTIONS=""-Dpython.home='/Users/stack/bin/jython2.2.1'"" HBASE_CLASSPATH=/Users/stack/bin/jython2.2.1/jython.jar ./bin/hbase org.python.util.jython
Jython 2.2.1 on java1.5.0_13
Type ""copyright"", ""credits"" or ""license"" for more information.
>>> from org.apache.hadoop.hbase import HBaseConfiguration 
>>> h = HBaseConfiguration()
>>> from org.apache.hadoop.hbase.client import HTable
>>> from org.apache.hadoop.io import Text
>>> n = Text('some_table')
>>> t = Table(h, n)
Traceback (innermost last):
  File ""<console>"", line 1, in ?
NameError: Table
>>> t = HTable(h, n)
08/03/01 20:59:53 INFO [main] ipc.Client: Retrying connect to server: localhost/127.0.0.1:60000. Already tried 1 time(s).
08/03/01 20:59:54 INFO [main] ipc.Client: Retrying connect to server: localhost/127.0.0.1:60000. Already tried 2 time(s).
{code}

i.e. jython context with all of hbase dependencies on the CLASSPATH;;;","02/Mar/08 05:06;stack;Or this:

{code}
durruti:~/Documents/checkouts/hbase/trunk stack$ HBASE_OPTS=""-Djruby.base=/Users/stack/bin/jruby -Djruby.home=/Users/stack/bin/jruby -Djruby.lib=/Users/stack/bin/jruby/lib -Djruby.script=jruby -Djruby.shell=/usr/sh"" HBASE_CLASSPTH=""/Users/stack/bin/jruby/lib/jruby.jar"" ./bin/hbase org.jruby.Main /Users/stack/bin/jruby/bin/jirb
irb(main):001:0
{code};;;","02/Mar/08 05:06;stack;Backport and update the jython wiki page after commit;;;","03/Mar/08 18:50;bryanduxbury;The patch is very simple. +1.

However, I wonder if you even need this much to get jirb up and running with HBase stuff. Couldn't you just do:

{code}
$ jruby -J-cp `get hbase classpath` bin/shell.rb 
irb(main): ...
{code}

Anything after a -J switch goes straight to the JVM. ;;;","03/Mar/08 18:59;jimk;looks fine. +1.;;;","03/Mar/08 19:15;stack;Thanks lads (Bryan, adding HBASE_CLASSPATH is generalized soln... your snazzy classpath-for-jruby works for jruby only).;;;","03/Mar/08 19:18;stack;Committed to branch and trunk.;;;","06/Mar/08 21:51;stack;Already backported.  Marking as fixed in 0.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegexpRowFilter behaves incorectly when there are multiple store files,HBASE-476,12389905,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,clint.morgan,clint.morgan,29/Feb/08 18:26,22/Aug/08 21:13,01/Jul/25 07:49,22/Mar/08 22:37,0.1.0,0.2.0,,,,0.1.0,0.2.0,,Filters,,,,0,"I noticed that after running some table Map/Reduces, then using a
RegExpRowFilter to scan through the table,  the scanner misses
rows when its columns are in different stores.

This (rather convoluted) unit test provokes the behavior.

- Set memcache flush size small to trigger multiple stores
- put in 10 row with 2 columns. Each row has the same value for col1 (which the RowFilter wants to match)
- Scan with and without the filter to be sure that we get all the rows with each
- Run an identity table M/R 10 times to fill up the memcache and trigger flush.
- Scan again. This time the filter does not pickup anything.

Attaching the log from this run as well.
",,,,,,,,,,,,,,,,,,HBASE-527,,,,,HBASE-527,,,,,,"29/Feb/08 18:29;clint.morgan;hbase-476-test.log;https://issues.apache.org/jira/secure/attachment/12376850/hbase-476-test.log","29/Feb/08 18:29;clint.morgan;hbase-476-test.patch;https://issues.apache.org/jira/secure/attachment/12376851/hbase-476-test.patch","05/Mar/08 00:49;clint.morgan;hbase-476-test2.patch;https://issues.apache.org/jira/secure/attachment/12377132/hbase-476-test2.patch","21/Mar/08 23:37;clint.morgan;hbase-476-test3.patch;https://issues.apache.org/jira/secure/attachment/12378422/hbase-476-test3.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25220,,,,,Sat Mar 22 22:37:37 UTC 2008,,,,,,,,,,"0|i0h7jb:",98483,,,,,,,,,,,,,,,,,,,,,"05/Mar/08 00:49;clint.morgan;This is a cleaner way to provoke the issue.

- Write enough to trigger a flush
- Let all flushes complete
- Write a bit more to the memcache
- Open up a scanner with a RegExpRowFilter
- Scanner fails to provide any results if some columns are in memcache, and some are in hstores.

Hopefully someone familiar with the Scanner implementation can take a look at it.;;;","21/Mar/08 17:40;jimk;Is this still an issue in the HBase release candidate?;;;","21/Mar/08 22:58;jimk;Clint,

Would you be willing to upload a version of hbase-476-test2.patch and grant an ASF license for it?

Without an ASF license, we cannot use it as the basis for a test case.

Thanks.;;;","21/Mar/08 23:37;clint.morgan;Sure, here it is.

However, I think the hbase-527's test is a better way at provoking the issue.;;;","22/Mar/08 22:37;jimk;Committed to 0.1.0 and trunk. Thanks Clint!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When a table is deleted, master sends multiple close messages to the region server",HBASE-473,12389637,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,bryanduxbury,jimk,jimk,27/Feb/08 00:52,22/Aug/08 21:13,01/Jul/25 07:49,28/Feb/08 22:08,0.2.0,,,,,0.2.0,,,master,,,,0,"While TestHBaseCluster succeeds, it demonstrates that the master tells the region server to close the region multiple times.

{code}
    [junit] 2008-02-26 15:42:26,718 DEBUG [IPC Server handler 1 on 60000] master.ChangeTableState(131): adding region test,,1204069326375 to kill list
    [junit] 2008-02-26 15:42:26,718 DEBUG [IPC Server handler 1 on 60000] master.ChangeTableState(138): inserted local kill list into kill list for server 10.69.80.2:2154
    [junit] 2008-02-26 15:42:26,796 INFO  [IPC Server handler 1 on 60000] master.HMaster(644): deleted table: test
    [junit] 2008-02-26 15:42:27,515 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:27,515 DEBUG [RegionServer:0.worker] regionserver.HRegion(410): compactions and cache flushes disabled for region test,,1204069326375
    [junit] 2008-02-26 15:42:27,515 DEBUG [RegionServer:0.worker] regionserver.HRegion(428): new updates and scanners for region test,,1204069326375 disabled
    [junit] 2008-02-26 15:42:27,515 DEBUG [RegionServer:0.worker] regionserver.HRegion(446): no more active scanners for region test,,1204069326375
    [junit] 2008-02-26 15:42:27,515 DEBUG [RegionServer:0.worker] regionserver.HRegion(452): no more row locks outstanding on region test,,1204069326375
    [junit] 2008-02-26 15:42:27,515 DEBUG [RegionServer:0.worker] regionserver.HRegion(889): Started memcache flush for region test,,1204069326375. Size 86.4k
    [junit] 2008-02-26 15:42:27,546 INFO  [RegionManager.rootScanner] master.BaseScanner(147): RegionManager.rootScanner scanning meta region {regionname: -ROOT-,,0, startKey: <>, server: 10.69.80.2:2154}
    [junit] 2008-02-26 15:42:27,562 DEBUG [RegionManager.rootScanner] master.BaseScanner(179): RegionManager.rootScanner regioninfo: {regionname: .META.,,1, startKey: <>, endKey: <>, encodedName: 1028785192, tableDesc: {name: .META., families: {info:={name: info, max versions: 1, compression: NONE, in memory: false, block cache enabled: false, max length: 2147483647, bloom filter: none}}}}, server: 10.69.80.2:2154, startCode: 1204069326359
    [junit] 2008-02-26 15:42:27,562 INFO  [RegionManager.rootScanner] master.BaseScanner(225): RegionManager.rootScanner scan of meta region {regionname: -ROOT-,,0, startKey: <>, server: 10.69.80.2:2154} complete
    [junit] 2008-02-26 15:42:27,812 DEBUG [RegionServer:0.worker] regionserver.HStore(1154): Added 1417693581/anchor/2354913287379000616 with 1000 entries, sequence id 2007, and size 60.0k for 1417693581/anchor
    [junit] 2008-02-26 15:42:28,125 DEBUG [RegionServer:0.worker] regionserver.HStore(1154): Added 1417693581/contents/295490293048850969 with 1000 entries, sequence id 2007, and size 55.1k for 1417693581/contents
    [junit] 2008-02-26 15:42:28,125 DEBUG [RegionServer:0.worker] regionserver.HRegion(995): Finished memcache flush for region test,,1204069326375 in 610ms, sequenceid=2007
    [junit] 2008-02-26 15:42:28,125 DEBUG [RegionServer:0.worker] regionserver.HStore(1063): closed 1417693581/anchor
    [junit] 2008-02-26 15:42:28,125 DEBUG [RegionServer:0.worker] regionserver.HStore(1063): closed 1417693581/contents
    [junit] 2008-02-26 15:42:28,125 INFO  [RegionServer:0.worker] regionserver.HRegion(478): closed test,,1204069326375
    [junit] 2008-02-26 15:42:28,515 DEBUG [IPC Server handler 0 on 60000] master.ServerManager(287): Received MSG_REPORT_CLOSE : test,,1204069326375 from 10.69.80.2:2154
    [junit] 2008-02-26 15:42:28,515 INFO  [IPC Server handler 0 on 60000] master.ServerManager(303): 10.69.80.2:2154 no longer serving test,,1204069326375
    [junit] 2008-02-26 15:42:28,515 DEBUG [HMaster] master.HMaster(410): Main processing loop: ProcessRegionClose of test,,1204069326375
    [junit] 2008-02-26 15:42:28,515 INFO  [HMaster] master.ProcessRegionClose(61): region closed: test,,1204069326375
    [junit] 2008-02-26 15:42:28,515 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:28,515 DEBUG [HMaster] master.RegionServerOperation(75): numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
    [junit] 2008-02-26 15:42:28,515 DEBUG [HMaster] regionserver.HRegion(1913): DELETING region hdfs://localhost:2123/user/jim/test/1417693581
    [junit] 2008-02-26 15:42:29,500 INFO  [RegionManager.metaScanner] master.BaseScanner(147): RegionManager.metaScanner scanning meta region {regionname: .META.,,1, startKey: <>, server: 10.69.80.2:2154}
    [junit] 2008-02-26 15:42:29,500 INFO  [RegionManager.metaScanner] master.BaseScanner(225): RegionManager.metaScanner scan of meta region {regionname: .META.,,1, startKey: <>, server: 10.69.80.2:2154} complete
    [junit] 2008-02-26 15:42:29,500 INFO  [RegionManager.metaScanner] master.MetaScanner(136): all meta regions scanned
    [junit] 2008-02-26 15:42:29,515 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:30,515 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:31,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:32,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:33,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:34,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:35,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:36,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:37,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:37,546 INFO  [RegionManager.rootScanner] master.BaseScanner(147): RegionManager.rootScanner scanning meta region {regionname: -ROOT-,,0, startKey: <>, server: 10.69.80.2:2154}
    [junit] 2008-02-26 15:42:37,562 DEBUG [RegionManager.rootScanner] master.BaseScanner(179): RegionManager.rootScanner regioninfo: {regionname: .META.,,1, startKey: <>, endKey: <>, encodedName: 1028785192, tableDesc: {name: .META., families: {info:={name: info, max versions: 1, compression: NONE, in memory: false, block cache enabled: false, max length: 2147483647, bloom filter: none}}}}, server: 10.69.80.2:2154, startCode: 1204069326359
    [junit] 2008-02-26 15:42:37,562 INFO  [RegionManager.rootScanner] master.BaseScanner(225): RegionManager.rootScanner scan of meta region {regionname: -ROOT-,,0, startKey: <>, server: 10.69.80.2:2154} complete
    [junit] 2008-02-26 15:42:38,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:39,500 INFO  [RegionManager.metaScanner] master.BaseScanner(147): RegionManager.metaScanner scanning meta region {regionname: .META.,,1, startKey: <>, server: 10.69.80.2:2154}
    [junit] 2008-02-26 15:42:39,500 INFO  [RegionManager.metaScanner] master.BaseScanner(225): RegionManager.metaScanner scan of meta region {regionname: .META.,,1, startKey: <>, server: 10.69.80.2:2154} complete
    [junit] 2008-02-26 15:42:39,500 INFO  [RegionManager.metaScanner] master.MetaScanner(136): all meta regions scanned
    [junit] 2008-02-26 15:42:39,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:40,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:41,531 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(726): MSG_REGION_CLOSE : test,,1204069326375
    [junit] 2008-02-26 15:42:41,812 INFO  [main] client.HBaseAdmin(248): table test deleted
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/08 21:01;bryanduxbury;473.patch;https://issues.apache.org/jira/secure/attachment/12376766/473.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25218,,,,,Thu Feb 28 22:08:08 UTC 2008,,,,,,,,,,"0|i0h7iv:",98481,,,,,,,,,,,,,,,,,,,,,"27/Feb/08 00:54;jimk;It appears that the master does not clear its return message queue after it sends the close message or it keeps finding the close request in some other data structure that is not getting cleared.;;;","28/Feb/08 21:01;bryanduxbury;Here's a patch that fixes the problem. It also sneaks in a naming-only change of the methods on RegionManager for dealing with regions to close and regions that are closing. 

As a side note, the problem listed in this issue is actually not the only thing that was wrong. When a region server was told to close a region, the region was never taken off of the to-close list, meaning that even if it was reopened for some reason, it would have been told to close again, ad nauseum. Nice catch, Jim.;;;","28/Feb/08 21:02;bryanduxbury;This breaks a lot of stuff. Making it a blocker.;;;","28/Feb/08 21:02;bryanduxbury;Review and run unit tests, please.;;;","28/Feb/08 21:54;jimk;Reviewed patch. +1

TTMR failed on windows, but that just may be windows.

+1
;;;","28/Feb/08 22:08;bryanduxbury;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Passing on edits, we dump all to log",HBASE-472,12389561,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,26/Feb/08 04:45,22/Aug/08 21:13,01/Jul/25 07:49,06/Mar/08 23:02,0.1.0,0.2.0,,,,0.1.0,0.2.0,,,,,,0,"Here is an example:

{code}
2008-02-25 20:50:30,746 DEBUG org.apache.hadoop.hbase.HStore: Passing on edit pagefetch,http://mobile.qusers.com/reply.php?type=html&topic_id=11126&post_index=0&topic_index=0 wap2 20080103151657,1203958365772, data:contents: 
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/08 22:36;stack;472-0.1.patch;https://issues.apache.org/jira/secure/attachment/12377292/472-0.1.patch","06/Mar/08 22:36;stack;472-0.16.0.patch;https://issues.apache.org/jira/secure/attachment/12377291/472-0.16.0.patch","06/Mar/08 22:36;stack;472-TRUNK.patch;https://issues.apache.org/jira/secure/attachment/12377293/472-TRUNK.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25217,,,,,Thu Mar 06 23:02:09 UTC 2008,,,,,,,,,,"0|i0h7in:",98480,,,,,,,,,,,,,,,,,,,,,"06/Mar/08 21:47;stack;Should we fix this for 0.1.0?;;;","06/Mar/08 22:15;bryanduxbury;This is just a question of logging or not logging this info in DEBUG mode?;;;","06/Mar/08 22:22;stack;Yes..  Currently full edit is output into log.  No purpose -- I think I was responsible for this little diamond -- and can be reams of data.;;;","06/Mar/08 22:23;stack;I'm looking at a recovery now... with full web pages dumped into the log.;;;","06/Mar/08 22:25;bryanduxbury;Then it's a no-brainer - take it out. Not the highest priority though.;;;","06/Mar/08 22:36;stack;Remove the log altogether.;;;","06/Mar/08 22:36;stack;For branch;;;","06/Mar/08 22:36;stack;For TRUNK;;;","06/Mar/08 22:43;stack;Fixed it because the instance of this bug that I'm looking at here adding almost 2M lines to a log in a matter of 10 minutes (Same log has the 400k lines from 'impossible state for createlease' in it).

Review when ye get a chance lads.;;;","06/Mar/08 22:49;bryanduxbury;+1 to the patches.;;;","06/Mar/08 23:02;stack;Committed to TRUNK and branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""region offline"" should throw IOException, not IllegalStateException",HBASE-452,12388816,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,bryanduxbury,bien,bien,15/Feb/08 21:24,22/Aug/08 21:13,01/Jul/25 07:49,04/Apr/08 19:42,0.1.0,0.2.0,,,,0.2.0,,,,,,,0,"It would be nice if I could wrap my HTable.get calls in try {} catch (IOException e).  But that doesn't work, since I also need to catch IllegalStateException.  I think that any time there is something wrong with hbase, hbase calls should throw an IOException (or subclass thereof).  Things like IllegalStateException should be reserved for programmer error.",,,,,,,,,,,,,,,,,,,,,,,HBASE-471,,,,,,"04/Apr/08 18:23;bryanduxbury;452.patch;https://issues.apache.org/jira/secure/attachment/12379419/452.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25206,,,,,Fri Apr 04 19:42:58 UTC 2008,,,,,,,,,,"0|i0h7e7:",98460,,,,,,,,,,,,,,,,,,,,,"04/Apr/08 18:23;bryanduxbury;This patch adds a new exception class, RegionOfflineException, and throws that instead of an ISE when it encounters an offline region in HConnectionManager, meaning HTable#get, etc no longer throw ISEs.

Passes tests locally.;;;","04/Apr/08 18:23;bryanduxbury;Review please.;;;","04/Apr/08 19:39;stack;+1 if the new Exception is moved under the client subpackage.   This is a nice improvement.;;;","04/Apr/08 19:42;bryanduxbury;Moved exception, committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fully qualified hbase.rootdir doesn't work,HBASE-446,12388598,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,stack,stack,14/Feb/08 02:13,22/Aug/08 21:13,01/Jul/25 07:49,14/Feb/08 19:02,0.1.0,0.2.0,,,,0.1.0,0.2.0,,,,,,0,"Jim was setting up cluster w/ new hbase.  Setting fully qualified hbase.rootdir was failing.  Complaint was that the filesystems didn't match -- i.e. the hdfs of the fully qualified hbase.rootdir didn't jibe w/ the default hadoop file:///.

Fix needs to be backported.

The problem was that because the hadoop config files were not found (because they are in a different directory and not on the classpath) then fs.get(conf) returns file:///
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/08 18:29;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12375615/patch.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25205,,,,,Thu Feb 14 19:02:07 UTC 2008,,,,,,,,,,"0|i0h7d3:",98455,,,,,,,,,,,,,,,,,,,,,"14/Feb/08 18:29;jimk;Contains fixes to HMaster, HRegionServer and overview.html;;;","14/Feb/08 19:02;jimk;Committed to trunk and 0.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase is very slow at determining table is not present,HBASE-444,12388500,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,bryanduxbury,bien,bien,13/Feb/08 01:14,22/Aug/08 21:13,01/Jul/25 07:49,13/Feb/08 20:32,0.2.0,,,,,0.2.0,,,,,,,0,"If I misspell a table name, it takes a very long time for hbase to determine that the table doesn't exist, because there are many levels of retries.  This often causes timeouts, which then obscure the true cause of the problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/08 03:59;bryanduxbury;444.patch;https://issues.apache.org/jira/secure/attachment/12375434/444.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25203,,,,,Wed Feb 13 20:32:25 UTC 2008,,,,,,,,,,"0|i0h7cn:",98453,,,,,,,,,,,,,,,,,,,,,"13/Feb/08 01:35;bryanduxbury;It would be nice if we could quickly say that a table doesn't exist. However, the question is, how do we do that? 

The current way of determining if a table exists is to locate the first region in the table. As noted, if the table doesn't even exist, you'll have to verify that 5 times before actually admitting it doesn't exist. I don't think this is necessarily behavior we want to change, though, because the table might have just been created, and therefore you don't want to assume a TableNotFound exception should skip retries. 

The alternative is to scan the .META. table to see if the table exists at all. There's even a method for this on HConnectionManager$TableServers - tableExists. Unfortunately, none of this info is cached at all, and that means every table creation would cause a genuine scan of the .META. table. This seems like it would be slow, too. 

I guess one option would be to cache NON-existing table names. After we've become certain that a table doesn't exist (retries have failed), then we could store a flag saying that a table decisively isn't there. The only problem with this approach is that when you then go and create the missing table, how do you inform the clients that it has been created?

Basically, I am open to suggestion as to how this process could be improved.;;;","13/Feb/08 01:48;bien;I'm not sure I understand.  If we check META for the first region, and it's not there, can't we just stop and say the table doesn't exist?;;;","13/Feb/08 01:53;bryanduxbury;No, because there's a chance the table is in the process of being created. If that's just an edge case we aren't very worried about, we can change the behavior to treat TableNotFound exceptions differently.;;;","13/Feb/08 02:01;bien;Ah.  I don't think we need to worry about that edge case -- if I ask for data from a table whose creation has not yet been completed, it seems reasonable to get a TableNotFound exception.;;;","13/Feb/08 03:59;bryanduxbury;This patch makes a special case for TableNotFoundException, rethrowing without any retries. It passes tests locally. 

As an interesting side note, it makes TestHTable, which tries non-existent tables, go from 93 seconds to 50 seconds. ;;;","13/Feb/08 17:06;stack;+1

Commit it Bryan.;;;","13/Feb/08 20:32;bryanduxbury;I just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMLOutputter state should be initialized.,HBASE-438,12388328,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,udanax,udanax,udanax,11/Feb/08 07:30,29/Sep/09 04:05,01/Jul/25 07:49,13/Feb/08 20:42,0.1.0,,,,,0.2.0,,,util,,,,0,"{code}
bash-3.00# bin/hbase shell --html
HQL, 0.2.0-dev version.
Copyright (c) 2008 by udanax, licensed to Apache Software Foundation.
Type 'help;' for usage.

hql > show tables;
<table>
 <tr>
  <th>
Name
  </th>
  <th>
Descriptor
  </th>
 </tr>
 <tr>
  <td>
a
  </td>
  <td>
name: a, families: {b:={name: b, max versions: 3, compression: NONE, in memory: false, block cache enabled: false, max length: 2147483647, bloom filter: none}}
  </td>
 </tr>
</table>
1 table(s) in set. (0.21 sec)
hql > show tables;
Exception in thread ""main"" java.lang.IllegalStateException: getState() == DOCUMENT_ENDED
        at org.znerd.xmlenc.XMLOutputter.startTag(Unknown Source)
        at org.apache.hadoop.hbase.hql.formatter.HtmlTableFormatter.header(HtmlTableFormatter.java:125)
        at org.apache.hadoop.hbase.hql.ShowCommand.execute(ShowCommand.java:66)
        at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:68)        at org.apache.hadoop.hbase.Shell.main(Shell.java:114)
bash-3.00#
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/08 06:57;udanax;438.patch;https://issues.apache.org/jira/secure/attachment/12375312/438.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25201,,,,,Wed Feb 13 20:42:50 UTC 2008,,,,,,,,,,"0|i0h7bb:",98447,,,,,,,,,,,,,,,,,,,,,"12/Feb/08 06:57;udanax;Submitting.;;;","13/Feb/08 01:12;bryanduxbury;Tests pass locally for me. Patch seems pretty simple. +1;;;","13/Feb/08 13:23;udanax;Thanks for your review, bryan.
Please commit it.;;;","13/Feb/08 20:36;jimk;Patch looks ok +1;;;","13/Feb/08 20:42;bryanduxbury;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear Command should use system.out,HBASE-437,12388323,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,udanax,udanax,udanax,11/Feb/08 05:46,29/Sep/09 04:05,01/Jul/25 07:49,11/Feb/08 17:26,0.1.0,,,,,0.2.0,,,util,,,,0,"Current clear command doesn't work, It should use system.out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/08 05:48;udanax;437.patch;https://issues.apache.org/jira/secure/attachment/12375204/437.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25200,,,,,Mon Feb 11 17:26:02 UTC 2008,,,,,,,,,,"0|i0h7b3:",98446,,,,,,,,,,,,,,,,,,,,,"11/Feb/08 05:48;udanax;Patch attach.;;;","11/Feb/08 05:48;udanax;Submitting.;;;","11/Feb/08 17:26;stack;Committed.  Thanks for the patch Edward.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestTableIndex failed in HBasePatch build #14,HBASE-434,12388290,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jimk,jimk,jimk,10/Feb/08 02:01,22/Aug/08 21:13,01/Jul/25 07:49,13/Feb/08 00:11,0.2.0,,,,,0.2.0,,,test,,,,0,"TestTableIndex failed in HBase-Patch build #14. See http://hudson.zones.apache.org/hudson/job/HBase-Patch/14/testReport/

junit.framework.AssertionFailedError
	at org.apache.hadoop.hbase.MultiRegionTable.makeMultiRegionTable(MultiRegionTable.java:137)
	at org.apache.hadoop.hbase.mapred.TestTableIndex.setUp(TestTableIndex.java:125)",,,,,,,,,,,,,,,,,,,,HBASE-435,,,,,,,,,"12/Feb/08 22:32;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12375422/patch.txt","12/Feb/08 22:09;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12375417/patch.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25198,,,,,Wed Feb 13 00:11:47 UTC 2008,,,,,,,,,,"0|i0h7af:",98443,,,,,,,,,,,,,,,,,,,,,"12/Feb/08 22:09;jimk;Fixes both HBASE-434 and HBASE-435;;;","12/Feb/08 22:32;jimk;Fix paths for patch.;;;","12/Feb/08 23:27;bryanduxbury;Tested locally, passes. +1;;;","13/Feb/08 00:11;jimk;HBASE-43[45] TestTableIndex and TestTableMapReduce failed in Hudson builds - passed per Bryan Duxbury;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
region server should deleted restore log after successfull restore,HBASE-433,12388282,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,viper799,viper799,09/Feb/08 23:03,22/Aug/08 21:13,01/Jul/25 07:49,12/Mar/08 17:07,0.1.0,,,,,0.1.0,0.2.0,,regionserver,,,,0,"Currently we do not remove the restore log ""oldlogfile.log"" after we reopen a region after a crashed region server.

Suggestion would be to remove after we successfully flush of all the edits to a mapfile

so something like:
replay log 
memcache flush
deleted log


",,,,,,,,,,,,,,,,,,,,HBASE-251,HBASE-236,,,,,,,,"12/Mar/08 15:13;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12377702/patch.txt","11/Mar/08 22:25;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12377645/patch.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25197,,,,,Wed Mar 12 17:07:20 UTC 2008,,,,,,,,,,"0|i0h7a7:",98442,,,,,,,,,,,,,,,,,,,,,"09/Feb/08 23:11;viper799;might also take a look at what point the master sees a region as crashed from a region server

example

say I have a failed region server and master loads and saves restore logs for each region 

on the reopen of a region and say it replaying the logs and crashes 

Would the master overwrite the old log file with a new one or would the master not consider the region as part of the region server on a crash until it receives an MSG_REPORT_PROCESS_OPEN message?
Should look in to this so we do not have a risk of over writing a restore log file that has not successfully loaded and flushed to disk.;;;","10/Feb/08 01:31;jimk;Current situation:

If a region server crashes before it has closed any log files, then all it will leave is one zero length log file which will be ignored.

However, if the region server crashes after closing one or more log files, and the region server was starting up a region that had an old log file (and the region server crashed before recovering the old log file), then yes, the old log file would be overwritten.

Solution for current situation:

HLog.splitLog should check for the presence of an existing log file, and copy its contents into a new file before processing the region server's log file(s).

Future (when HDFS has appends):

The region server will never leave a zero length log file unless it has received no updates since it started or since it closed the most recent log file.

Solution for future:

If HDFS supports appends to an existing file, then splitLog should open the region's old log file for append or create it if it does not exist.

If HDFS does not support appends to an existing file, then the solution for the current situation would still work.;;;","10/Feb/08 01:33;jimk;And yes, the region server should delete the old log file once it has been completely recovered and flushed.;;;","11/Mar/08 22:25;jimk;HLog
- don't overwrite oldlogfile in splitLog if it already exists. Rename it and copy it into the new oldlogfile. Then delete it once it has been copied.
- use FileUtil.fullyDelete to delete region server log directory.

HRegion
- delete oldlogfile once it has been successfully processed
;;;","11/Mar/08 22:25;jimk;Please review.;;;","12/Mar/08 01:07;viper799;I would test but my test cluster is in route to the data center. someone could hard kill a region server and see if the logs get removed after the region recovers.;;;","12/Mar/08 05:00;bryanduxbury;Silly comment, but should the math happen after the LOG.isDebugEnabled() so that when not in debug we don't do the math on HLog:586?

Tests pass, code looks pretty good. +1;;;","12/Mar/08 15:06;jimk;Good point. I had just cut and pasted from below, but you're right. Why do the math if you aren't going to use the results? I'll change that before I check it in.
;;;","12/Mar/08 15:13;jimk;Same as before, but don't do the math if you aren't going to use it.;;;","12/Mar/08 17:07;jimk;Committed to 0.1 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Under continuous upload of rows, WrongRegionExceptions are thrown that reach the client even after retries",HBASE-428,12388196,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,happyharris,happyharris,08/Feb/08 14:36,22/Aug/08 21:13,01/Jul/25 07:49,26/Feb/08 05:10,0.1.0,0.2.0,,,,0.2.0,,,regionserver,,,,0,"I have installed 0.16.0 rc 1 which I believe contains a fix for similar issue HBASE-138,  but I still see the same problem.

- I am using a single node.
- The client application runs in a single thread, loading data into a single table.
- I get good throughput of about 200 rows/sec to start with, with occasional significant drops due to NotServingRegionException's that are recoverable on client retry (internal to hbase).
- After 54 minutes, and about 500,000 rows I start to see WrongRegionException's in the client application, i.e. real failures. (Note that this compares to 0.15.3 which would being to throw NotServingRegionExceptions after a few tens of thousands of rows).

My data consists of a single table with 5 column families. The data written is as follows:>>
key: a URL
family 1: a small string, often emty, 2 longs, 1 int
family 2: a byte averaging averaging between 1k and 10k, a small string
family 3: several columns with different names per row, values of small strings
family 4: most rows have zero columns, some rows have 1 or more columns with a UL value
The URLs are typically ""long-ish"" URL as seen when crawling a site, not short home page URLs  
 
I am assuming the data is stored in files of the form <hbaseroot>//<tablename>/<9digitnum>/data/mapfiles/<19digitnum>/data. I have attached a csv file showing the distribution of size of these files. Average size is 19Mb, but the sizes are not evenly distributed at all


Here are two sample exceptions thrown, copied from the region server log:

2008-02-08 02:08:22,495 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 60020, call batchUpdate(pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924,1202401088077, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@feb215) from 66.135.42.137:38484: error: org.apache.hadoop.hbase.WrongRegionException: Requested row out of range for HRegion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924,1202401088077, startKey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', getEndKey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', row='http://go2purdue.com/Redeemer_University.cfm?pt=2&sp=2&vid=1199243289_3X02X1468757255&rpt=2&kt=4&kp=1 wap2 20080102081237'
org.apache.hadoop.hbase.WrongRegionException: Requested row out of range for HRegion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924,1202401088077, startKey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', getEndKey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', row='http://go2purdue.com/Redeemer_University.cfm?pt=2&sp=2&vid=1199243289_3X02X1468757255&rpt=2&kt=4&kp=1 wap2 20080102081237'
        at org.apache.hadoop.hbase.HRegion.checkRow(HRegion.java:1486)
        at org.apache.hadoop.hbase.HRegion.obtainRowLock(HRegion.java:1531)
        at org.apache.hadoop.hbase.HRegion.batchUpdate(HRegion.java:1226)
        at org.apache.hadoop.hbase.HRegionServer.batchUpdate(HRegionServer.java:1433)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
2008-02-08 02:08:22,696 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 60020, call batchUpdate(pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924,1202401088077, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@15d9be1) from 66.135.42.137:38484: error: org.apache.hadoop.hbase.WrongRegionException: Requested row out of range for HRegion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924,1202401088077, startKey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', getEndKey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', row='http://go2umass.com/Travel.cfm?pt=2&sp=2&vid=1199230721_3X04X1485302803&rpt=2&kt=5&kp=8 wap2 20080102081239'
org.apache.hadoop.hbase.WrongRegionException: Requested row out of range for HRegion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924,1202401088077, startKey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', getEndKey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=D2ED1EB898163CDB27135DC2CF6958B3.197B?rsi=78011 wap2 20080102052924', row='http://go2umass.com/Travel.cfm?pt=2&sp=2&vid=1199230721_3X04X1485302803&rpt=2&kt=5&kp=8 wap2 20080102081239'
        at org.apache.hadoop.hbase.HRegion.checkRow(HRegion.java:1486)
        at org.apache.hadoop.hbase.HRegion.obtainRowLock(HRegion.java:1531)
        at org.apache.hadoop.hbase.HRegion.batchUpdate(HRegion.java:1226)
        at org.apache.hadoop.hbase.HRegionServer.batchUpdate(HRegionServer.java:1433)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
",Linux 2.6.9-67.0.1.ELsmp #1 SMP Wed Dec 19 16:01:12 EST 2007 i686 athlon i386 GNU/Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/08 23:44;stack;428-2.patch;https://issues.apache.org/jira/secure/attachment/12375766/428-2.patch","17/Feb/08 01:10;stack;428-v3.patch;https://issues.apache.org/jira/secure/attachment/12375768/428-v3.patch","17/Feb/08 01:34;stack;428-v4.patch;https://issues.apache.org/jira/secure/attachment/12375769/428-v4.patch","28/Feb/08 22:01;happyharris;428-v5.patch;https://issues.apache.org/jira/secure/attachment/12376771/428-v5.patch","15/Feb/08 21:56;stack;428.patch;https://issues.apache.org/jira/secure/attachment/12375719/428.patch","08/Feb/08 14:45;happyharris;filesbysize.csv;https://issues.apache.org/jira/secure/attachment/12375075/filesbysize.csv","11/Feb/08 19:49;happyharris;lsr;https://issues.apache.org/jira/secure/attachment/12375263/lsr","12/Feb/08 00:51;happyharris;selectfrommeta.txt;https://issues.apache.org/jira/secure/attachment/12375293/selectfrommeta.txt",,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25193,,,,,Thu Feb 28 22:01:57 UTC 2008,,,,,,,,,,"0|i0h793:",98437,,,,,,,,,,,,,,,,,,,,,"08/Feb/08 14:45;happyharris;Shows distribution of data file size;;;","11/Feb/08 19:33;stack;Marc:

On sizes, yeah, they'll vary.  In future, just do a ./bin/hadoop fs -lsr on your hbase.rootdir if you want to convey the varying sizes.  The lsr will also show better how the mapfiles are grouped (and we can see if compaction and splitting is keeping up w/ the upload rate).;;;","11/Feb/08 19:49;happyharris;executed
   bin/hadoop -fs -lsr / 
as suggested (hbase is the only thing in the hadoop instance);;;","11/Feb/08 20:09;stack;Thanks Marc.  That listing looks pretty good.  Says to me that hbase is not being overwhelmed, its just the WRE thats crippled the upload.  If we can figure that...;;;","12/Feb/08 00:51;happyharris;The results for running select * from .META. in an HBASE shell. I notice that there some non-printable characters. I'm not sure if those are a results of the terminal I was using, or something else. I couldn't figure out a way to redirect the output of the select command directly to a file.;;;","15/Feb/08 06:14;stack;Thanks for posting the .META. select Marc.

I've noticed a few things.  Here's a region whose start and end key is same:

{code}
2008-02-10 16:18:15,134 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_OPEN : pagefetch,http://fun.twilightwap.com/rate.asp?joke_id=183&rating=0 wap2 20080102055026,1202660291003 from 66.135.42.137:60020
2008-02-10 16:18:15,134 DEBUG org.apache.hadoop.hbase.HMaster: Main processing loop: PendingOpenOperation from 66.135.42.137:60020
2008-02-10 16:18:15,134 INFO org.apache.hadoop.hbase.HMaster: 66.135.42.137:60020 serving pagefetch,http://fun.twilightwap.com/rate.asp?joke_id=183&rating=0 wap2 20080102055026,1202660291003
2008-02-10 16:18:15,134 INFO org.apache.hadoop.hbase.HMaster: regionname: pagefetch,http://flirtbox.mobi/new.php?type=html&forum_id=95&topic_index=0 wap2 20071222232620,1202660291003, startKey: <http://flirtbox.mobi/new.php?type=html&forum_id=95&topic_index=0 wap2 20071222232620>, endKey: <http://fun.twilightwap.com/rate.asp?joke_id=183&rating=0 wap2 20080102055026>, encodedName: 1636112728, tableDesc: {name: pagefetch, families: {changedata:={name: changedata, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, data:={name: data, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, headers:={name: headers, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, info:={name: info, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, redirects:={name: redirects, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}} open on 66.135.42.137:60020
{code}

Here is the region that was split that produced the above:

{code}
2008-02-10 16:17:54,112 INFO org.apache.hadoop.hbase.HMaster: regionname: pagefetch,http://flirtbox.mobi/new.php?type=html&forum_id=95&topic_index=0 wap2 20071222232620,1202660269165, startKey: <http://flirtbox.mobi/new.php?type=html&forum_id=95&topic_index=0 wap2 20071222232620>, endKey: <http://go2uwash.com/ wap2 20071222205139>, encodedName: 7645492, tableDesc: {name: pagefetch, families: {changedata:={name: changedata, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, data:={name: data, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, headers:={name: headers, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, info:={name: info, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, redirects:={name: redirects, max versions: 1, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}} open on 66.135.42.137:60020
{code}

Looks like it has go2uwash as end key.  Why doesn't fun.twilightwap.com region have go2wash as its end key?  The row we are trying to insert is 'http://go2purdue.com/Indiana_State_University_Terre_Haute.cfm?pt=2&sp=2&vid=1199235588_3X02X1468516268&rpt=2&kt=5&kp=8 wap2 20080102090745' which would go into this region if go2wash was the end key.

For good measure, here is the regionserver split report:

{code}
2008-02-10 16:18:12,053 INFO org.apache.hadoop.hbase.HRegionServer: region split, META updated, and report to master all successful. Old region=pagefetch,http://flirtbox.mobi/new.php?type=html&forum_id=95&topic_index=0 wap2 20071222232620,1202660269165, new regions: pagefetch,http://flirtbox.mobi/new.php?type=html&forum_id=95&topic_index=0 wap2 20071222232620,1202660291003, pagefetch,http://fun.twilightwap.com/rate.asp?joke_id=183&rating=0 wap2 20080102055026,1202660291003. Split took 1sec
{code};;;","15/Feb/08 06:15;stack;Marking blocker.  Assigning myself.  Fix needs to be backported.;;;","15/Feb/08 21:56;stack;Patch to add more logging around split.  Adds consistency check to make sure start and end keys are not same on a split.  Logs split details.;;;","16/Feb/08 14:33;happyharris;The patch seems to have got past this error, but now a new one (out of heap) occurs later. Possibly this bug should be considered fixed and a new one opened.

Current results:
I no longer get WrongRegionException's
After approx 700,000 rows uploaded, the region server throws an OutOfMemoryError, followed by many ""Server not running"" exceptions (exception log below).
I am able to restart the hbase region and master servers (and the client app), and store another 800,000 rows before the same OutOfMemoryError.
After that, I can restart the hbase region and master servers (and the client app), but continuing the upload causes more OutOfMemoryError exceptions quickly.

Full logs will be sent to stack by e-mail.

008-02-16 02:24:38,884 INFO org.apache.hadoop.hbase.HLog: new log writer created at hdfs://server14:54310/hbase/log_66.135.42.137_1203123804816_60020/hlog.dat.322
2008-02-16 02:25:45,751 DEBUG org.apache.hadoop.hbase.HRegion: Started memcache flush for region pagefetch,http://www.marketwatch.com/hdml wap2 20071222205256,1203126936284. Size 62.6m
2008-02-16 02:25:57,378 FATAL org.apache.hadoop.hbase.HRegionServer: Set stop flag in regionserver/0:0:0:0:0:0:0:0:60020.cacheFlusher
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$Packet.<init>(DFSClient.java:1518)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:2125)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:141)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:100)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:41)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)
	at org.apache.hadoop.io.MapFile$Writer.append(MapFile.java:188)
	at org.apache.hadoop.hbase.HStoreFile$BloomFilterMapFile$Writer.append(HStoreFile.java:721)
	at org.apache.hadoop.hbase.HStore.internalFlushCache(HStore.java:1113)
	at org.apache.hadoop.hbase.HStore.flushCache(HStore.java:1081)
	at org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:954)
	at org.apache.hadoop.hbase.HRegion.flushcache(HRegion.java:852)
	at org.apache.hadoop.hbase.HRegionServer$Flusher.run(HRegionServer.java:417)
2008-02-16 02:25:57,405 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 60020, call batchUpdate(pagefetch,http://mobility.mobi/showthread.php?goto=newpost&t=2677 wap2 20071223005632,1203126357490, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@9bad5a) from 66.135.42.137:56275: error: java.io.IOException: Server not running
java.io.IOException: Server not running
	at org.apache.hadoop.hbase.HRegionServer.checkOpen(HRegionServer.java:1626)
	at org.apache.hadoop.hbase.HRegionServer.batchUpdate(HRegionServer.java:1429)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
2008-02-16 02:25:57,406 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 60020, call getClosestRowBefore(.META.,,1, pagefetch,http://pda.physorg.com/lofi-news-seafloor-fault-tsunami_114370203.html wap2 20080102111657,999999999999999, 9223372036854775807) from 66.135.42.137:56275: error: java.io.IOException: Server not running

;;;","16/Feb/08 23:14;stack;Marc, see if #3 in the FAQ helps: http://wiki.apache.org/hadoop/Hbase/FAQ#3.  Let us know how it goes.;;;","16/Feb/08 23:44;stack;Cleaned up version of patch I sent to Marc;;;","17/Feb/08 00:29;stack;Thanks for sending the logs Marc.  I took a look.  Things are messy -- lots of concurrent compactions going on -- but seemed to be holding up.

The patch doesn't actually prevent the core problem.  The patch added extra logging and it added not splitting if it would result in a region that had same start and end key.  Here's what I see:

{code}
2008-02-16 02:03:05,324 DEBUG org.apache.hadoop.hbase.HRegion: Split details for pagefetch,http://www.marketwatch.com/hdml wap2 20071222205256,1203126936284: startKey http://www.marketwatch.com/hdml wap2 20071222205256, midkey: http://www.marketwatch.com/hdml wap2 20071222205256, endKey http://www.myarmoury.com/mobile wap2 20071222205348
2008-02-16 02:03:05,324 DEBUG org.apache.hadoop.hbase.HRegion: Startkey and midkey are same, not splitting
{code}

In other words, no more WrongRegionExceptions because we notice ahead of time that the split will produce a region w/ same start and end key and we skip.  Watching the logs, the region continues to grow in size and further attempts at splitting also pass because it would result in region w/ same start and end key.  There is something up in our midkey calculation.

;;;","17/Feb/08 01:10;stack;v3 has more cleanup.  Fixes rare case where if a store file had a single entry only, we were overwriting the calculated midKey for the region (could be whats wrong w/ our midkey calc. but wouldn't want to bet on it -- seems like too rare an occurance).

M  src/java/org/apache/hadoop/hbase/HStoreFile.java
    (finalKey) Removed checkKey if trying to get finalKey on top-half
    of a HalfMapFile.  Was failing because it would compare the
    HalfMapFile midkey to the empty passed key into which we're to
    set the mapfile last key.
M src/java/org/apache/hadoop/hbase/HStore.java
    If a return is being done inside the 'if' of an 'if/else', then
    the else is not needed.  Fixed two of these.  Removed commented
    out logging.  Rewrote a cumbersome if/else as a ?:.
    Removed unused rowKey assignment. Removed unnecessary casts.
    Renamed localvariable midkey as mk because it was too close
    to the passed in arg midKey.  Do NOT copy
    midkey/mk before test that it was equal to start/end keys. A
    store could have a midkey/mk that equaled region start/end keys
    and we were nonetheless overwriting midKey, the passed in
    arg.
M src/java/org/apache/hadoop/hbase/HRegionServer.java
    Removed commented out code.  Output full regioninfo when splitting
    so can see start and end keys, etc.
M  src/java/org/apache/hadoop/hbase/HRegion.java
    If end or start key matches mid key, don't split.;;;","17/Feb/08 01:34;stack;Added test for splittability.  We were going ahead calculating midkey though store wasn't splittalble anways 'cos it still had references.;;;","19/Feb/08 18:42;bryanduxbury;+1 the patch passes unit tests for me locally. ;;;","19/Feb/08 19:04;stack;Applied v4 to branch and trunk.  Will wait on feedback from Marc as to whether this addresses his issue.;;;","25/Feb/08 20:21;bryanduxbury;This needs to get done before 0.2 can roll.;;;","25/Feb/08 22:34;happyharris;Since applying 428-v4.patch and increasing my heap to 2G, I have not had a recurrence of this issue.;;;","26/Feb/08 05:10;stack;Marc sent me logs of an uploading to review.  No more instances of WRE.  Splits, compactions, and flushes run fine.   Resolving.;;;","28/Feb/08 22:01;happyharris;428-v4.patch was made against slightly the wrong version. v4 included the removal of some commented out code in HRegionServer.java that caused the patch to fail (the code was already removed in the official 0.16.0-rc1 release;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase can't find remote filesystem,HBASE-426,12388144,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,stack,stack,07/Feb/08 23:32,22/Aug/08 21:13,01/Jul/25 07:49,09/Feb/08 01:55,0.1.0,0.2.0,,,,0.1.0,0.2.0,,,,,,0,"If filesystem is remote, e.g. its an Hadoop DFS running ""over there"", there is no means of pointing hbase at it currently (unless you count copying hadoop-site.xml into hbase/conf).  Should be possible to just set a fully qualified hbase.rootdir and that should be sufficient for hbase figuring the fs (needs to be backported to 0.1 too).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/08 01:41;stack;rootdir-v2.patch;https://issues.apache.org/jira/secure/attachment/12375132/rootdir-v2.patch","09/Feb/08 01:43;stack;rootdir-v3.patch;https://issues.apache.org/jira/secure/attachment/12375133/rootdir-v3.patch","09/Feb/08 01:33;stack;rootdir.patch;https://issues.apache.org/jira/secure/attachment/12375131/rootdir.patch",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25192,,,,,Sat Feb 09 01:55:12 UTC 2008,,,,,,,,,,"0|i0h78n:",98435,,,,,,,,,,,,,,,,,,,,,"08/Feb/08 00:00;stack;Made it a blocker (for 0.2 and 0.1);;;","08/Feb/08 17:39;bryanduxbury;There was a suggestion of having the hbase.rootdir parameter be a filesystem and path combined, like hdfs://server:port/path/to/hbase/root. This seems like it'd be a really simple, clear, useful way to specify where your files go.

Also, I think there should be a reasonable default value for this in hbase-site.xml, so that you KNOW that you have to change it. Putting it in hbase-default.xml would just be confusing.;;;","09/Feb/08 01:33;stack;Change hbase.rootdir to expectation is that its fully-qualified: i.e. includes the filesystem implementation and full location information.  Addressed also issue Billy Pearson found where we still referred to hadoop.tmp.dir.  Removed fs.default.name setting in hbase-site.xml for test.  Tests now use home directory in minidfs as hbase.rootdir. Bulk of the patch is setting into HBaseConfiguration the rootdir (minidfs randomly picks a port).  Also removed 'default rootdir'. HBase should fail if you haven't specified root dir for you hbase.;;;","09/Feb/08 01:41;stack;Minor changes to getting started so its clearer how you point at a filesystem.;;;","09/Feb/08 01:43;stack;v3 includes test I failed to update.;;;","09/Feb/08 01:55;stack;Committed to branch and TRUNK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestRegionServerExit broken,HBASE-421,12388050,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jimk,jimk,jimk,07/Feb/08 08:24,22/Aug/08 21:13,01/Jul/25 07:49,08/Feb/08 06:45,0.1.0,0.2.0,,,,0.1.0,0.2.0,,test,,,,0,"TestRegionServerExit has a couple of problems:

1. Region server tries to start http server on a port already in use:
    [junit] 2008-02-07 07:01:06,529 FATAL [RegionServer:2] hbase.HRegionServer(867): Failed init
    [junit] java.io.IOException: Problem starting http server
    [junit] 	at org.apache.hadoop.hbase.util.InfoServer.start(InfoServer.java:227)
    [junit] 	at org.apache.hadoop.hbase.HRegionServer.startServiceThreads(HRegionServer.java:928)
    [junit] 	at org.apache.hadoop.hbase.HRegionServer.init(HRegionServer.java:863)
    [junit] 	at org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:633)
    [junit] 	at java.lang.Thread.run(Thread.java:595)
    [junit] Caused by: org.mortbay.util.MultiException[java.net.BindException: Address already in use]
    [junit] 	at org.mortbay.http.HttpServer.doStart(HttpServer.java:731)
    [junit] 	at org.mortbay.util.Container.start(Container.java:72)
    [junit] 	at org.apache.hadoop.hbase.util.InfoServer.start(InfoServer.java:205)
    [junit] 	... 4 more
    [junit] 2008-02-07 07:01:06,530 FATAL [RegionServer:2] hbase.HRegionServer(772): Unhandled exception. Aborting...

The region server that died apparently was serving the root region.

The test case apparently has a long timeout for finding the root region because you see a lot of 

    [junit] 2008-02-07 07:04:14,813 DEBUG [Thread-540] hbase.HConnectionManager$TableServers(708): Wake. Retry finding root region.
    [junit] 2008-02-07 07:04:14,814 DEBUG [Thread-540] hbase.HConnectionManager$TableServers(704): Sleeping. Waiting for root region.
    [junit] 2008-02-07 07:04:24,823 DEBUG [Thread-540] hbase.HConnectionManager$TableServers(708): Wake. Retry finding root region.
    [junit] 2008-02-07 07:04:24,827 DEBUG [Thread-540] hbase.HConnectionManager$TableServers(704): Sleeping. Waiting for root region.
    [junit] 2008-02-07 07:04:34,833 DEBUG [Thread-540] hbase.HConnectionManager$TableServers(708): Wake. Retry finding root region.
    [junit] 2008-02-07 07:04:34,836 DEBUG [Thread-540] hbase.HConnectionManager$TableServers(704): Sleeping. Waiting for root region.
    [junit] 2008-02-07 07:04:44,842 DEBUG [Thread-540] hbase.HConnectionManager$TableServers(708): Wake. Retry finding root region.

until finally the client gives up:

    [junit] 2008-02-07 07:04:44,843 FATAL [Thread-540] hbase.TestRegionServerExit$1(161): could not re-open meta table because
    [junit] org.apache.hadoop.hbase.NoServerForRegionException: Timed out trying to locate root region
    [junit] 	at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:718)
    [junit] 	at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:329)
    [junit] 	at org.apache.hadoop.hbase.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:311)
    [junit] 	at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:476)
    [junit] 	at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:339)
    [junit] 	at org.apache.hadoop.hbase.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:311)
    [junit] 	at org.apache.hadoop.hbase.HTable.getRegionLocation(HTable.java:114)
    [junit] 	at org.apache.hadoop.hbase.HTable$ClientScanner.nextScanner(HTable.java:889)
    [junit] 	at org.apache.hadoop.hbase.HTable$ClientScanner.<init>(HTable.java:817)
    [junit] 	at org.apache.hadoop.hbase.HTable.obtainScanner(HTable.java:522)
    [junit] 	at org.apache.hadoop.hbase.HTable.obtainScanner(HTable.java:411)
    [junit] 	at org.apache.hadoop.hbase.TestRegionServerExit$1.run(TestRegionServerExit.java:156)
    [junit] 	at java.lang.Thread.run(Thread.java:595)
    [junit] Exception in thread ""Thread-540"" junit.framework.AssertionFailedError
    [junit] 	at junit.framework.Assert.fail(Assert.java:47)
    [junit] 	at junit.framework.Assert.fail(Assert.java:53)
    [junit] 	at org.apache.hadoop.hbase.TestRegionServerExit$1.run(TestRegionServerExit.java:162)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

Which is not the way the test is supposed to run at all.
It appears that when we start multiple region servers in a MiniHBaseCluster, they all try to start their http server on the same port. In the past I believe that the http server start failure was not fatal, so the test ran.

We should either have some kind of setting for MiniHBaseCluster that tells the master and region servers not to start their http servers, or some way of telling multiple servers not to start on the same port, or making http startup failure non-fatal.

Tests like these are good as they (eventually) point out a regression to us.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/08 03:07;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12375033/patch.txt","08/Feb/08 00:47;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12375029/patch.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25189,,,,,Fri Feb 08 06:45:33 UTC 2008,,,,,,,,,,"0|i0h77j:",98430,,,,,,,,,,,,,,,,,,,,,"07/Feb/08 17:23;jimk;TestRegionServerExit does not report failure when if fails. It does write a success message if it succeeds, but does not do a fail() if it never reaches that point.;;;","07/Feb/08 17:31;stack;(repeat from mail to BD and JK)

In src/test/hbase-site.xml, it has following configuration:

{code}
  <property>
    <name>hbase.regionserver.info.port</name>
    <value>-1</value>
    <description>The port for the hbase regionserver web UI
    Set to -1 if you do not want the info server to run.
    </description>
  </property>
{code}

This is supposed to disable info servers for tests (only turned on explicitly for the test that tests info server).  Maybe its not working any more?;;;","07/Feb/08 17:40;jimk;IIRC, both the master and the region servers (but maybe only the region servers - I'll check) are trying to start their http servers on the default port, meaning either they are not seeing the config setting or are broken in some other manner.

That they could not was not an issue in the past because not being able to start the http server was not a fatal error (and if they really are supposed to, it should be a fatal error);;;","07/Feb/08 19:35;bryanduxbury;This needs to get done ASAP. It's adding 15 minutes to every test suite run, so let's get this knocked out.;;;","08/Feb/08 03:07;jimk;Increase memory for tests, 256M may be too little;;;","08/Feb/08 04:02;bryanduxbury;v2 patch passes everything. +1, let's commit.;;;","08/Feb/08 04:27;stack;Marking this as an issue to backport to 0.1 branch;;;","08/Feb/08 04:58;jimk;Committed to trunk. Still need to backport to 0.1.0;;;","08/Feb/08 06:45;jimk;Committed to 0.2.0 and backported to 0.1.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shell should allow deletions in .META. and -ROOT- tables,HBASE-281,12387503,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,udanax,bryanduxbury,bryanduxbury,30/Jan/08 21:09,29/Sep/09 04:05,01/Jul/25 07:49,01/Mar/08 16:45,0.1.0,0.16.0,0.2.0,,,0.1.0,0.2.0,,util,,,,0,"For administrative and debugging purposes, it would be nice to be able to delete rows from .META. via the shell. The alternative is writing custom java code to do such operations, which is just ridiculous. The reality of HBase's maturity is that from time to time we're going to have to reach into the .META. and -ROOT- tables to fix things, so I think the shell should be where that happens.

Currently, attempting to delete from either table gives a ""non-existant table"" error. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/08 01:41;udanax;2744.patch;https://issues.apache.org/jira/secure/attachment/12374429/2744.patch","06/Feb/08 01:14;bryanduxbury;281-v2.patch;https://issues.apache.org/jira/secure/attachment/12374836/281-v2.patch","06/Feb/08 18:35;bryanduxbury;281-v3.patch;https://issues.apache.org/jira/secure/attachment/12374892/281-v3.patch","04/Feb/08 19:10;bryanduxbury;281.patch;https://issues.apache.org/jira/secure/attachment/12374699/281.patch",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,25065,,,,,Thu Mar 06 21:50:07 UTC 2008,,,,,,,,,,"0|i0h67r:",98269,,,,,,,,,,,,,,,,,,,,,"31/Jan/08 01:44;bryanduxbury;I have not applied and tested, but the patch looks good. +1;;;","31/Jan/08 03:11;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12374429/2744.patch
against trunk revision 616796.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1714/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1714/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1714/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1714/console

This message is automatically generated.;;;","04/Feb/08 19:10;bryanduxbury;Updated for new svn.;;;","06/Feb/08 01:14;bryanduxbury;Original edited patch was messed up... stupid sed!;;;","06/Feb/08 18:35;bryanduxbury;I was wondering, would the changes in the v3 patch I'm posting solve the problem? It addresses the problem more from the core than from the shell, which is probably a better solution.;;;","07/Feb/08 00:29;udanax;+1 to v2 patch.

;;;","07/Feb/08 06:20;bryanduxbury;Edward - did you mean to indicate the latest (v3), or are you actually pointing to the v3 patch? Is there some reason that v2 is preferable to v3?;;;","07/Feb/08 21:44;stack;Committed (Mistakenly along with HBASE-28).  I committed v3.  I ran unit tests locally and it doesn't break anything.  Also tried it and does the right thing (I do notice that we seem to now scan -ROOT- and .META. twice first time through -- has it always been so?).

Thanks for earlier patches Edward and thanks for final Bryan.;;;","01/Mar/08 00:22;stack;Backport this issue.  Useful for fixing broken hbase tables.;;;","01/Mar/08 00:23;stack;Made it a blocker.  Need to backport to 0.1;;;","01/Mar/08 16:44;stack;Backported.  Closing again.;;;","06/Mar/08 21:50;stack;Already backported. Marking as fixed i 0.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[hbase] hregioninfo cell empty in meta table,HBASE-27,12387000,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jimk,stack,stack,24/Jan/08 05:23,22/Aug/08 21:13,01/Jul/25 07:49,13/Mar/08 01:27,0.1.0,,,,,0.1.0,0.2.0,,master,,,,0,"When we notice one of these, instead of reporting on it over and over -- see below -- lets just axe the whole row.   Its never going to get better on its own.  We should also figure how these horked rows get manufactured.  Below is about split cells but also instances where servercode and servername are all thats left in a row.

{code}
2008-01-24 02:01:02,761 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,761 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,762 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,762 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,762 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,763 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitB]
2008-01-24 02:01:02,763 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,763 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,764 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,764 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,764 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitB]
2008-01-24 02:01:02,765 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,765 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,765 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,766 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,766 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,766 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitB]
2008-01-24 02:01:02,767 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,767 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitB]
2008-01-24 02:01:02,767 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,768 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitB]
2008-01-24 02:01:02,768 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitB]
2008-01-24 02:01:02,768 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,769 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,769 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,769 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,770 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,770 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]
2008-01-24 02:01:02,771 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,771 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitB]
2008-01-24 02:01:02,771 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA, info:splitB]
2008-01-24 02:01:02,772 WARN org.apache.hadoop.hbase.HMaster:
info:regioninfo is empty; has keys: [info:splitA]


{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/08 18:05;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12377630/patch.txt","10/Mar/08 21:28;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12377554/patch.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,24865,,,,,Thu Mar 13 01:27:32 UTC 2008,,,,,,,,,,"0|i0h3lb:",97844,,,,,,,,,,,,,,,,,,,,,"24/Jan/08 15:55;jimk;It appears that we are not cleaning out all the possible cells when a region is deleted.;;;","24/Jan/08 17:48;bryanduxbury;Easy enough to switch to deleteAll(row). ;;;","02/Feb/08 00:35;bryanduxbury;Let's get this done in 0.17.;;;","07/Mar/08 00:38;bryanduxbury;Not that important. Does this even happen anymore in trunk?;;;","10/Mar/08 21:28;jimk;Patch for 0.1.0;;;","10/Mar/08 21:45;stack;Why is the internal class RowMap made public?

This patch is a band-aid on a real issue we have yet to find -- improper clean up of rows.  Maybe it can help us figure where the real prob. is by outputting more info about the row its deleting?  E.g., in deleteEmptyMetaRows, do you think it should enumerate all the cells that are remaining?  Might give us clue as to where the meta row removes is working improperly.

Otherwise, patch looks good to me.

;;;","10/Mar/08 21:53;bryanduxbury;Is there any chance that we are seeing the row halfway through an update? If so, then deleting the entire thing when one of the cells is empty would really break things. 

The original suggestion was to use deleteAll in situations where we wanted to delete a row, and assume that the problem was some rows were incompletely cleaned up because we weren't doing deleteAll before. In that regard, I think the change to HRegion is correct, but perhaps the cleanup in HMaster is premature. 

Should we commit a patch with just the HRegion deleteAll change, and see if this problem manifests itself any further?;;;","10/Mar/08 22:51;jimk;stack - 10/Mar/08 02:45 PM 
> Why is the internal class RowMap made public?

The class is private static meaning that it is only visible to HMaster.

The problem with the methods being private is that they had to be accessed through synthetic accessors. Could use package private I suppose, but if the class itself is not visible outside of HMaster, is it really a problem? I'll change to package visibility if you think so.

> This patch is a band-aid on a real issue we have yet to find - improper clean up of rows.
> Maybe it can help us figure where the real prob. is by outputting more info about the row its deleting?
> E.g., in deleteEmptyMetaRows, do you think it should enumerate all the cells that are remaining?
> Might give us clue as to where the meta row removes is working improperly.

The patch is a band-aid intended to be as minimal as possible while still making 0.1 work well
enough to release.

Since all the calls to deleteEmptyMetaRows are made after a scan of a meta region (because
you don't want to update while you're scanning), I could make the callers output more information
since they know the context better than the low level method.

> Bryan Duxbury - 10/Mar/08 02:53 PM
> Is there any chance that we are seeing the row halfway through an update? If so, then deleting the entire
> thing when one of the cells is empty would really break things.

HRegionInfo should never be null so long as any other cell exists. It is written when a region is created
either via create table or split, and should not be deleted until both splitA and splitB are empty after
a split.

> The original suggestion was to use deleteAll in situations where we wanted to delete a row, and
> assume that the problem was some rows were incompletely cleaned up because we weren't doing
> deleteAll before. In that regard, I think the change to HRegion is correct, but perhaps the cleanup in
> HMaster is premature. Should we commit a patch with just the HRegion deleteAll change, and see
> if this problem manifests itself any further?

I think it will continue to happen even with deleteAll and since it really breaks things to have these
kind of rows around, nuking them can't really make matters worse.

Adding the context around a delete should give us more insight into what we are doing when we
run over these land mines.


;;;","11/Mar/08 18:05;jimk;Summary of changes:

HMaster:
- When a row has an empty HRegionInfo (info:regioninfo), log it with the row name and and the other keys still in the row.

- Log the number of rows with empty HRegionInfo

- Delete the rows

- Make RowMap inner class static, change methods to have package scope to avoid synthetic accessors.

- Provide row name to getHRegionInfo so it can issue better log messages

- add method deleteEmptyMetaRows to remove rows with empty HRegionInfo

HRegion

- change removeRegionFromMETA to use deleteAll rather than using a BatchUpdate containing deletes for each cell.

TestEmptyMetaInfo

- new test case

Sample of new log messages:
{code}
    [junit] 2008-03-11 11:00:34,078 INFO  [HMaster.metaScanner] hbase.HMaster$BaseScanner(211): HMaster.metaScanner scanning meta region {regionname: .META.,,1, startKey: <>, server: 10.69.80.2:3204}
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster(3231): info:regioninfo is empty for row: tablename,1,1205258426000; has keys: [info:server]
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster(3231): info:regioninfo is empty for row: tablename,1205258426000; has keys: [info:server]
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster(3231): info:regioninfo is empty for row: tablename,2,1205258426000; has keys: [info:server]
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster(3231): info:regioninfo is empty for row: tablename,3,1205258426000; has keys: [info:server]
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster(3231): info:regioninfo is empty for row: tablename,4,1205258426000; has keys: [info:server]
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster$BaseScanner(288): Found 5 rows with empty HRegionInfo while scanning meta region .META.,,1
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster(3252): Removed region: tablename,1,1205258426000 from meta region: .META.,,1 because HRegionInfo was empty
    [junit] 2008-03-11 11:00:34,093 WARN  [HMaster.metaScanner] hbase.HMaster(3252): Removed region: tablename,1205258426000 from meta region: .META.,,1 because HRegionInfo was empty
    [junit] 2008-03-11 11:00:34,109 WARN  [HMaster.metaScanner] hbase.HMaster(3252): Removed region: tablename,2,1205258426000 from meta region: .META.,,1 because HRegionInfo was empty
    [junit] 2008-03-11 11:00:34,109 WARN  [HMaster.metaScanner] hbase.HMaster(3252): Removed region: tablename,3,1205258426000 from meta region: .META.,,1 because HRegionInfo was empty
    [junit] 2008-03-11 11:00:34,109 WARN  [HMaster.metaScanner] hbase.HMaster(3252): Removed region: tablename,4,1205258426000 from meta region: .META.,,1 because HRegionInfo was empty
    [junit] 2008-03-11 11:00:34,109 INFO  [HMaster.metaScanner] hbase.HMaster$BaseScanner(303): HMaster.metaScanner scan of meta region {regionname: .META.,,1, startKey: <>, server: 10.69.80.2:3204} complete
{code}
;;;","12/Mar/08 04:38;stack;Patch looks great as do the commit comments.  Suggest you include them when you apply the patch.  The patch applied to branch with some noise but no rejections.  All tests pass except TTMR but that ain't nothing new. +1;;;","13/Mar/08 01:27;jimk;Committed to 0.1 and trunk. Resolving issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when hbase regionserver restarts, it says ""impossible state for createLease()""",HBASE-12,12384041,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,stack,bien,bien,06/Dec/07 02:14,22/Aug/08 21:13,01/Jul/25 07:49,15/Apr/08 03:36,0.2.0,,,,,0.1.2,0.2.0,,regionserver,,,,0,"I restarted a regionserver, and got this error in its logs:

org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.AssertionError: Impossible state for createLease(): Lease -435227488/-435227488 is still held.
        at org.apache.hadoop.hbase.Leases.createLease(Leases.java:145)
        at org.apache.hadoop.hbase.HMaster.regionServerStartup(HMaster.java:1278
)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)

        at org.apache.hadoop.ipc.Client.call(Client.java:482)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at $Proxy0.regionServerStartup(Unknown Source)
        at org.apache.hadoop.hbase.HRegionServer.reportForDuty(HRegionServer.jav
a:1025)
        at org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:659)
        at java.lang.Thread.run(Unknown Source)",,,,,,,,,,,,,,,,,,,,,,,HBASE-495,,,,,,"14/Apr/08 21:11;stack;12-0.1.patch;https://issues.apache.org/jira/secure/attachment/12380114/12-0.1.patch","14/Apr/08 20:21;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380108/patch.txt","14/Apr/08 20:08;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380103/patch.txt","14/Apr/08 19:06;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380092/patch.txt","14/Apr/08 16:54;jimk;patch.txt;https://issues.apache.org/jira/secure/attachment/12380076/patch.txt",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,24850,,,,,Tue Apr 15 03:36:00 UTC 2008,,,,,,,,,,"0|i0h3f3:",97816,,,,,,,,,,,,,,,,,,,,,"07/Jan/08 19:12;bryanduxbury;Did this actually cause any problems? Did the Region Server continue to start up ok, or is this just a spurious error message?;;;","07/Jan/08 19:17;bien;I can't remember exactly.  I believe the regionserver keeps trying to connect, until eventually the old lease times out on the master.  ;;;","15/Jan/08 20:03;jimk;Is this still a problem? When did it last occur?;;;","17/Jan/08 22:53;stack;Happened this morning on a pset machine (About 2 weeks behind TRUNK).  Here is excerpt from shutdown followed closely by restart:

{code}
...
2008-01-16 03:17:06,124 INFO org.apache.hadoop.hbase.HRegionServer: worker thread exiting
2008-01-16 03:17:06,124 INFO org.apache.hadoop.hbase.HRegionServer: Shutdown thread complete
2008-01-16 03:17:09,755 DEBUG org.apache.hadoop.hbase.HRegionServer: Telling master we are up
2008-01-16 03:17:09,790 WARN org.apache.hadoop.hbase.HRegionServer: error telling master we are up
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.AssertionError: Impossible state for createLease(): Lease -1387125223/-1387125223 is still held.
        at org.apache.hadoop.hbase.Leases.createLease(Leases.java:145)
        at org.apache.hadoop.hbase.HMaster.regionServerStartup(HMaster.java:1302) 
        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379) 
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
        
        at org.apache.hadoop.ipc.Client.call(Client.java:482)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at $Proxy0.regionServerStartup(Unknown Source)
        at org.apache.hadoop.hbase.HRegionServer.reportForDuty(HRegionServer.java:1073)
        at org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:688)
        at java.lang.Thread.run(Unknown Source)
...
{code}

Goes on and on... will try and get master side of things....;;;","18/Jan/08 02:05;stack;Here's more info...

Does above exceptions for 4 hours and then...

{code}
...
2008-01-16 07:18:58,757 DEBUG org.apache.hadoop.hbase.HRegionServer: Done telling master we are up
2008-01-16 07:18:58,757 DEBUG org.apache.hadoop.hbase.HRegionServer: Config from master: fs.default.name=XX.XX.XX.XX:10000
2008-01-16 07:18:58,757 DEBUG org.apache.hadoop.hbase.HRegionServer: Config from master: hbase.rootdir=/hbase/XX.XX.XX.XX
...
{code}

... then it does this... (which don't look too pretty):

{code}
...
2008-01-16 07:18:59,517 INFO org.apache.hadoop.hbase.HRegionServer: HRegionServer started at: XX.XX.XX.232:60020
2008-01-16 07:18:59,517 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 60020: starting
2008-01-16 07:36:01,780 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:36:01,804 DEBUG org.apache.hadoop.hbase.HStore: starting .META.,,1/info (1028785192/info) (no reconstruction log)
2008-01-16 07:36:01,832 DEBUG org.apache.hadoop.hbase.HStore: maximum sequence id for hstore .META.,,1/info (1028785192/info) is 26067834
2008-01-16 07:36:01,852 DEBUG org.apache.hadoop.hbase.HRegion: Next sequence id for region .META.,,1 is 26067835
2008-01-16 07:36:01,853 INFO org.apache.hadoop.hbase.HRegion: region .META.,,1 available
2008-01-16 07:36:01,854 DEBUG org.apache.hadoop.hbase.HLog: changing sequence number from 0 to 26067835
2008-01-16 07:37:04,137 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:37:10,144 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:37:21,476 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:39:01,468 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:39:22,489 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:40:27,559 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:41:12,603 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:41:27,619 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:41:36,628 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:41:42,633 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:41:48,640 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:41:57,650 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:42:03,657 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:42:21,675 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:43:03,719 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:46:47,721 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:47:05,738 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:47:11,744 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:47:43,403 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:47:49,408 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:48:04,423 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:48:37,457 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:52:18,805 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:52:30,817 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:55:39,002 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:59:40,590 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 07:59:55,605 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:03:34,072 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:03:40,079 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:04:01,099 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:04:10,108 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:04:16,113 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:04:22,120 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:04:46,724 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:04:52,730 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:05:16,754 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:05:22,760 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:09:11,033 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:12:01,093 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:12:07,100 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:13:55,994 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:14:05,002 INFO org.apache.hadoop.hbase.HRegionServer: MSG_REGION_OPEN : .META.,,1
2008-01-16 08:17:20,146 WARN org.apache.hadoop.hbase.HRegionServer: Processing message (Retry: 0)
java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:484)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at $Proxy0.regionServerReport(Unknown Source)
        at org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:714)
        at java.lang.Thread.run(Unknown Source)
2008-01-16 08:17:20,147 FATAL org.apache.hadoop.hbase.HRegionServer: unable to report to master for 63015 milliseconds - aborting server
..
{code}

.. and shuts itself down.

Back later w/ master side of things...;;;","06/Feb/08 22:28;jdcryans;Got pretty much the same thing here. Fetched 0.17-dev, compiled, config hadoop, started dfs, config hbase with just the master adress and regionservers (12 nodes), started hbase and then this. (sorry for long post)

HBase master log :
{code}
...
2008-02-06 16:57:22,131 INFO org.apache.hadoop.hbase.HMaster: Root region dir: /tmp/hadoop-root/hbase/-ROOT-/70236052
2008-02-06 16:57:23,577 INFO org.apache.hadoop.hbase.HMaster: bootstrap: creating ROOT and first META regions
2008-02-06 16:57:23,792 INFO org.apache.hadoop.hbase.HLog: new log writer created at hdfs://192.168.0.1:9000/tmp/hadoop-root/hbase/-ROOT-/70236052/log/hlog.dat.000
2008-02-06 16:57:23,964 INFO org.apache.hadoop.hbase.HRegion: region -ROOT-,,0 available
2008-02-06 16:57:24,014 INFO org.apache.hadoop.hbase.HLog: new log writer created at hdfs://192.168.0.1:9000/tmp/hadoop-root/hbase/.META./1028785192/log/hlog.dat.000
2008-02-06 16:57:24,079 INFO org.apache.hadoop.hbase.HRegion: region .META.,,1 available
2008-02-06 16:57:24,217 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2008-02-06 16:57:25,398 INFO org.apache.hadoop.hbase.HRegion: closed -ROOT-,,0
2008-02-06 16:57:25,474 INFO org.apache.hadoop.hbase.HRegion: closed .META.,,1
2008-02-06 16:57:25,887 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing RPC Metrics with serverName=60000, port=60000
2008-02-06 16:57:26,342 INFO org.apache.hadoop.hbase.HMaster: HMaster initialized on 192.168.0.1:60000
2008-02-06 16:57:26,748 INFO org.mortbay.util.Credential: Checking Resource aliases
2008-02-06 16:57:26,959 INFO org.mortbay.http.HttpServer: Version Jetty/5.1.4
2008-02-06 16:57:26,963 INFO org.mortbay.util.Container: Started HttpContext[/static,/static]
2008-02-06 16:57:26,963 INFO org.mortbay.util.Container: Started HttpContext[/logs,/logs]
2008-02-06 16:57:28,489 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@8997d1
2008-02-06 16:57:28,670 INFO org.mortbay.util.Container: Started WebApplicationContext[/,/]
2008-02-06 16:57:29,815 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@1b7ae22
2008-02-06 16:57:29,827 INFO org.mortbay.util.Container: Started WebApplicationContext[/api,rest]
2008-02-06 16:57:29,839 INFO org.mortbay.http.SocketListener: Started SocketListener on 0.0.0.0:60010
2008-02-06 16:57:29,839 INFO org.mortbay.util.Container: Started org.mortbay.jetty.Server@b301f2
2008-02-06 16:57:29,840 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2008-02-06 16:57:29,842 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 60000: starting
2008-02-06 16:57:29,873 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 60000: starting
2008-02-06 16:57:29,874 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 60000: starting
2008-02-06 16:57:29,875 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 60000: starting
2008-02-06 16:57:29,876 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 60000: starting
2008-02-06 16:57:29,877 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 60000: starting
2008-02-06 16:57:29,878 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 60000: starting
2008-02-06 16:57:29,878 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 60000: starting
2008-02-06 16:57:29,879 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 60000: starting
2008-02-06 16:57:29,880 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 60000: starting
2008-02-06 16:57:29,881 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 60000: starting
2008-02-06 16:57:29,923 INFO org.apache.hadoop.hbase.HMaster: received start message from: 127.0.0.1:60020
2008-02-06 16:57:29,962 INFO org.apache.hadoop.hbase.HMaster: received start message from: 127.0.0.1:60020
2008-02-06 16:57:29,972 INFO org.apache.hadoop.hbase.HMaster: received start message from: 127.0.0.1:60020
2008-02-06 16:57:29,982 INFO org.apache.hadoop.hbase.HMaster: received start message from: 127.0.0.1:60020
2008-02-06 16:57:29,995 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 60000, call regionServerStartup(address: 127.0.0.1:60020, startcode: 1202333840817, load: (requests: 0 regions: 0)) from 192.168.0.5:43524: error: java.io.IOException: java.lang.AssertionError: Impossible state for createLease(): Lease -1109690057/-1109690057 is still held.
java.io.IOException: java.lang.AssertionError: Impossible state for createLease(): Lease -1109690057/-1109690057 is still held.
        at org.apache.hadoop.hbase.Leases.createLease(Leases.java:145)
        at org.apache.hadoop.hbase.HMaster.regionServerStartup(HMaster.java:1310)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
... many times

and finally

2008-02-06 17:14:44,701 INFO org.apache.hadoop.hbase.HMaster: Cancelling lease for 127.0.0.1:60020
2008-02-06 17:14:44,701 INFO org.apache.hadoop.hbase.HMaster: received start message from: 127.0.0.1:60020
2008-02-06 17:14:44,701 INFO org.apache.hadoop.hbase.HMaster: Region server 127.0.0.1:60020: MSG_REPORT_EXITING -- lease cancelled
2008-02-06 17:14:45,289 INFO org.apache.hadoop.hbase.HMaster: Cancelling lease for 127.0.0.1:60020
2008-02-06 17:14:46,901 INFO org.apache.hadoop.hbase.HMaster: All user tables quiesced. Proceeding with shutdown
2008-02-06 17:14:46,904 INFO org.apache.hadoop.hbase.HMaster$MetaScanner: HMaster.metaScanner exiting
2008-02-06 17:14:47,161 INFO org.apache.hadoop.hbase.HMaster: All user tables quiesced. Proceeding with shutdown
2008-02-06 17:14:47,587 INFO org.apache.hadoop.hbase.HMaster: All user tables quiesced. Proceeding with shutdown
2008-02-06 17:14:47,628 INFO org.apache.hadoop.hbase.HMaster: All user tables quiesced. Proceeding with shutdown
2008-02-06 17:14:48,936 INFO org.apache.hadoop.hbase.HMaster: Stopping infoServer
2008-02-06 17:14:48,937 INFO org.mortbay.util.ThreadedServer: Stopping Acceptor ServerSocket[addr=0.0.0.0/0.0.0.0,port=0,localport=60010]
2008-02-06 17:14:48,967 INFO org.mortbay.http.SocketListener: Stopped SocketListener on 0.0.0.0:60010
2008-02-06 17:14:49,307 INFO org.mortbay.util.Container: Stopped HttpContext[/static,/static]
2008-02-06 17:14:49,756 INFO org.mortbay.util.Container: Stopped HttpContext[/logs,/logs]
2008-02-06 17:14:49,757 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.servlet.WebApplicationHandler@1a4c5b4
2008-02-06 17:14:50,087 INFO org.mortbay.util.Container: Stopped WebApplicationContext[/,/]
2008-02-06 17:14:50,087 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.servlet.WebApplicationHandler@5b675e
2008-02-06 17:14:50,466 INFO org.mortbay.util.Container: Stopped WebApplicationContext[/api,rest]
2008-02-06 17:14:50,467 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.Server@a00185
2008-02-06 17:14:50,467 INFO org.apache.hadoop.ipc.Server: Stopping server on 60000
2008-02-06 17:14:50,469 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 60000: exiting
2008-02-06 17:14:50,469 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 60000: exiting
2008-02-06 17:14:50,469 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 60000: exiting
2008-02-06 17:14:50,470 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 60000: exiting
2008-02-06 17:14:50,470 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 60000: exiting
2008-02-06 17:14:50,470 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 60000: exiting
2008-02-06 17:14:50,470 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 60000: exiting
2008-02-06 17:14:50,471 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 60000
2008-02-06 17:14:50,471 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 60000: exiting
2008-02-06 17:14:50,472 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 60000: exiting
2008-02-06 17:14:50,473 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 60000: exiting
2008-02-06 17:14:50,483 INFO org.apache.hadoop.hbase.Leases: HMaster closing leases
2008-02-06 17:14:50,484 INFO org.apache.hadoop.hbase.Leases$LeaseMonitor: HMaster.leaseChecker exiting
2008-02-06 17:14:50,485 INFO org.apache.hadoop.hbase.Leases: HMaster closed leases
2008-02-06 17:14:50,495 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
{code}

regionservers stop after few seconds, unable to stop master properly.;;;","07/Feb/08 20:12;jdcryans;Got the same thing today with 0.2.0 from trunk. Messages are different like  ""Lease -1109690057/-1109690057 is still held ""is now  lease '127.0.0.1:60020' already exists"". Seems to me that each regionservers are asking for the same lease. Is it configuration related?;;;","07/Feb/08 20:19;stack;Are regionservers bouncing Jean-Daniel?  That is, are they going down then coming back up again soon after?  Check the logs under the regionservers themselves to see.  Enable DEBUG too.  You'll get picture of whats going on (See FAQ for how).;;;","07/Feb/08 20:46;jdcryans;Regionservers starts right away with (here node11) :

{code}
2008-02-07 17:29:08,057 WARN org.apache.hadoop.hbase.HRegionServer: error telling master we are up
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.IllegalStateException: lease '127.0.0.1:60020' already exists
        at org.apache.hadoop.hbase.Leases.createLease(Leases.java:142)
        at org.apache.hadoop.hbase.HMaster.regionServerStartup(HMaster.java:1310)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)

        at org.apache.hadoop.ipc.Client.call(Client.java:512)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:210)
        at $Proxy0.regionServerStartup(Unknown Source)
        at org.apache.hadoop.hbase.HRegionServer.reportForDuty(HRegionServer.java:1014)
        at org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:633)
        at java.lang.Thread.run(Thread.java:619)
...
{code}

Then it proceeds with shutting down.;;;","07/Feb/08 20:56;stack;Can you try shutting down the cluster to make it go away?  Do you have a recipe for triggering the issue?  That'd help.  Thanks.;;;","07/Feb/08 21:00;stack;Could your machines be misconfigured?  Are they all reporting in to the master that they are 127.0.0.1:60020?;;;","07/Feb/08 21:06;jdcryans;For your 12:56 comment stack, nope, it always does. But the DFS works in distributed mode.

I also come to think it's machine misconfiguration. They all have the same log so they are reporting that they are 127.0.0.1:60020 but I'm not really sure where to look first. OS digging time...;;;","12/Feb/08 20:43;jdcryans;Finally solved for me. I had to insert the dfs.datanode.dns.interface property in hbase-site.xml to have the regionservers send their good IP (and not 127.0.0.1). 

Thanks.;;;","19/Feb/08 00:45;bryanduxbury;What's the status of this issue? Is it still alive? The latest comments seem to indicate it was a DNS misconfiguration problem. Stack, could that have been the cause of your experience with the issue?;;;","19/Feb/08 18:33;stack;The misconfiguration I believe was cause of Jean-Daniel's 'Impossible Lease' exceptions but I don't believe it cause of the original report nor my subsequent observance..  ;;;","19/Feb/08 18:39;jimk;This problem can arise when a region server restarts before its original lease times out.;;;","19/Feb/08 20:02;bryanduxbury;What should we do when this exception shows up? Immediately cancel the existing lease? Let the lease time out and have the RegionServer keep retrying?

I think we should add to the error message and describe possible causes and resolutions. Something like ""Check your DNS configuration so that all region servers are reporting their true IPs and not 127.0.0.1. Otherwise, this problem will resolve itself in ${lease-timeout} seconds."";;;","19/Feb/08 20:11;jimk;I like your latter suggestion:

> Bryan Duxbury - 19/Feb/08 12:02 PM
> I think we should add to the error message and describe possible causes and resolutions.
> Something like ""Check your DNS configuration so that all region servers are reporting their true
> IPs and not 127.0.0.1. Otherwise, this problem will resolve itself in ${lease-timeout} seconds.""

But the region server should stay alive until lease timeout period expires and report for duty
again at that time.;;;","19/Feb/08 20:36;jdcryans;Bryan, I like your suggestion too. Would have saved me a couple of hours searching in my OS, Hadoop and HBase configurations.

And how come the dfs.datanode.dns.interface isn't documented anyway?
I know this is HBase's Jira but HDFS was working fine even if it was set to default, unlike my HBase setup.

Thx

;;;","07/Mar/08 19:44;jimk;The solution for ""impossible state for createLease"" is addressed by HBASE-495.

However, leaving this issue open until ""friendlier"" error message is added.;;;","14/Apr/08 16:54;jimk;Provides friendly message while region server is waiting for old lease to expire;;;","14/Apr/08 16:56;jimk;Passes tests. Please review.;;;","14/Apr/08 18:54;stack;LOG.info would be better than system.out I'd say... Thats where I think people will be looking.  And I like bryan's message better... And perhaps a particular exception based on IOE rather than ISE?;;;","14/Apr/08 19:06;jimk;Uses LOG.info instead of System.out.println;;;","14/Apr/08 19:06;jimk;Change System.out.println to LOG.info;;;","14/Apr/08 20:08;jimk;Address comments.;;;","14/Apr/08 20:09;jimk;Ok. this is the patch for 12.;;;","14/Apr/08 20:21;jimk;In 0.1.x, these errors are AssertionErrors. Catch them and try to continue.;;;","14/Apr/08 20:41;stack;Latest patch will output log about DNS and leases if we get any throwable (OOME, etc.).  That'll confuse.  Making a proposal....;;;","14/Apr/08 21:11;stack;Alternate proposal;;;","14/Apr/08 21:18;bien;+1 for 12-0.1.patch

I don't think we should throw AssertionError in reachable code (excluding software bugs).;;;","15/Apr/08 03:36;stack;Committed to trunk and branch.  Thanks for the review Michael.;;;",,,,,,,,,,,,,,,,,,,,
